

[
  
  
    
    
      {
        "title": "Set up Python, Keras, and Jupyter",
        "excerpt": "This page will show you how to set up your computer to follow along with the creative prediction examples.\n\nAll of our code examples are provided as Python 3 code in jupyter notebooks, you download these to your computer or alternatively open them directy in a browser-based Python environment.\n\n\n\nRunning examples on your computer (Python/Jupyter Setup)\n\nBasically: you need a Python 3 environment with up-to-date versions of: keras, tensorflow, numpy, pandas, matplotlib, music21, jupyter, keras-mdn-layer\n\nAnnoyingly, there are two popular ways to set up Python on your computer: one installs python packages together in a special “environment” for each project you do, and the other sets up the packages for every project on your system.\n\nSetup Miniconda to get the right Python\n\n\n  Follow the installation instructions here\n\n\nOn Windows and confused? Try this page for help.\n\nInstall packages with Pip\n\n\n  Open a terminal or command line window.\n  Install the libraries with pip install tensorflow tensorflow-probability keras numpy pandas matplotlib music21 jupyter glob3 svgwrite keras-mdn-layer (this might take a while).\n  Start up Jupyter notebook: jupyter notebook\n  You’re ready!\n\n\nVirtual Environments\n\nMany people like to keep their Python libraries in separate “environments” for each project. If you do this, you can access each environment from Jupyter individually which is handy! Here’s how to for this.\n\nRunning examples in a browser (Google Colab)\n\nExercise: Try out a Google Colab Notebook\n\nYou can also run these notebooks on Google Colaboratory, a free-to-use Jupyter notebook environment that has most of the necessary Python libraries pre-installed. It even works on a tablet! If you want to get started quickly without slowing down to get your Python install right, Colab is a great way to go. The tutorials page has links ot open each example directory in Colab.\n\nColab has some amazing features:\n\n\n  You can load all the notebooks for this workshop straight from the GitHub repo.\n  You can use a GPU for free to train a biggish RNN\n  Here’s some more instructions!\n\n\nThere are some downsides though:\n\n\n  uploading and downloading data is a bit fiddly.\n  you can’t use music21’s playback features to hear generated MIDI scores, or visualise them with musescore.\n  you can’t run IMPS in Colab (it’s not a jupyter notebook).\n\n",
        "content": "This page will show you how to set up your computer to follow along with the creative prediction examples.\n\nAll of our code examples are provided as Python 3 code in jupyter notebooks, you download these to your computer or alternatively open them directy in a browser-based Python environment.\n\n\n\nRunning examples on your computer (Python/Jupyter Setup)\n\nBasically: you need a Python 3 environment with up-to-date versions of: keras, tensorflow, numpy, pandas, matplotlib, music21, jupyter, keras-mdn-layer\n\nAnnoyingly, there are two popular ways to set up Python on your computer: one installs python packages together in a special “environment” for each project you do, and the other sets up the packages for every project on your system.\n\nSetup Miniconda to get the right Python\n\n\n  Follow the installation instructions here\n\n\nOn Windows and confused? Try this page for help.\n\nInstall packages with Pip\n\n\n  Open a terminal or command line window.\n  Install the libraries with pip install tensorflow tensorflow-probability keras numpy pandas matplotlib music21 jupyter glob3 svgwrite keras-mdn-layer (this might take a while).\n  Start up Jupyter notebook: jupyter notebook\n  You’re ready!\n\n\nVirtual Environments\n\nMany people like to keep their Python libraries in separate “environments” for each project. If you do this, you can access each environment from Jupyter individually which is handy! Here’s how to for this.\n\nRunning examples in a browser (Google Colab)\n\nExercise: Try out a Google Colab Notebook\n\nYou can also run these notebooks on Google Colaboratory, a free-to-use Jupyter notebook environment that has most of the necessary Python libraries pre-installed. It even works on a tablet! If you want to get started quickly without slowing down to get your Python install right, Colab is a great way to go. The tutorials page has links ot open each example directory in Colab.\n\nColab has some amazing features:\n\n\n  You can load all the notebooks for this workshop straight from the GitHub repo.\n  You can use a GPU for free to train a biggish RNN\n  Here’s some more instructions!\n\n\nThere are some downsides though:\n\n\n  uploading and downloading data is a bit fiddly.\n  you can’t use music21’s playback features to hear generated MIDI scores, or visualise them with musescore.\n  you can’t run IMPS in Colab (it’s not a jupyter notebook).\n\n",
        "url": "/hack/setup/"
      },
    
      {
        "title": "Try out a Colab Notebook",
        "excerpt": "This one’s more a link than a tutorial ;-)\n\nCheck out this Google Colab Notebook, it’s a self-contained tutorial!\n\n\n",
        "content": "This one’s more a link than a tutorial ;-)\n\nCheck out this Google Colab Notebook, it’s a self-contained tutorial!\n\n\n",
        "url": "/hack/colab/"
      },
    
      {
        "title": "Make your own text generator",
        "excerpt": "\n\nFor this exercise you’ll need to be running our Star Trek Episode\nTitle Generator either in Colab or on your own computer.\n\nNotebook: Star Trek Titles charRNN\n(Github)\n(open in Colab)\n\n\n  Start by opening the example code for inventing Star Trek episode titles. (try the advanced version which is less verbose)\n  Step through the code, and make sure that you can get it to generate some interesting text.\n  Try changing the diversity/temperature to produce more “normal” and more “weird” results.\n  Now it gets interesting. Think of some data that YOU would like to generate which can be represented as text. This could be actual text, or something else (images, or sound) represented as text characters. Almost everything in computing can be represented as text, so there’s a lot of scope for experimentation! Change the LSTM text generator to learn from your dataset and to generate new creative data.\n  Think about how your creative data generator could be applied in a work of art. Could it be integrated into a website, or some kind of art installation? Come up with a artistic concept that relies on genuine data generation. What questions would this work ask of technology, and of us?\n\n",
        "content": "\n\nFor this exercise you’ll need to be running our Star Trek Episode\nTitle Generator either in Colab or on your own computer.\n\nNotebook: Star Trek Titles charRNN\n(Github)\n(open in Colab)\n\n\n  Start by opening the example code for inventing Star Trek episode titles. (try the advanced version which is less verbose)\n  Step through the code, and make sure that you can get it to generate some interesting text.\n  Try changing the diversity/temperature to produce more “normal” and more “weird” results.\n  Now it gets interesting. Think of some data that YOU would like to generate which can be represented as text. This could be actual text, or something else (images, or sound) represented as text characters. Almost everything in computing can be represented as text, so there’s a lot of scope for experimentation! Change the LSTM text generator to learn from your dataset and to generate new creative data.\n  Think about how your creative data generator could be applied in a work of art. Could it be integrated into a website, or some kind of art installation? Come up with a artistic concept that relies on genuine data generation. What questions would this work ask of technology, and of us?\n\n",
        "url": "/hack/text/"
      },
    
      {
        "title": "Make some music with an RNN",
        "excerpt": "\n\nLet’s make some music with an LSTM-RNN!\n\nFor this exercise, you can start with the “music RNN” example here on\nyour computer.\n\nThis example works on Colab as well, but you won’t be able to view or\nhear the generated music as easily so it’s not quite as much fun.\n\nNotebook: Zeldic RNN\n(Github)\n(open in Colab)\n\nThe melody RNN needs a few extra pieces of software to work correctly.\nYou’ll need to install the Python library music21 to read and\nwrite MIDI files, and musescore to play them back in Jupyter\nNotebooks.\n\n\n  The melody RNN code is designed to train a small RNN on a corpus of\nMIDI files. Just as the charRNN generates text letter by letter, the\nmelody RNN generates music note-by-note. There’s a small corpus of\nMIDI data (already processed) provided with the example, try running\nthe code to see if you can train an RNN with this data.\n  Find some of your own MIDI files to load into the melody-RNN.\nYou’ll have to run the dataset generator code to get them into\na format that the RNN can train from.\n  The musical representation for this RNN is very simple: Integers\nsuch that 0-127 means a note-on at that pitch, 128 means do\nnothing, and 129 means note-off. Can you think of a different way\nof representing the notes that would work with this RNN?\n  Think about how a melody generator RNN could be integrated into an\ninstrument or artwork. How would it challenge or extend the role of\nthe performer, and of the listener?\n\n\nNotes on the MIDI representation:\n\nThis script uses a very simple method to represent MIDI melodies as a\nsequence of integers.\n\n\n  0-127 play a note at that MIDI note number. (MELODY_NOTE_ON)\n  128 stop whatever note was playing. (MELODY_NOTE_OFF)\n  129 do nothing. (MELODY_NO_EVENT)\n\n\nThis encoding is inspired (and borrowed) from Magenta’s Melody_RNN\nmodel.\n\nNote that this is an early and (now) superseded practice for\nrepresenting MIDI data to a deep neural network. The most modern\nmodels\nlike\nMusic Transformer\nand PerformanceRNN tend to use more complicated representations. One\ndownside of this representation is that 128 and 129 are extremely\nover-represented in the data which can cause issues with learning very\nlarge amounts of data or more complicated models.\n\nHowever, this one is simple and easy to calculate. I have a more\nadvanced script for converting a lot of MIDI files if you’re\ninterested.\n\n\n\n",
        "content": "\n\nLet’s make some music with an LSTM-RNN!\n\nFor this exercise, you can start with the “music RNN” example here on\nyour computer.\n\nThis example works on Colab as well, but you won’t be able to view or\nhear the generated music as easily so it’s not quite as much fun.\n\nNotebook: Zeldic RNN\n(Github)\n(open in Colab)\n\nThe melody RNN needs a few extra pieces of software to work correctly.\nYou’ll need to install the Python library music21 to read and\nwrite MIDI files, and musescore to play them back in Jupyter\nNotebooks.\n\n\n  The melody RNN code is designed to train a small RNN on a corpus of\nMIDI files. Just as the charRNN generates text letter by letter, the\nmelody RNN generates music note-by-note. There’s a small corpus of\nMIDI data (already processed) provided with the example, try running\nthe code to see if you can train an RNN with this data.\n  Find some of your own MIDI files to load into the melody-RNN.\nYou’ll have to run the dataset generator code to get them into\na format that the RNN can train from.\n  The musical representation for this RNN is very simple: Integers\nsuch that 0-127 means a note-on at that pitch, 128 means do\nnothing, and 129 means note-off. Can you think of a different way\nof representing the notes that would work with this RNN?\n  Think about how a melody generator RNN could be integrated into an\ninstrument or artwork. How would it challenge or extend the role of\nthe performer, and of the listener?\n\n\nNotes on the MIDI representation:\n\nThis script uses a very simple method to represent MIDI melodies as a\nsequence of integers.\n\n\n  0-127 play a note at that MIDI note number. (MELODY_NOTE_ON)\n  128 stop whatever note was playing. (MELODY_NOTE_OFF)\n  129 do nothing. (MELODY_NO_EVENT)\n\n\nThis encoding is inspired (and borrowed) from Magenta’s Melody_RNN\nmodel.\n\nNote that this is an early and (now) superseded practice for\nrepresenting MIDI data to a deep neural network. The most modern\nmodels\nlike\nMusic Transformer\nand PerformanceRNN tend to use more complicated representations. One\ndownside of this representation is that 128 and 129 are extremely\nover-represented in the data which can cause issues with learning very\nlarge amounts of data or more complicated models.\n\nHowever, this one is simple and easy to calculate. I have a more\nadvanced script for converting a lot of MIDI files if you’re\ninterested.\n\n\n\n",
        "url": "/hack/melody/"
      },
    
      {
        "title": "Make a RoboJam model",
        "excerpt": "\n\nIn this exercise you’ll try making your own MDRNN. The example code\nfor RoboJam and the Kanji RNN are below.\n\nNotebook: MDN Robojam Touch Generation\n(Github)\n(open in Colab)\n\n\n  Start by stepping through the code for either the Kanji or RoboJam\nMDRNN. These systems both use 3D RNNs: 1 dimension for each of the\nx and y directions, and the pen state (down or up) for the Kanji\nRNN, and time for RoboJam.\n  Think about some continuous-valued data you would like to model. It\ncould be 3D like the Kanji, or RoboJam RNN, or have more or less\ndimensions (it’s easy to change the dimension of the RNN). Try to\nfind some data in form that is easy to open with pandas and\nnumpy. Try to load the data into the notebook and plot sections\nof it.\n  Try training a new MDRNN with your data! It might take a while to\nstart getting useful results, but keep track of what you did so\nthat you can improve your MDRNN next time.\n  Can your generated data be incorporated into an artwork,\ncomposition or instrument? Think about how the different dimensions\nof the data are represented. Are they related or independent?\n\n\n",
        "content": "\n\nIn this exercise you’ll try making your own MDRNN. The example code\nfor RoboJam and the Kanji RNN are below.\n\nNotebook: MDN Robojam Touch Generation\n(Github)\n(open in Colab)\n\n\n  Start by stepping through the code for either the Kanji or RoboJam\nMDRNN. These systems both use 3D RNNs: 1 dimension for each of the\nx and y directions, and the pen state (down or up) for the Kanji\nRNN, and time for RoboJam.\n  Think about some continuous-valued data you would like to model. It\ncould be 3D like the Kanji, or RoboJam RNN, or have more or less\ndimensions (it’s easy to change the dimension of the RNN). Try to\nfind some data in form that is easy to open with pandas and\nnumpy. Try to load the data into the notebook and plot sections\nof it.\n  Try training a new MDRNN with your data! It might take a while to\nstart getting useful results, but keep track of what you did so\nthat you can improve your MDRNN next time.\n  Can your generated data be incorporated into an artwork,\ncomposition or instrument? Think about how the different dimensions\nof the data are represented. Are they related or independent?\n\n\n",
        "url": "/hack/robojam/"
      },
    
      {
        "title": "Try out IMPS, Processing, and Pd",
        "excerpt": "\n\nIn this exercise you’ll try out the Interactive Music Prediction\nSystem (IMPS) in your own NIME, or a demo NIME that’s provided with\nthe code.\n\nIMPS is a bit more complicated and doesn’t run from a Jupyter\nnotebook, instead, you’ll download the source code and run it as a\nstandalone python application. It only has a command-line interface,\nso you’ll need to be familiar with how to navigate around your\ncomputer in a terminal or command-line shell to do this exercise.\n\nInstallation\n\nIMPS is written in Python with Keras and TensorFlow Probability, so it\nshould work on any platform where Tensorflow can be installed. Python\n3 is required.\n\nYou can find the installation instructions on\nthe IMPS page as well.\n\nFirst you should clone this repository or download it to your computer:\n\ngit clone https://github.com/cpmpercussion/imps.git\ncd imps\n\n\nThe python requirements can be installed as follows:\n\npip install -r requirements.txt\n\n\nIn addition, it’s handy to have two extra tools on your computer to\ntry out IMPs with a demo interface:\n\n\n  Processing\n  Pure Data (pd)\n\n\nInstallers for both of these can be found on their respective websites.\n\n1: Connecting to a musical interface\n\nTo start with, let’s connect IMPS to a simple demo NIME written in\nProcessing\n(install Processing if you haven’t\ngot it already). This is called processing_interface_demo and is\ncontained in the midi_controllers folder of the IMPS repository.\n\nThis demo NIME consists of a 2D mousing area that controls 2\nsynthesiser parameters (frequency and amplitude).\n\n\n\nTry making some sounds with the interface to make sure it’s working.\n\nNote that this is possibly the world’s worst NIME and that you can, and will, definitely do\nbetter. Humour me by trying out IMPS with a bad NIME for a bit before\nstarting to make your own.\n\n2. Log some training data\n\nStart up IMPS in logging only mode with this command\n\npython3 predictive_music_model.py --dimension=3 --log -o\n\n\nNote that the dimension is set to “3”: this is because our interface\nis 2 dimensions and we need one extra dimension for time.\n\nPlay around with the interface for a while to record some data. Maybe\nyou could try just doing a certain “style” of performance with it to\ntrain the MDRNN to reproduce a certain behaviour.\n\n3. Train an MDRNN\n\nThere’s two steps for training: Generate a dataset file, and train the predictive model.\n\nUse the generate_dataset command:\n\npython3 generate_dataset.py --dimension=3\n\n\nThis command collates all the logs with dimension “3” in the logging\ndirectory into a single .npz file.\n\nNow let’s do the training, here’s the command for the smallest MDRNN,\nwith early-stopping enabled so that it doesn’t take too long.\nHopefully your computer’s fan starts making a lot of\nnoise—good!—that’s how you know it’s working!\n\npython3 train_predictive_music_model.py --dimension=3 --modelsize=xs --earlystopping\n\n\nBecause we don’t have much data, and have set a very small model size, this shouldn’t take too long.\n\n4. Perform with your model.\n\nNow that you have a trained model, you can run this command to start making predictions:\n\npython3 predictive_music_model.py -d=3 --modelsize=xs --log -c\n\n\nIf your interface software is still running, you should hear it play\nitself! cool!\n\n5. Make this terrible NIME better\n\nThere’s many ways to make this NIME better. You could make the sounds\nnicer, make the visual interface nicer, or just scrap it and start\nagain in an environment of your choice! You could also\n\n6. Try with your own NIME!\n\nThis is a bit more of a project. First of all, what kind of data does\nyour NIME generate? Is it one or many dimensions? Secondly, when are\nthese data generated and available for a synthesiser. Are all\ndimensions sent on independent timescales (like individual keys on a\npiano for instance) or are they all combined into one stream of data?\n\nIMPS can only deal with messages that include EVERY bit of data from a\nNIME, so you might need to work on your code to get your output data into a format\nthat IMPS can handle. There’s some more details on the [IMPS\npage}(/imps/).\n\nOnce you’ve figured out how to get data into IMPS you can log some\ninteraction and train an MDRNN. Great!\n\nSo now you have a trained MDRNN, but what can you do with the\npredictions? WIll it play the same sounds as your regular instrument,\nor different ones? Will be be visualised or physicalised in some\nway? What benefit is there to having this data during a performances?\n\n7. Extend IMPS and use in your own projects!\n\nIMPS is an open source project and I hope that this community\nmakes some cool music with it. It has shortcomings, and it’s\nopinionated, so don’t hesitate to hack it into a predictive musical\nmodel that you want! (or the predictive musical model we deserve??)\n",
        "content": "\n\nIn this exercise you’ll try out the Interactive Music Prediction\nSystem (IMPS) in your own NIME, or a demo NIME that’s provided with\nthe code.\n\nIMPS is a bit more complicated and doesn’t run from a Jupyter\nnotebook, instead, you’ll download the source code and run it as a\nstandalone python application. It only has a command-line interface,\nso you’ll need to be familiar with how to navigate around your\ncomputer in a terminal or command-line shell to do this exercise.\n\nInstallation\n\nIMPS is written in Python with Keras and TensorFlow Probability, so it\nshould work on any platform where Tensorflow can be installed. Python\n3 is required.\n\nYou can find the installation instructions on\nthe IMPS page as well.\n\nFirst you should clone this repository or download it to your computer:\n\ngit clone https://github.com/cpmpercussion/imps.git\ncd imps\n\n\nThe python requirements can be installed as follows:\n\npip install -r requirements.txt\n\n\nIn addition, it’s handy to have two extra tools on your computer to\ntry out IMPs with a demo interface:\n\n\n  Processing\n  Pure Data (pd)\n\n\nInstallers for both of these can be found on their respective websites.\n\n1: Connecting to a musical interface\n\nTo start with, let’s connect IMPS to a simple demo NIME written in\nProcessing\n(install Processing if you haven’t\ngot it already). This is called processing_interface_demo and is\ncontained in the midi_controllers folder of the IMPS repository.\n\nThis demo NIME consists of a 2D mousing area that controls 2\nsynthesiser parameters (frequency and amplitude).\n\n\n\nTry making some sounds with the interface to make sure it’s working.\n\nNote that this is possibly the world’s worst NIME and that you can, and will, definitely do\nbetter. Humour me by trying out IMPS with a bad NIME for a bit before\nstarting to make your own.\n\n2. Log some training data\n\nStart up IMPS in logging only mode with this command\n\npython3 predictive_music_model.py --dimension=3 --log -o\n\n\nNote that the dimension is set to “3”: this is because our interface\nis 2 dimensions and we need one extra dimension for time.\n\nPlay around with the interface for a while to record some data. Maybe\nyou could try just doing a certain “style” of performance with it to\ntrain the MDRNN to reproduce a certain behaviour.\n\n3. Train an MDRNN\n\nThere’s two steps for training: Generate a dataset file, and train the predictive model.\n\nUse the generate_dataset command:\n\npython3 generate_dataset.py --dimension=3\n\n\nThis command collates all the logs with dimension “3” in the logging\ndirectory into a single .npz file.\n\nNow let’s do the training, here’s the command for the smallest MDRNN,\nwith early-stopping enabled so that it doesn’t take too long.\nHopefully your computer’s fan starts making a lot of\nnoise—good!—that’s how you know it’s working!\n\npython3 train_predictive_music_model.py --dimension=3 --modelsize=xs --earlystopping\n\n\nBecause we don’t have much data, and have set a very small model size, this shouldn’t take too long.\n\n4. Perform with your model.\n\nNow that you have a trained model, you can run this command to start making predictions:\n\npython3 predictive_music_model.py -d=3 --modelsize=xs --log -c\n\n\nIf your interface software is still running, you should hear it play\nitself! cool!\n\n5. Make this terrible NIME better\n\nThere’s many ways to make this NIME better. You could make the sounds\nnicer, make the visual interface nicer, or just scrap it and start\nagain in an environment of your choice! You could also\n\n6. Try with your own NIME!\n\nThis is a bit more of a project. First of all, what kind of data does\nyour NIME generate? Is it one or many dimensions? Secondly, when are\nthese data generated and available for a synthesiser. Are all\ndimensions sent on independent timescales (like individual keys on a\npiano for instance) or are they all combined into one stream of data?\n\nIMPS can only deal with messages that include EVERY bit of data from a\nNIME, so you might need to work on your code to get your output data into a format\nthat IMPS can handle. There’s some more details on the [IMPS\npage}(/imps/).\n\nOnce you’ve figured out how to get data into IMPS you can log some\ninteraction and train an MDRNN. Great!\n\nSo now you have a trained MDRNN, but what can you do with the\npredictions? WIll it play the same sounds as your regular instrument,\nor different ones? Will be be visualised or physicalised in some\nway? What benefit is there to having this data during a performances?\n\n7. Extend IMPS and use in your own projects!\n\nIMPS is an open source project and I hope that this community\nmakes some cool music with it. It has shortcomings, and it’s\nopinionated, so don’t hesitate to hack it into a predictive musical\nmodel that you want! (or the predictive musical model we deserve??)\n",
        "url": "/hack/imps-demo/"
      },
    
      {
        "title": "Design a Predictive NIME",
        "excerpt": "\n",
        "content": "\n",
        "url": "/hack/design-nime/"
      },
    
      {
        "title": "Hacking on Creative Prediction",
        "excerpt": "Here’s some exercises/projects/tutorials, to get you started with\ncreative prediction.\n\nRemember to check out the setup page to get your\nPython/Keras/Jupyter tools running.\n\n\n  \n    Set up Python, Keras, and Jupyter\n  \n  \n    Try out a Colab Notebook\n  \n  \n    Make your own text generator\n  \n  \n    Make some music with an RNN\n  \n  \n    Make a RoboJam model\n  \n  \n    Try out IMPS, Processing, and Pd\n  \n  \n    Design a Predictive NIME\n  \n\n\nTutorials/Demos\n\n\n  Generating Creative Sequences\n    \n      \n        Generating Text with a CharRNN; inventing Star Trek episode titles. github colab\n      \n      \n        Advanced CharRNN techniques. github colab\n      \n      \n        Continuing musical sequences. github colab\n      \n      \n        Combining multiple sequences in a CharRNN with “Gesture-RNN” (not working right now..). github colab\n      \n    \n  \n  RNNs for continuous-valued data: Mixture Density Layers (MDNs)\n    \n      \n        Mixture distribution examples github colab\n      \n      \n        Introduction to Mixture Density Layers  github colab\n      \n      \n        Predicting sketching: Kanji generation with a Mixture Density RNN. github colab\n      \n      \n        Predicting time and place - musical scribbles with RoboJam. github colab\n      \n    \n  \n  VAE, GAN, and World Models\n    \n      \n        DoomVAE example github colab\n      \n      Doom World Model example (not done yet!)\n      MicroGAN example github colab\n    \n  \n\n\n",
        "content": "Here’s some exercises/projects/tutorials, to get you started with\ncreative prediction.\n\nRemember to check out the setup page to get your\nPython/Keras/Jupyter tools running.\n\n\n  \n    Set up Python, Keras, and Jupyter\n  \n  \n    Try out a Colab Notebook\n  \n  \n    Make your own text generator\n  \n  \n    Make some music with an RNN\n  \n  \n    Make a RoboJam model\n  \n  \n    Try out IMPS, Processing, and Pd\n  \n  \n    Design a Predictive NIME\n  \n\n\nTutorials/Demos\n\n\n  Generating Creative Sequences\n    \n      \n        Generating Text with a CharRNN; inventing Star Trek episode titles. github colab\n      \n      \n        Advanced CharRNN techniques. github colab\n      \n      \n        Continuing musical sequences. github colab\n      \n      \n        Combining multiple sequences in a CharRNN with “Gesture-RNN” (not working right now..). github colab\n      \n    \n  \n  RNNs for continuous-valued data: Mixture Density Layers (MDNs)\n    \n      \n        Mixture distribution examples github colab\n      \n      \n        Introduction to Mixture Density Layers  github colab\n      \n      \n        Predicting sketching: Kanji generation with a Mixture Density RNN. github colab\n      \n      \n        Predicting time and place - musical scribbles with RoboJam. github colab\n      \n    \n  \n  VAE, GAN, and World Models\n    \n      \n        DoomVAE example github colab\n      \n      Doom World Model example (not done yet!)\n      MicroGAN example github colab\n    \n  \n\n\n",
        "url": "/hack/"
      },
    
  
    
    
      {
        "title": "Welcome to Jekyll!",
        "excerpt": "You’ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve, which launches a web server and auto-regenerates your site when a file is updated.\n\nTo add new posts, simply add a file in the _posts directory that follows the convention YYYY-MM-DD-name-of-post.ext and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.\n\nJekyll also offers powerful support for code snippets:\n\ndef print_hi(name)\n  puts \"Hi, #{name}\"\nend\nprint_hi('Tom')\n#=&gt; prints 'Hi, Tom' to STDOUT.\n\nCheck out the Jekyll docs for more info on how to get the most out of Jekyll. File all bugs/feature requests at Jekyll’s GitHub repo. If you have questions, you can ask them on Jekyll Talk.\n\n",
        "content": "You’ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve, which launches a web server and auto-regenerates your site when a file is updated.\n\nTo add new posts, simply add a file in the _posts directory that follows the convention YYYY-MM-DD-name-of-post.ext and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.\n\nJekyll also offers powerful support for code snippets:\n\ndef print_hi(name)\n  puts \"Hi, #{name}\"\nend\nprint_hi('Tom')\n#=&gt; prints 'Hi, Tom' to STDOUT.\n\nCheck out the Jekyll docs for more info on how to get the most out of Jekyll. File all bugs/feature requests at Jekyll’s GitHub repo. If you have questions, you can ask them on Jekyll Talk.\n\n",
        "url": "/jekyll/update/2018/03/03/welcome-to-jekyll/"
      },
    
  
    
    
      {
        "title": "Making Predictive NIMEs with Neural Networks",
        "excerpt": "\n\n\n\n    Creative Prediction with Neural Networks\n    A course in ML/AI for creative expression\n\n\nMaking Predictive NIMEs with Neural Networks\n\nCharles Martin - The Australian National University\n\n\n\t\n\n\n\n\nOutline\n\nWhat kinds of neural networks are useful in NIMEs?\n\nHow do recurrent neural networks work? \n\nHow can we use them to generate creative data? \n\nIs there a more convenient way to do this? \n\nOutline\n\nFocus on hands on experiments\n\nFocus on small data \n\nUse widely available tools \n\nEverybody will train a neural network (?) \n\nRough Plan\n\n\n  9:00 Meeting, Intro, and Python Setup\n  9:15 Overview of Deep Learning and Creativity\n  9:45 Generating Text and Music with RNNs (practical)\n  10:30 (coffee break)\n  10:45 Hack: inventing Star Trek episode titles\n  11:30 Using Mixture Density Networks (MDNs)\n  13:00 Lunch (90 minutes)\n  14:30 Making Predictive Musical Interactions with the IMPS system\n  16:00 (coffee break)\n  16:15 Future directions for creative neural networks at NIME and beyond\n\n\nLearning to Predict Sequences\n\n\n\nRecurrent Neural Networks and LSTM Units\n\n\n\nMixture Density RNNs\n\n\n\nInteracting with Musical Predictions\n\n\n\nStarting a community\n\n\n\nPredictions\n\nMusical Predictions\n\n\n\nWhat are predictions?\n\n\n\nWhere can they fit into a NIME?\n\n\n\nWhere can they fit into a performance?\n\n\n\nExample: Wekinator-style NIMEs\n\n\n\nSource: FlowerPowa74 (YouTube Video)\n\nExample: RoboJam\n\n\n\nRoboJam Demo\n\nExample: Neural iPad Ensemble\n\n\n\nYouTube Video\n\ntalk\n\nwhat kinds of predictions would you like a NIME to make?\n\nTools\n\nPython is powerful.\n\n\n\nWe can all be Python superheroes! \n\nPython setup can be hard.\n\n\n\n(Sorry) \n\nColab to the Rescue?\n\n\n\nextra tools\n\nProcessing\n\nPure Data\n\nOr use whatever computer music environment you want (needs to send and receive OSC messages).\n\nget your tools together\n\nNow’s the time to look at the setup page\non creativeprediction.xyz.\n\nThis course works best with Python, Keras, and Jupyter on your computer, but you\ncan use (online) Google Colab Notebooks as well for most of the\nexamples.\n",
        "content": "\n\n\n\n    Creative Prediction with Neural Networks\n    A course in ML/AI for creative expression\n\n\nMaking Predictive NIMEs with Neural Networks\n\nCharles Martin - The Australian National University\n\n\n\t\n\n\n\n\nOutline\n\nWhat kinds of neural networks are useful in NIMEs?\n\nHow do recurrent neural networks work? \n\nHow can we use them to generate creative data? \n\nIs there a more convenient way to do this? \n\nOutline\n\nFocus on hands on experiments\n\nFocus on small data \n\nUse widely available tools \n\nEverybody will train a neural network (?) \n\nRough Plan\n\n\n  9:00 Meeting, Intro, and Python Setup\n  9:15 Overview of Deep Learning and Creativity\n  9:45 Generating Text and Music with RNNs (practical)\n  10:30 (coffee break)\n  10:45 Hack: inventing Star Trek episode titles\n  11:30 Using Mixture Density Networks (MDNs)\n  13:00 Lunch (90 minutes)\n  14:30 Making Predictive Musical Interactions with the IMPS system\n  16:00 (coffee break)\n  16:15 Future directions for creative neural networks at NIME and beyond\n\n\nLearning to Predict Sequences\n\n\n\nRecurrent Neural Networks and LSTM Units\n\n\n\nMixture Density RNNs\n\n\n\nInteracting with Musical Predictions\n\n\n\nStarting a community\n\n\n\nPredictions\n\nMusical Predictions\n\n\n\nWhat are predictions?\n\n\n\nWhere can they fit into a NIME?\n\n\n\nWhere can they fit into a performance?\n\n\n\nExample: Wekinator-style NIMEs\n\n\n\nSource: FlowerPowa74 (YouTube Video)\n\nExample: RoboJam\n\n\n\nRoboJam Demo\n\nExample: Neural iPad Ensemble\n\n\n\nYouTube Video\n\ntalk\n\nwhat kinds of predictions would you like a NIME to make?\n\nTools\n\nPython is powerful.\n\n\n\nWe can all be Python superheroes! \n\nPython setup can be hard.\n\n\n\n(Sorry) \n\nColab to the Rescue?\n\n\n\nextra tools\n\nProcessing\n\nPure Data\n\nOr use whatever computer music environment you want (needs to send and receive OSC messages).\n\nget your tools together\n\nNow’s the time to look at the setup page\non creativeprediction.xyz.\n\nThis course works best with Python, Keras, and Jupyter on your computer, but you\ncan use (online) Google Colab Notebooks as well for most of the\nexamples.\n",
        "url": "/presentations/intro/"
      },
    
      {
        "title": "Deep Dive on RNNs",
        "excerpt": "\n\n\n\n    Creative Prediction with Neural Networks\n    A course in ML/AI for creative expression\n\n\nDeep Dive on RNNs\n\nCharles Martin - The Australian National University\n\n\n\t\n\n\n\n\n\n  \n  Ngunnawal &amp; Ngambri &amp; Ngarigu Country\n  \n\n\nWhat is an Artificial Neuron?\n\n\n\nSource - Wikimedia Commons\n\nFeed-Forward Network\n\n\nFor each unit: \\(y = \\text{tanh}\\big(Wx + b \\big)\\)\n\nRecurrent Network\n\n\nFor each unit: \\(y_t = \\text{tanh}\\big(Ux_t + Vh_{t-1} + b \\big)\\)\n\nSequence Learning Tasks\n\n\n\n\n\nRecurrent Network\n\n\n\nsimplifying…\n\nRecurrent Network\n\n\n\nsimplifying and rotating…\n\n“State” in Recurrent Networks\n\n\n\n\n  Recurrent Networks are all about storing a “state” in between computations…\n  A “lossy summary of… past sequences”\n  h is the “hidden state” of our RNN\n  What influences h?\n\n\nDefining the RNN State\n\n\n\nWe can define a simplified RNN represented by this diagram as follows:\n\n\\[h_t = \\text{tanh}\\big(Ux_t + Vh_{t-1} + b \\big)\\]\n\n\\[\\hat{y}_t = \\text{softmax}(c + Wh_t)\\]\n\nUnfolding an RNN in Time\n\n\n\n\n  By unfolding the RNN we can compute \\(\\hat{y}\\) for a given length of sequence.\n  Note that the weight matrices \\(U\\), \\(V\\), \\(W\\) are the same for each timestep; this is the big advantage of RNNs!\n\n\nForward Propagation\n\n\n\n\n\nWe can now use the following equations to compute \\(\\hat{y}_t\\), by computing \\(h\\) for the previous steps:\n\\[h_t = \\text{tanh}\\big(Ux_t + Vh_{t-1} + b \\big)\\]\n\\[\\hat{y}_t = \\text{softmax}(c + Wh_t)\\]\n\n\n\nY-hat is Softmax’d\n\n\n\n\\(\\hat{y}\\) is a probability distribution!\n\n\\[\\sigma(\\mathbf{z})_j = \\frac{e^{z_j}}{\\sum_{k=1}^K e^{z_k}} \\text{ for } j = 1,\\ldots, K\\]\n\nCalculating Loss: Categorical Cross Entropy\n\n\n\n\n\nWe use the categorical cross-entropy function for loss:\n\\[\\begin{align*}\nh_t &amp;= \\text{tanh}\\big( {b} + {Vh}_{t-1} + {Ux}_t \\big) \\\\\n\\hat{y}_t &amp;= \\text{softmax}(c + Wh_t) \\\\\nL_t &amp;= -y_t \\cdot \\text{log}(\\hat{y}_t) \\\\\n\\text{Loss} &amp;= \\sum_t L_t \\\\\n\\end{align*}\\]\n\n\n\nBackpropagation Through Time (BPTT)\n\n\n\n\n\nPropagates error correction backwards through the network graph, adjusting all parameters (U, V, W) to minimise loss.\n\n\n\nExample: Character-level text model\n\n\nTraining data: a collection of text.\nInput (X): snippets of 30 characters from the collection.\nTarget output (y): 1 character, the next one after the 30 in each X.\n\n\nTraining the Character-level Model\n\n\n\n\n\n\nTarget: A probability distribution with \\(P(n) = 1\\)\nOutput: A probability distribution over all next letters.\nE.g.: “My cat is named Simon” would lead to X: “My cat is named Simo” and y: “n”\n\n\n\n\nUsing the trained model to generate text\n\n\n\n\n\n\nS: Sampling function, sample a letter using the output probability distribution.\nThe generated letter is reinserted at as the next input.\nWe don’t want to always draw the most likely character. The would give frequent repetition and “copying” from the training text. Need a sampling strategy.\n\n\n\n\nChar-RNN\n\n\n\n\nRNN as a sequence generator\nInput is current symbol, output is next predicted symbol.\nConnect output to input and continue!\nCharRNN simply applies this to a (subset) of ASCII characters.\nTrain and generate on any text corpus: Fun!\n\n\n\nSee: Karpathy, A. (2015). The unreasonable effectiveness of recurrent neural networks.\n\n\n\nChar-RNN Examples\n\n\n\nShakespeare (Karpathy, 2015):\n\nSecond Senator: They are away this miseries, produced upon my soul, Breaking and strongly should be buried, when I perish The earth and thoughts of many states.\nDUKE VINCENTIO: Well, your wit is in the care of side and that.\n\n\nLatex Algebraic Geometry:\n\nN.B. “Proof. Omitted.” Lol.\n\n\n\nTime to Hack\n\nMaking an RNN that generates Star Trek titles\n\nopen in Colab\n\n\n\nRNN Architectures and LSTM\n\nBidirectional RNNs\n\n\n\n\n\n\nUseful for tasks where the whole sequence is available.\nEach output unit (\\(\\hat{y}\\)) depends on both past and future - but most sensitive to closer times.\nPopular in speech recognition, translation etc.\n\n\n\n\nEncoder-Decoder (seq-to-seq)\n\n\nLearns to generate output sequence (y) from an input sequence (x).\nFinal hidden state of encoder is used to compute a context variable C.\nFor example, translation.\n\nDeep RNNs\n\n\n\n\n\n\nDoes adding deeper layers to an RNN make it work better?\nSeveral options for architecture.\nSimply stacking RNN layers is very popular; shown to work better by Graves et al. (2013)\nIntuitively: layers might learn some hierarchical knowledge automatically.\nTypical setup: up to three recurrent layers.\n\n\n\n\nLong-Term Dependencies\n\n\n\n\nLearning long dependencies is a mathematical challenge.\nBasically: gradients propagated through the same weights tend to vanish (mostly) or explode (rarely)\nE.g., consider a simplified RNN with no nonlinear activation function or input.\nEach time step multiplies h(0) by W.\nThis corresponds to raising power of eigenvalues in \\(\\Lambda\\).\nEventually, components of h(0) not aligned with the largest eigenvector will be discarded.\n\n\n\\[\\begin{align*}\nh_t &amp;= Wh_{t-1}\\\\\nh_t &amp;= (W^t)h_0\n\\end{align*}\\]\n(supposing W admits eigendecomposition with orthogonal matrix Q)\n\\[\\begin{align*}\nW &amp;= Q\\Lambda Q^{\\top}\\\\\nh_t &amp;= Q\\Lambda ^t Qh_0\n\\end{align*}\\]\n\n\n\nVanishing and Exploding Gradients\n\n\n\n\n“in order to store memories in a way that is robust to small perturbations, the RNN must enter a region of parameter space where gradients vanish”\n“whenever the model is able to represent long term dependencies, the gradient of a long term interaction has exponentially smaller magnitude than the gradient of a short term interaction.”\n\n\n\nNote that this problem is only relevant for recurrent networks since the weights W affecting the hidden state are the same at each time step.\nGoodfellow and Benigo (2016): “the problem of learning long-term dependencies remains one of the main challenges in deep learning”\nWildML (2015). Backpropagation Through Time and Vanishing Gradients\nML for artists\n\n\n\n\nGated RNNs\n\n\n\n\n\n\n  Provide gates that can change the hidden state a little bit at each step.\n  The gates are controlled by learnable weights as well!\n  Hidden state weights that may change at each time step.\n  Create paths through time with derivatives that do not vanish/explode.\n  Gates choose information to accumulate or forget at each step.\n\n\n\n\nLong Short-Term Memory\n\n\n\n\nSelf-loop containing internal state (c).\nThree extra gating units:\n\nForget gate: controls how much memory is preserved.\nInput gate: control how much of current input is stored.\nOutput gate: control how much of state is shown to output.\n\nEach gate has own weights and biases, so this uses lots more parameters.\n\n\n\n\n\n\n\nOther Gating Units\n\n\n\n  \n    Source: (Olah, C. 2015.)\n\n\nAre three gates necessary?\nOther gating units are simpler, e.g., Gated Recurrent Unit (GRU)\nFor the moment, LSTMs are winning in practical use.\nAlternative unit design: project idea?\n\n\n\n\nVisualising LSTM activations\n\nSometimes, the LSTM cell state corresponds with features of the sequential data:\n\nSource: (Karpathy, 2015)\n\nCharRNN Applications: FolkRNN\n\nSome kinds of music can be represented in a text-like manner.\n\nSource: Sturm et al. 2015. Folk Music Style Modelling by Recurrent Neural Networks with Long Short Term Memory Units\n\nOther CharRNN Applications\n\n\n  \n    \n Teaching Recurrent Neural Networks about Monet\n  \n    \n New Paint Colours Invented by Neural Network\n\n\n\nGoogle Magenta Performance RNN\n\n\n\n\n\n\n\n\n\nState-of-the-art in music generating RNNs.\nEncode MIDI musical sequences as categorical data.\nNow supports polyphony (multiple notes), dynamics (volume), expressive timing (rubato).\nE.g.: YouTube demo\n\n\nNeural iPad Band, another CharRNN\n\n\n\n  \n    \n    \n  \n\n\niPad music transcribed as sequence of numbers for each performer.\nTrick: encode multiple ints as one (preserving ordering).\nVideo\n\n\n\n\nBooks and Learning References\n\nIan Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press.\n\nFrançois Chollet. 2018. Manning.\n\nChris Olah. 2015. Understanding LSTMs\n\nRNNs in Tensorflow\n\nMaybe RNN/LSTM is dead? CNNs can work similarly to BLSTMs\n\nKarpathy. 2015. The Unreasonable Effectiveness of RNNs\n\nFoster. 2019. Generative Deep Learning: Teaching Machines to Paint, Write, Compose, and Play\n\nTime to Hack\n\nThese examples run in Google Colaboratory, just click the link to start them up.\n\nStar Trek RNN (open in Colab)\n\nAdvanced CharRNN (open in Colab)\n\nMelody Generation (open in Colab)\n\nSummary\n\n\n  Recurrent Neural Networks let us capture and model the structure of sequential data.\n  Sampling from trained RNNs allow us to generate new, creative sequences.\n  The internal state of RNNs make them interesting for interactive applications, since it lets them capture and continue from the current context or “style”.\n  LSTM units are able to overcome the vanishing gradient problem to some extent.\n\n\n",
        "content": "\n\n\n\n    Creative Prediction with Neural Networks\n    A course in ML/AI for creative expression\n\n\nDeep Dive on RNNs\n\nCharles Martin - The Australian National University\n\n\n\t\n\n\n\n\n\n  \n  Ngunnawal &amp; Ngambri &amp; Ngarigu Country\n  \n\n\nWhat is an Artificial Neuron?\n\n\n\nSource - Wikimedia Commons\n\nFeed-Forward Network\n\n\nFor each unit: \\(y = \\text{tanh}\\big(Wx + b \\big)\\)\n\nRecurrent Network\n\n\nFor each unit: \\(y_t = \\text{tanh}\\big(Ux_t + Vh_{t-1} + b \\big)\\)\n\nSequence Learning Tasks\n\n\n\n\n\nRecurrent Network\n\n\n\nsimplifying…\n\nRecurrent Network\n\n\n\nsimplifying and rotating…\n\n“State” in Recurrent Networks\n\n\n\n\n  Recurrent Networks are all about storing a “state” in between computations…\n  A “lossy summary of… past sequences”\n  h is the “hidden state” of our RNN\n  What influences h?\n\n\nDefining the RNN State\n\n\n\nWe can define a simplified RNN represented by this diagram as follows:\n\n\\[h_t = \\text{tanh}\\big(Ux_t + Vh_{t-1} + b \\big)\\]\n\n\\[\\hat{y}_t = \\text{softmax}(c + Wh_t)\\]\n\nUnfolding an RNN in Time\n\n\n\n\n  By unfolding the RNN we can compute \\(\\hat{y}\\) for a given length of sequence.\n  Note that the weight matrices \\(U\\), \\(V\\), \\(W\\) are the same for each timestep; this is the big advantage of RNNs!\n\n\nForward Propagation\n\n\n\n\n\nWe can now use the following equations to compute \\(\\hat{y}_t\\), by computing \\(h\\) for the previous steps:\n\\[h_t = \\text{tanh}\\big(Ux_t + Vh_{t-1} + b \\big)\\]\n\\[\\hat{y}_t = \\text{softmax}(c + Wh_t)\\]\n\n\n\nY-hat is Softmax’d\n\n\n\n\\(\\hat{y}\\) is a probability distribution!\n\n\\[\\sigma(\\mathbf{z})_j = \\frac{e^{z_j}}{\\sum_{k=1}^K e^{z_k}} \\text{ for } j = 1,\\ldots, K\\]\n\nCalculating Loss: Categorical Cross Entropy\n\n\n\n\n\nWe use the categorical cross-entropy function for loss:\n\\[\\begin{align*}\nh_t &amp;= \\text{tanh}\\big( {b} + {Vh}_{t-1} + {Ux}_t \\big) \\\\\n\\hat{y}_t &amp;= \\text{softmax}(c + Wh_t) \\\\\nL_t &amp;= -y_t \\cdot \\text{log}(\\hat{y}_t) \\\\\n\\text{Loss} &amp;= \\sum_t L_t \\\\\n\\end{align*}\\]\n\n\n\nBackpropagation Through Time (BPTT)\n\n\n\n\n\nPropagates error correction backwards through the network graph, adjusting all parameters (U, V, W) to minimise loss.\n\n\n\nExample: Character-level text model\n\n\nTraining data: a collection of text.\nInput (X): snippets of 30 characters from the collection.\nTarget output (y): 1 character, the next one after the 30 in each X.\n\n\nTraining the Character-level Model\n\n\n\n\n\n\nTarget: A probability distribution with \\(P(n) = 1\\)\nOutput: A probability distribution over all next letters.\nE.g.: “My cat is named Simon” would lead to X: “My cat is named Simo” and y: “n”\n\n\n\n\nUsing the trained model to generate text\n\n\n\n\n\n\nS: Sampling function, sample a letter using the output probability distribution.\nThe generated letter is reinserted at as the next input.\nWe don’t want to always draw the most likely character. The would give frequent repetition and “copying” from the training text. Need a sampling strategy.\n\n\n\n\nChar-RNN\n\n\n\n\nRNN as a sequence generator\nInput is current symbol, output is next predicted symbol.\nConnect output to input and continue!\nCharRNN simply applies this to a (subset) of ASCII characters.\nTrain and generate on any text corpus: Fun!\n\n\n\nSee: Karpathy, A. (2015). The unreasonable effectiveness of recurrent neural networks.\n\n\n\nChar-RNN Examples\n\n\n\nShakespeare (Karpathy, 2015):\n\nSecond Senator: They are away this miseries, produced upon my soul, Breaking and strongly should be buried, when I perish The earth and thoughts of many states.\nDUKE VINCENTIO: Well, your wit is in the care of side and that.\n\n\nLatex Algebraic Geometry:\n\nN.B. “Proof. Omitted.” Lol.\n\n\n\nTime to Hack\n\nMaking an RNN that generates Star Trek titles\n\nopen in Colab\n\n\n\nRNN Architectures and LSTM\n\nBidirectional RNNs\n\n\n\n\n\n\nUseful for tasks where the whole sequence is available.\nEach output unit (\\(\\hat{y}\\)) depends on both past and future - but most sensitive to closer times.\nPopular in speech recognition, translation etc.\n\n\n\n\nEncoder-Decoder (seq-to-seq)\n\n\nLearns to generate output sequence (y) from an input sequence (x).\nFinal hidden state of encoder is used to compute a context variable C.\nFor example, translation.\n\nDeep RNNs\n\n\n\n\n\n\nDoes adding deeper layers to an RNN make it work better?\nSeveral options for architecture.\nSimply stacking RNN layers is very popular; shown to work better by Graves et al. (2013)\nIntuitively: layers might learn some hierarchical knowledge automatically.\nTypical setup: up to three recurrent layers.\n\n\n\n\nLong-Term Dependencies\n\n\n\n\nLearning long dependencies is a mathematical challenge.\nBasically: gradients propagated through the same weights tend to vanish (mostly) or explode (rarely)\nE.g., consider a simplified RNN with no nonlinear activation function or input.\nEach time step multiplies h(0) by W.\nThis corresponds to raising power of eigenvalues in \\(\\Lambda\\).\nEventually, components of h(0) not aligned with the largest eigenvector will be discarded.\n\n\n\\[\\begin{align*}\nh_t &amp;= Wh_{t-1}\\\\\nh_t &amp;= (W^t)h_0\n\\end{align*}\\]\n(supposing W admits eigendecomposition with orthogonal matrix Q)\n\\[\\begin{align*}\nW &amp;= Q\\Lambda Q^{\\top}\\\\\nh_t &amp;= Q\\Lambda ^t Qh_0\n\\end{align*}\\]\n\n\n\nVanishing and Exploding Gradients\n\n\n\n\n“in order to store memories in a way that is robust to small perturbations, the RNN must enter a region of parameter space where gradients vanish”\n“whenever the model is able to represent long term dependencies, the gradient of a long term interaction has exponentially smaller magnitude than the gradient of a short term interaction.”\n\n\n\nNote that this problem is only relevant for recurrent networks since the weights W affecting the hidden state are the same at each time step.\nGoodfellow and Benigo (2016): “the problem of learning long-term dependencies remains one of the main challenges in deep learning”\nWildML (2015). Backpropagation Through Time and Vanishing Gradients\nML for artists\n\n\n\n\nGated RNNs\n\n\n\n\n\n\n  Provide gates that can change the hidden state a little bit at each step.\n  The gates are controlled by learnable weights as well!\n  Hidden state weights that may change at each time step.\n  Create paths through time with derivatives that do not vanish/explode.\n  Gates choose information to accumulate or forget at each step.\n\n\n\n\nLong Short-Term Memory\n\n\n\n\nSelf-loop containing internal state (c).\nThree extra gating units:\n\nForget gate: controls how much memory is preserved.\nInput gate: control how much of current input is stored.\nOutput gate: control how much of state is shown to output.\n\nEach gate has own weights and biases, so this uses lots more parameters.\n\n\n\n\n\n\n\nOther Gating Units\n\n\n\n  \n    Source: (Olah, C. 2015.)\n\n\nAre three gates necessary?\nOther gating units are simpler, e.g., Gated Recurrent Unit (GRU)\nFor the moment, LSTMs are winning in practical use.\nAlternative unit design: project idea?\n\n\n\n\nVisualising LSTM activations\n\nSometimes, the LSTM cell state corresponds with features of the sequential data:\n\nSource: (Karpathy, 2015)\n\nCharRNN Applications: FolkRNN\n\nSome kinds of music can be represented in a text-like manner.\n\nSource: Sturm et al. 2015. Folk Music Style Modelling by Recurrent Neural Networks with Long Short Term Memory Units\n\nOther CharRNN Applications\n\n\n  \n    \n Teaching Recurrent Neural Networks about Monet\n  \n    \n New Paint Colours Invented by Neural Network\n\n\n\nGoogle Magenta Performance RNN\n\n\n\n\n\n\n\n\n\nState-of-the-art in music generating RNNs.\nEncode MIDI musical sequences as categorical data.\nNow supports polyphony (multiple notes), dynamics (volume), expressive timing (rubato).\nE.g.: YouTube demo\n\n\nNeural iPad Band, another CharRNN\n\n\n\n  \n    \n    \n  \n\n\niPad music transcribed as sequence of numbers for each performer.\nTrick: encode multiple ints as one (preserving ordering).\nVideo\n\n\n\n\nBooks and Learning References\n\nIan Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press.\n\nFrançois Chollet. 2018. Manning.\n\nChris Olah. 2015. Understanding LSTMs\n\nRNNs in Tensorflow\n\nMaybe RNN/LSTM is dead? CNNs can work similarly to BLSTMs\n\nKarpathy. 2015. The Unreasonable Effectiveness of RNNs\n\nFoster. 2019. Generative Deep Learning: Teaching Machines to Paint, Write, Compose, and Play\n\nTime to Hack\n\nThese examples run in Google Colaboratory, just click the link to start them up.\n\nStar Trek RNN (open in Colab)\n\nAdvanced CharRNN (open in Colab)\n\nMelody Generation (open in Colab)\n\nSummary\n\n\n  Recurrent Neural Networks let us capture and model the structure of sequential data.\n  Sampling from trained RNNs allow us to generate new, creative sequences.\n  The internal state of RNNs make them interesting for interactive applications, since it lets them capture and continue from the current context or “style”.\n  LSTM units are able to overcome the vanishing gradient problem to some extent.\n\n\n",
        "url": "/presentations/deep-dive-on-rnns/"
      },
    
      {
        "title": "Mixture Density Networks",
        "excerpt": "\n\n\n\n    Creative Prediction with Neural Networks\n    A course in ML/AI for creative expression\n\n\nMixture Density Networks\n\nCharles Martin - The Australian National University\n\n\n\t\n\n\n\n\n\nSo far; RNNs that Model Categorical Data\n\n\n\nRemember that most RNNs (and most deep learning models) end with a softmax layer.\nThis layer outputs a probability distribution for a set of categorical predictions.\nE.g.:\n\nimage labels,\nletters, words,\nmusical notes,\nrobot commands,\nmoves in chess.\n\n\n\n  \n    \n    \n  \n\n\n\n\n\nExpressive Data is Often Continuous\n\n\n\n\n\n\n\n\n\n\n\n  So are Bio-Signals\n  \n\n\n  \n\nImage Credit: Wikimedia\n\n\n\nCategorical vs. Continuous Models\n\n\n\n\n\n\n\n\n\n\nNormal (Gaussian) Distribution\n\n\n\nThe “Standard” probability distribution\nHas two parameters:\n\nmean (\\(\\mu\\)) and\nstandard deviation (\\(\\sigma\\))\n\nProbability Density Function:\n\\[\\mathcal{N}(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2} } e^{ -\\frac{(x-\\mu)^2}{2\\sigma^2} }\\]\n&lt;/div&gt;\n\n\n&lt;/div&gt;\n&lt;/section&gt;\n\n\nProblem: Normal distribution might not fit data\n\n\nWhat if the data is complicated?\nIt’s easy to “fit” a normal model to any data.\nJust calculate \\(\\mu\\) and \\(\\sigma\\)\n(might not fit the data well)\n\n\n\n\n\n\n\n\nMixture of Normals\n\n\nThree groups of parameters:\n\nmeans (\\(\\boldsymbol\\mu\\)): location of each component\nstandard deviations (\\(\\boldsymbol\\sigma\\)): width of each component\nWeight (\\(\\boldsymbol\\pi\\)): height of each curve\n\nProbability Density Function:\n\\[p(x) = \\sum_{i=1}^K \\pi_i\\mathcal{N}(x \\mid \\mu, \\sigma^2)\\]\n\n\n\n\n\n\n\n\nThis solves our problem:\n\n\nReturning to our modelling problem, let’s plot the PDF of a evenly-weighted mixture of the two sample normal models.\nWe set:\n\n\\(K = 2\\)\n\\(\\boldsymbol\\pi = [0.5, 0.5]\\)\n\\(\\boldsymbol\\mu = [-5, 5]\\)\n\\(\\boldsymbol\\sigma = [2, 3]\\)\n(bold used to indicate the vector of parameters for each component)\n\n\n  \n  In this case, I knew the right parameters, but normally you would have to estimate, or learn, these somehow…\n\n\n\n\n\nMixture Density Networks\n\n\nNeural networks used to model complicated real-valued data.\ni.e., data that might not be very “normal”\nUsual approach: use a neuron with linear activation to make predictions.\nTraining function could be MSE (mean squared error).\nProblem! This is equivalent to fitting to a single normal model!\n(See Bishop, C (1994) for proof and more details)\n\n\n\n\nMixture Density Networks\n\n\nIdea: output parameters of a mixture model instead!\nRather than MSE for training, use the PDF of the mixture model.\nNow network can model complicated distributions! 😌\n\n\n\n\nSimple Example in Keras\n\nDifficult data is not hard to find! Think about modelling an inverse sine (arcsine) function.\n\n   input value takes multiple outputs…\n   is not going to go well for a single normal model.\n\n\n\n\n  Feedforward MSE Network\n  Simple two-hidden-layer network (286 parameters):\n  \nmodel = Sequential()\nmodel.add(Dense(15, batch_input_shape=(None, 1), activation='tanh'))\nmodel.add(Dense(15, activation='tanh'))\nmodel.add(Dense(1, activation='linear'))\nmodel.compile(loss='mse', optimizer='rmsprop')\nmodel.fit(x=x_data, y=y_data, batch_size=128, epochs=200, validation_split=0.15)\n  \n\n\n\n  Feedforward MSE Network (Result)\n  Simple two-hidden-layer network (286 parameters):\n\n\n\n\nMDN Architecture:\n\nLoss function for MDN is negative log of likelihood function \\(\\mathcal{L}\\).\n\\(\\mathcal{L}\\) measures likelihood of \\(t\\) being drawn from a mixture parametrised by \\(\\mu\\), \\(\\sigma\\), and \\(\\pi\\) which are generated by the network inputs \\(x\\):\n\\[\\mathcal{L} = \\sum_{i=1}^K\\pi_i(\\mathbf{x})\\mathcal{N}\\bigl(\\mu_i(\\mathbf{x}), \\sigma_i^2(\\mathbf{x}); \\mathbf{t} \\bigr)\\]\n\n\n\nFeedforward MDN Solution\nTwo-hidden-layer MDN (510 parameters)---code snippet:\n\nN_MIXES = 5\nmodel = Sequential()\nmodel.add(Dense(15, batch_input_shape=(None, 1), activation='relu'))\nmodel.add(Dense(15, activation='relu'))\nmodel.add(mdn.MDN(1, N_MIXES)) # here's the MDN layer!\nmodel.compile(loss=mdn.get_mixture_loss_func(1,N_MIXES), optimizer='rmsprop')\nmodel.summary()\n\n\n\n\n  Feedforward MDN Results\n  Two-hidden-layer MDN (510 parameters)---works much better!\n \n\n\n\nGetting inside the MDN layer\n\ndef elu_plus_one_plus_epsilon(x):\n    return (K.elu(x) + 1 + 1e-8)\n\nN_HIDDEN = 15; N_MIXES = 5\ninputs = Input(shape=(1,), name='inputs')\nhidden1 = Dense(N_HIDDEN, activation='relu', name='hidden1')(inputs)\nhidden2 = Dense(N_HIDDEN, activation='relu', name='hidden2')(hidden1)\nmdn_mus = Dense(N_MIXES, name='mdn_mus')(hidden2)\nmdn_sigmas = Dense(N_MIXES, activation=elu_plus_one_plus_epsilon, name='mdn_sigmas')(hidden2)\nmdn_pi = Dense(N_MIXES, name='mdn_pi')(hidden2)\nmdn_out = Concatenate(name='mdn_outputs')([mdn_mus, mdn_sigmas, mdn_pi])\nmodel = Model(inputs=inputs, outputs=mdn_out)\n\n\n\n\nLoss Function: The Tricky Bit.\nLoss function for the MDN should be the negative log likelihood:\n\ndef mdn_loss(y_true, y_pred):\n    # Split the inputs into paramaters\n    out_mu, out_sigma, out_pi = tf.split(y_pred, num_or_size_splits=[N_MIXES, N_MIXES, N_MIXES],\n                                         axis=-1, name='mdn_coef_split')\n    mus = tf.split(out_mu, num_or_size_splits=N_MIXES, axis=1)\n    sigs = tf.split(out_sigma, num_or_size_splits=N_MIXES, axis=1)\n    # Construct the mixture models\n    cat = tfd.Categorical(logits=out_pi)\n    coll = [tfd.MultivariateNormalDiag(loc=loc, scale_diag=scale) for loc, scale\n            in zip(mus, sigs)]\n    mixture = tfd.Mixture(cat=cat, components=coll)\n    # Calculate the loss function\n    loss = mixture.log_prob(y_true)\n    loss = tf.negative(loss)\n    loss = tf.reduce_mean(loss)\n    return loss\n\nmodel.compile(loss=mdn_loss, optimizer='rmsprop')\n\nLet’s go through bit by bit…\n\n\n\nLoss Function: Part 1:\nFirst we have to extract the mixture paramaters.\n\n# Split the inputs into paramaters\nout_mu, out_sigma, out_pi = tf.split(y_pred, num_or_size_splits=[N_MIXES, N_MIXES, N_MIXES],\n                                     axis=-1, name='mdn_coef_split')\nmus = tf.split(out_mu, num_or_size_splits=N_MIXES, axis=1)\nsigs = tf.split(out_sigma, num_or_size_splits=N_MIXES, axis=1)\n\n\nSplit up the parameters \\(\\boldsymbol\\mu\\), \\(\\boldsymbol\\sigma\\), and \\(\\boldsymbol\\pi\\), remember that there are N_MIXES \\(= K\\) of each of these.\n\\(\\boldsymbol\\mu\\) and \\(\\boldsymbol\\sigma\\) have to be split again so that we can iterate over them (you can’t iterate over an axis of a tensor…)\n\n\n\n\nLoss Function: Part 2:\nNow we have to construct the mixture model’s PDF.\n\n# Construct the mixture models\ncat = tfd.Categorical(logits=out_pi) \ncoll = [tfd.Normal(loc=loc, scale=scale) for loc, scale\n        in zip(mus, sigs)]\nmixture = tfd.Mixture(cat=cat, components=coll)\n\n\nFor this, we’re using the Mixture abstraction provided in tensorflow-probability.distributions.\nThis takes a categorical (a.k.a. softmax, a.k.a. generalized Bernoulli distribution) model, and a list the component distributions.\nEach normal PDF is contructed using tfd.Normal.\nCan do this from first principles as well, but good to use abstractions that are available (?)\n\n\n\n\nLoss Function: Part 3:\nFinally, we calculate the loss:\n\nloss = mixture.log_prob(y_true)\nloss = tf.negative(loss)\nloss = tf.reduce_mean(loss)\n\n\nmixture.log_prob(y_true) means “the log-likelihood of sampling y_true from the distribution called mixture.”\n\n\n\n\nSome more details….\n\n\nThis “version” of a mixture model works for a mixture of 1D normal distributions.\nNot too hard to extend to multivariate normal distributions, which are useful for lots of problems.\nThis is how it actually works in my Keras MDN layer, have a look at the code for more details…\n\n\n\n\nMDN-RNNs\n\nMDNs can be handy at the end of an RNN! Imagine a robot calculating moves forward through space, it might have to choose from a number of valid positions, each of which could be modelled by a 2D Normal model.\n\n\n\nMDN-RNN Architecture\n\nCan be as simple as putting an MDN layer after recurrent layers!\n\n\n\nUse Cases: Handwriting Generation\n\n\n\nHandwriting Generation RNN (Graves, 2013).\nTrained on handwriting data.\nPredicts the next location of the pen (\\(dx\\), \\(dy\\), and up/down)\nNetwork takes text to write as an extra input, RNN learns to decide what character to write next.\n\n\n\n\nUse Cases: SketchRNN\n \n\nSketchRNN Kanji (Ha, 2015); similar to handwriting generation, trained on kanji and then generates new “fake” characters\nSketchRNN VAE (Ha et al., 2017); similar again, but trained on human-sourced sketches. VAE architecture with bidirectional RNN encoder and MDN in the decoder part.\n\n\n\n\nUse Cases: RoboJam\n\n\n\n\n\n\nRoboJam (Martin et al., 2018); similar to the kanji RNN, but trained on touchscreen musical performances\nExtra complexity: have to model touch position (\\(x\\), \\(y\\)) and time (\\(dt\\)).\nImplemented in my MicroJam app (have a go: microjam.info)\n\n\n\n\n\n\nUse Cases: World Models\n\n\n\nWorld Models (Ha &amp; Schmidhuber, 2018)\nTrain a VAE for visual perception an environment (e.g., VizDoom), now each frame from the environment can be represented by a vector \\(z\\)\nTrain MDN to predict next \\(z\\), use this to help train an agent to operate in the environment.\n\n\n  \n  \n\n\n\n\n\nReferences\n\nChristopher M. Bishop. 1994. Mixture Density Networks. Technical Report NCRG/94/004. Neural Computing Research Group, Aston University.\nAxel Brando. 2017. Mixture Density Networks (MDN) for distribution and uncertainty estimation. Master’s thesis. Universitat Politècnica de Catalunya.\nA. Graves. 2013. Generating Sequences With Recurrent Neural Networks. ArXiv e-prints (Aug. 2013). ArXiv:1308.0850\nDavid Ha and Douglas Eck. 2017. A Neural Representation of Sketch Drawings. ArXiv e-prints (April 2017). ArXiv:1704.03477\nCharles P. Martin and Jim Torresen. 2018. RoboJam: A Musical Mixture Density Network for Collaborative Touchscreen Interaction. In Evolutionary and Biologically Inspired Music, Sound, Art and Design: EvoMUSART ’18, A. Liapis et al. (Ed.). Lecture Notes in Computer Science, Vol. 10783. Springer International Publishing. DOI:10.1007/9778-3-319-77583-8_11\nD. Ha and J. Schmidhuber. 2018. Recurrent World Models Facilitate Policy Evolution. ArXiv e-prints (Sept. 2018). ArXiv:1809.01999\n\n\n\n",
        "content": "\n\n\n\n    Creative Prediction with Neural Networks\n    A course in ML/AI for creative expression\n\n\nMixture Density Networks\n\nCharles Martin - The Australian National University\n\n\n\t\n\n\n\n\n\nSo far; RNNs that Model Categorical Data\n\n\n\nRemember that most RNNs (and most deep learning models) end with a softmax layer.\nThis layer outputs a probability distribution for a set of categorical predictions.\nE.g.:\n\nimage labels,\nletters, words,\nmusical notes,\nrobot commands,\nmoves in chess.\n\n\n\n  \n    \n    \n  \n\n\n\n\n\nExpressive Data is Often Continuous\n\n\n\n\n\n\n\n\n\n\n\n  So are Bio-Signals\n  \n\n\n  \n\nImage Credit: Wikimedia\n\n\n\nCategorical vs. Continuous Models\n\n\n\n\n\n\n\n\n\n\nNormal (Gaussian) Distribution\n\n\n\nThe “Standard” probability distribution\nHas two parameters:\n\nmean (\\(\\mu\\)) and\nstandard deviation (\\(\\sigma\\))\n\nProbability Density Function:\n\\[\\mathcal{N}(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2} } e^{ -\\frac{(x-\\mu)^2}{2\\sigma^2} }\\]\n&lt;/div&gt;\n\n\n&lt;/div&gt;\n&lt;/section&gt;\n\n\nProblem: Normal distribution might not fit data\n\n\nWhat if the data is complicated?\nIt’s easy to “fit” a normal model to any data.\nJust calculate \\(\\mu\\) and \\(\\sigma\\)\n(might not fit the data well)\n\n\n\n\n\n\n\n\nMixture of Normals\n\n\nThree groups of parameters:\n\nmeans (\\(\\boldsymbol\\mu\\)): location of each component\nstandard deviations (\\(\\boldsymbol\\sigma\\)): width of each component\nWeight (\\(\\boldsymbol\\pi\\)): height of each curve\n\nProbability Density Function:\n\\[p(x) = \\sum_{i=1}^K \\pi_i\\mathcal{N}(x \\mid \\mu, \\sigma^2)\\]\n\n\n\n\n\n\n\n\nThis solves our problem:\n\n\nReturning to our modelling problem, let’s plot the PDF of a evenly-weighted mixture of the two sample normal models.\nWe set:\n\n\\(K = 2\\)\n\\(\\boldsymbol\\pi = [0.5, 0.5]\\)\n\\(\\boldsymbol\\mu = [-5, 5]\\)\n\\(\\boldsymbol\\sigma = [2, 3]\\)\n(bold used to indicate the vector of parameters for each component)\n\n\n  \n  In this case, I knew the right parameters, but normally you would have to estimate, or learn, these somehow…\n\n\n\n\n\nMixture Density Networks\n\n\nNeural networks used to model complicated real-valued data.\ni.e., data that might not be very “normal”\nUsual approach: use a neuron with linear activation to make predictions.\nTraining function could be MSE (mean squared error).\nProblem! This is equivalent to fitting to a single normal model!\n(See Bishop, C (1994) for proof and more details)\n\n\n\n\nMixture Density Networks\n\n\nIdea: output parameters of a mixture model instead!\nRather than MSE for training, use the PDF of the mixture model.\nNow network can model complicated distributions! 😌\n\n\n\n\nSimple Example in Keras\n\nDifficult data is not hard to find! Think about modelling an inverse sine (arcsine) function.\n\n   input value takes multiple outputs…\n   is not going to go well for a single normal model.\n\n\n\n\n  Feedforward MSE Network\n  Simple two-hidden-layer network (286 parameters):\n  \nmodel = Sequential()\nmodel.add(Dense(15, batch_input_shape=(None, 1), activation='tanh'))\nmodel.add(Dense(15, activation='tanh'))\nmodel.add(Dense(1, activation='linear'))\nmodel.compile(loss='mse', optimizer='rmsprop')\nmodel.fit(x=x_data, y=y_data, batch_size=128, epochs=200, validation_split=0.15)\n  \n\n\n\n  Feedforward MSE Network (Result)\n  Simple two-hidden-layer network (286 parameters):\n\n\n\n\nMDN Architecture:\n\nLoss function for MDN is negative log of likelihood function \\(\\mathcal{L}\\).\n\\(\\mathcal{L}\\) measures likelihood of \\(t\\) being drawn from a mixture parametrised by \\(\\mu\\), \\(\\sigma\\), and \\(\\pi\\) which are generated by the network inputs \\(x\\):\n\\[\\mathcal{L} = \\sum_{i=1}^K\\pi_i(\\mathbf{x})\\mathcal{N}\\bigl(\\mu_i(\\mathbf{x}), \\sigma_i^2(\\mathbf{x}); \\mathbf{t} \\bigr)\\]\n\n\n\nFeedforward MDN Solution\nTwo-hidden-layer MDN (510 parameters)---code snippet:\n\nN_MIXES = 5\nmodel = Sequential()\nmodel.add(Dense(15, batch_input_shape=(None, 1), activation='relu'))\nmodel.add(Dense(15, activation='relu'))\nmodel.add(mdn.MDN(1, N_MIXES)) # here's the MDN layer!\nmodel.compile(loss=mdn.get_mixture_loss_func(1,N_MIXES), optimizer='rmsprop')\nmodel.summary()\n\n\n\n\n  Feedforward MDN Results\n  Two-hidden-layer MDN (510 parameters)---works much better!\n \n\n\n\nGetting inside the MDN layer\n\ndef elu_plus_one_plus_epsilon(x):\n    return (K.elu(x) + 1 + 1e-8)\n\nN_HIDDEN = 15; N_MIXES = 5\ninputs = Input(shape=(1,), name='inputs')\nhidden1 = Dense(N_HIDDEN, activation='relu', name='hidden1')(inputs)\nhidden2 = Dense(N_HIDDEN, activation='relu', name='hidden2')(hidden1)\nmdn_mus = Dense(N_MIXES, name='mdn_mus')(hidden2)\nmdn_sigmas = Dense(N_MIXES, activation=elu_plus_one_plus_epsilon, name='mdn_sigmas')(hidden2)\nmdn_pi = Dense(N_MIXES, name='mdn_pi')(hidden2)\nmdn_out = Concatenate(name='mdn_outputs')([mdn_mus, mdn_sigmas, mdn_pi])\nmodel = Model(inputs=inputs, outputs=mdn_out)\n\n\n\n\nLoss Function: The Tricky Bit.\nLoss function for the MDN should be the negative log likelihood:\n\ndef mdn_loss(y_true, y_pred):\n    # Split the inputs into paramaters\n    out_mu, out_sigma, out_pi = tf.split(y_pred, num_or_size_splits=[N_MIXES, N_MIXES, N_MIXES],\n                                         axis=-1, name='mdn_coef_split')\n    mus = tf.split(out_mu, num_or_size_splits=N_MIXES, axis=1)\n    sigs = tf.split(out_sigma, num_or_size_splits=N_MIXES, axis=1)\n    # Construct the mixture models\n    cat = tfd.Categorical(logits=out_pi)\n    coll = [tfd.MultivariateNormalDiag(loc=loc, scale_diag=scale) for loc, scale\n            in zip(mus, sigs)]\n    mixture = tfd.Mixture(cat=cat, components=coll)\n    # Calculate the loss function\n    loss = mixture.log_prob(y_true)\n    loss = tf.negative(loss)\n    loss = tf.reduce_mean(loss)\n    return loss\n\nmodel.compile(loss=mdn_loss, optimizer='rmsprop')\n\nLet’s go through bit by bit…\n\n\n\nLoss Function: Part 1:\nFirst we have to extract the mixture paramaters.\n\n# Split the inputs into paramaters\nout_mu, out_sigma, out_pi = tf.split(y_pred, num_or_size_splits=[N_MIXES, N_MIXES, N_MIXES],\n                                     axis=-1, name='mdn_coef_split')\nmus = tf.split(out_mu, num_or_size_splits=N_MIXES, axis=1)\nsigs = tf.split(out_sigma, num_or_size_splits=N_MIXES, axis=1)\n\n\nSplit up the parameters \\(\\boldsymbol\\mu\\), \\(\\boldsymbol\\sigma\\), and \\(\\boldsymbol\\pi\\), remember that there are N_MIXES \\(= K\\) of each of these.\n\\(\\boldsymbol\\mu\\) and \\(\\boldsymbol\\sigma\\) have to be split again so that we can iterate over them (you can’t iterate over an axis of a tensor…)\n\n\n\n\nLoss Function: Part 2:\nNow we have to construct the mixture model’s PDF.\n\n# Construct the mixture models\ncat = tfd.Categorical(logits=out_pi) \ncoll = [tfd.Normal(loc=loc, scale=scale) for loc, scale\n        in zip(mus, sigs)]\nmixture = tfd.Mixture(cat=cat, components=coll)\n\n\nFor this, we’re using the Mixture abstraction provided in tensorflow-probability.distributions.\nThis takes a categorical (a.k.a. softmax, a.k.a. generalized Bernoulli distribution) model, and a list the component distributions.\nEach normal PDF is contructed using tfd.Normal.\nCan do this from first principles as well, but good to use abstractions that are available (?)\n\n\n\n\nLoss Function: Part 3:\nFinally, we calculate the loss:\n\nloss = mixture.log_prob(y_true)\nloss = tf.negative(loss)\nloss = tf.reduce_mean(loss)\n\n\nmixture.log_prob(y_true) means “the log-likelihood of sampling y_true from the distribution called mixture.”\n\n\n\n\nSome more details….\n\n\nThis “version” of a mixture model works for a mixture of 1D normal distributions.\nNot too hard to extend to multivariate normal distributions, which are useful for lots of problems.\nThis is how it actually works in my Keras MDN layer, have a look at the code for more details…\n\n\n\n\nMDN-RNNs\n\nMDNs can be handy at the end of an RNN! Imagine a robot calculating moves forward through space, it might have to choose from a number of valid positions, each of which could be modelled by a 2D Normal model.\n\n\n\nMDN-RNN Architecture\n\nCan be as simple as putting an MDN layer after recurrent layers!\n\n\n\nUse Cases: Handwriting Generation\n\n\n\nHandwriting Generation RNN (Graves, 2013).\nTrained on handwriting data.\nPredicts the next location of the pen (\\(dx\\), \\(dy\\), and up/down)\nNetwork takes text to write as an extra input, RNN learns to decide what character to write next.\n\n\n\n\nUse Cases: SketchRNN\n \n\nSketchRNN Kanji (Ha, 2015); similar to handwriting generation, trained on kanji and then generates new “fake” characters\nSketchRNN VAE (Ha et al., 2017); similar again, but trained on human-sourced sketches. VAE architecture with bidirectional RNN encoder and MDN in the decoder part.\n\n\n\n\nUse Cases: RoboJam\n\n\n\n\n\n\nRoboJam (Martin et al., 2018); similar to the kanji RNN, but trained on touchscreen musical performances\nExtra complexity: have to model touch position (\\(x\\), \\(y\\)) and time (\\(dt\\)).\nImplemented in my MicroJam app (have a go: microjam.info)\n\n\n\n\n\n\nUse Cases: World Models\n\n\n\nWorld Models (Ha &amp; Schmidhuber, 2018)\nTrain a VAE for visual perception an environment (e.g., VizDoom), now each frame from the environment can be represented by a vector \\(z\\)\nTrain MDN to predict next \\(z\\), use this to help train an agent to operate in the environment.\n\n\n  \n  \n\n\n\n\n\nReferences\n\nChristopher M. Bishop. 1994. Mixture Density Networks. Technical Report NCRG/94/004. Neural Computing Research Group, Aston University.\nAxel Brando. 2017. Mixture Density Networks (MDN) for distribution and uncertainty estimation. Master’s thesis. Universitat Politècnica de Catalunya.\nA. Graves. 2013. Generating Sequences With Recurrent Neural Networks. ArXiv e-prints (Aug. 2013). ArXiv:1308.0850\nDavid Ha and Douglas Eck. 2017. A Neural Representation of Sketch Drawings. ArXiv e-prints (April 2017). ArXiv:1704.03477\nCharles P. Martin and Jim Torresen. 2018. RoboJam: A Musical Mixture Density Network for Collaborative Touchscreen Interaction. In Evolutionary and Biologically Inspired Music, Sound, Art and Design: EvoMUSART ’18, A. Liapis et al. (Ed.). Lecture Notes in Computer Science, Vol. 10783. Springer International Publishing. DOI:10.1007/9778-3-319-77583-8_11\nD. Ha and J. Schmidhuber. 2018. Recurrent World Models Facilitate Policy Evolution. ArXiv e-prints (Sept. 2018). ArXiv:1809.01999\n\n\n\n",
        "url": "/presentations/mixture-density-networks/"
      },
    
      {
        "title": "Deep Learning in the Cloud",
        "excerpt": "\n\n\n\n    Creative Prediction with Neural Networks\n    A course in ML/AI for creative expression\n\n\nDeep Learning in the Cloud\n\nCharles Martin - The Australian National University\n\n\n\t\n\n\n\n\nWhy cloud for ML/AI?\n\n\n  Not always convenient/cost effective to use big workstation.\n  We like small laptops without hot GPUs and processors.\n  We might want to move from research to product!\n  The internet is cool/fun?\n\n\nCloud Models\n\n \n\nsrc: www.comgt.com/lib/sw/deliverymodels/\n\nPizza as a Service\n\n \n\nsrc: Albert Barron\n\nWhat do we need?\n\n\n  GPUs: Nvidia [GTX1080TI, K80, P100, V100] or… “Tensor Processing Units”\n  OS: Linux?\n  CUDA + CUDnn\n  Python\n  Python libraries: Tensorflow, Keras, SKLearn, etc.\n  Jupyter\n\n\nOn Premises: Workstations\n\n\n\n\n  Workstations (15-50KNOK)\n  Pro: fun to play with\n  Pro: good for small number of users\n  Pro: one-time cost\n  Con: not practical for many users\n  Con: have to keep setting up eduroam\n  Con: I don’t like sharing?\n\n\nInfrastructure-as-a-Service\n\n\n\n\n  Virtual servers\n  Set up server, access via Linux shell\n  Amazon Web Services (AWS)\n  Google Cloud Platform (GCP)\n  DigitalOcean (DO)\n  UH Cloud (UiO)\n\n\nPlatform-as-a-Service\n\n\n\n\n  Google Cloud Kubenetes Engine\n  Deploy “Containerised” application to servers.\n  (Deploy DL to Kubernetes)\n  Sigma2 (UiO)\n\n\nSoftware-as-a-Service\n\n\n\n\n  Google Colaboratory (👏🏼)\n  Kaggle Kernels\n\n\nExample: Robojam\n\n\n\nExample: Robojam\n\n\n\nRoboJam is a Keras project, now deployed as a Flask web application.\n\nStarting point: Local + SaaS\n\n\n  Developed on local shared workstation\n  Also worked on Colaboratory\n  Tips:\n  keep jupyter sessions around with screen\n  tunnel jupyter port with ssh -L 8888:localhost:8888\n  Could also use Google Cloud VMs with GPUs for short training runs\n\n\nStarting point: IaaS + PaaS\n\n\n\n\n  Can use VMs and Containers for DL development\n  Google’s “Cloud Deep Learning VM Image”\n  Comes with jupyterhub running to do development in a browser.\n  Expensive for a good machine: K80 GPU 0.45USD/h\n\n\nSaaS Architecture on Colab\n\n\n\nProduction: Turning into a web service\n\n\n  Used flask framework to create a RESTful web API\n  Just one endpoint: https://0.0.0.0:5000/api/predict\n  Send performance as JSON to that endpoit\n  robojam RNN model is conditioned with input, then a continuation is predicted.\n  prediction returned as JSON\n  “Deploying DL models with Flask”\n\n\nProduction: Deploying to DigitalOcean\n\n\n  Using cheapest DigitalOcean VM: 1vCPU, 1GB, $5 per month.\n  Login, clone git repo, run server in a detached screen.\n  Works! Deployed for about a year.\n  Predictions take about 1.0s-1.5s, not too bad.\n  Problem: what if the app gets popular?\n\n\nIaaS Architecture on DigitalOcean\n\n\n\nContainerising: Docker\n\n\n  We want to make a “container” that includes Robojam and all necessary libraries to run on any Docker installation.\n  We’ll start with the tensorflow docker which includes a development environment for tensorflow.\n\n\nContainerising: Dockerfile\n\nFROM tensorflow/tensorflow:latest-py3\nMAINTAINER Charles Martin \"charlepm@ifi.uio.no\"\n\nCOPY requirements.txt /tmp/\nRUN pip install --requirement /tmp/requirements.txt\nCOPY . /tmp/\nWORKDIR /tmp\nCMD [ \"python\", \"./serve_tiny_performance_mdrnn.py\" ]\n\n\nContainerising: Building the container\n\nsudo docker build -t robojam:latest .\ndocker tag robojam:latest charlepm/robojam:latest\ndocker push charlepm/robojam:latest\n\n\nContainerising: Running the Container\n\ndocker run -d -p 5000:5000 robojam:latest\n\nContainerised Architecture with Docker\n\n\n\nDeploying to Kubenetes\n\n\n  Kubernetes is a system to run docker containers on multiple computers simultaneously.\n  Let’s set up a little cluster on Google Cloud Platform and deploy Robojam.\n  Need to set up computers through the web interface\n  Then use command interface to start Robojam.\n\n\nDeploying Robojam to Cluster\n\nkubectl run robojam-cluster --image=charlepm/robojam:latest --port 5000\nkubectl get pods\nkubectl expose deployment robojam-cluster --type=LoadBalancer --port 5000 --target-port 5000\nkubectl get service\n\n\nMicro-Service Architecture with Kubernetes\n\n\n\nConclusion\n\n\n  ML/AI isn’t just for research, we can make cool applications too!\n  Cloud resources very available:\n  Not too expensive to use powerful servers for short time (training)\n  can do a lot with cheap servers for production\n  Try out docker etc, makes life much easier.\n  Try out Jupyterhub for development. Might be way of future?\n\n\nExtra links\n\n\n  Maximise your GPU\nDollars\n\n",
        "content": "\n\n\n\n    Creative Prediction with Neural Networks\n    A course in ML/AI for creative expression\n\n\nDeep Learning in the Cloud\n\nCharles Martin - The Australian National University\n\n\n\t\n\n\n\n\nWhy cloud for ML/AI?\n\n\n  Not always convenient/cost effective to use big workstation.\n  We like small laptops without hot GPUs and processors.\n  We might want to move from research to product!\n  The internet is cool/fun?\n\n\nCloud Models\n\n \n\nsrc: www.comgt.com/lib/sw/deliverymodels/\n\nPizza as a Service\n\n \n\nsrc: Albert Barron\n\nWhat do we need?\n\n\n  GPUs: Nvidia [GTX1080TI, K80, P100, V100] or… “Tensor Processing Units”\n  OS: Linux?\n  CUDA + CUDnn\n  Python\n  Python libraries: Tensorflow, Keras, SKLearn, etc.\n  Jupyter\n\n\nOn Premises: Workstations\n\n\n\n\n  Workstations (15-50KNOK)\n  Pro: fun to play with\n  Pro: good for small number of users\n  Pro: one-time cost\n  Con: not practical for many users\n  Con: have to keep setting up eduroam\n  Con: I don’t like sharing?\n\n\nInfrastructure-as-a-Service\n\n\n\n\n  Virtual servers\n  Set up server, access via Linux shell\n  Amazon Web Services (AWS)\n  Google Cloud Platform (GCP)\n  DigitalOcean (DO)\n  UH Cloud (UiO)\n\n\nPlatform-as-a-Service\n\n\n\n\n  Google Cloud Kubenetes Engine\n  Deploy “Containerised” application to servers.\n  (Deploy DL to Kubernetes)\n  Sigma2 (UiO)\n\n\nSoftware-as-a-Service\n\n\n\n\n  Google Colaboratory (👏🏼)\n  Kaggle Kernels\n\n\nExample: Robojam\n\n\n\nExample: Robojam\n\n\n\nRoboJam is a Keras project, now deployed as a Flask web application.\n\nStarting point: Local + SaaS\n\n\n  Developed on local shared workstation\n  Also worked on Colaboratory\n  Tips:\n  keep jupyter sessions around with screen\n  tunnel jupyter port with ssh -L 8888:localhost:8888\n  Could also use Google Cloud VMs with GPUs for short training runs\n\n\nStarting point: IaaS + PaaS\n\n\n\n\n  Can use VMs and Containers for DL development\n  Google’s “Cloud Deep Learning VM Image”\n  Comes with jupyterhub running to do development in a browser.\n  Expensive for a good machine: K80 GPU 0.45USD/h\n\n\nSaaS Architecture on Colab\n\n\n\nProduction: Turning into a web service\n\n\n  Used flask framework to create a RESTful web API\n  Just one endpoint: https://0.0.0.0:5000/api/predict\n  Send performance as JSON to that endpoit\n  robojam RNN model is conditioned with input, then a continuation is predicted.\n  prediction returned as JSON\n  “Deploying DL models with Flask”\n\n\nProduction: Deploying to DigitalOcean\n\n\n  Using cheapest DigitalOcean VM: 1vCPU, 1GB, $5 per month.\n  Login, clone git repo, run server in a detached screen.\n  Works! Deployed for about a year.\n  Predictions take about 1.0s-1.5s, not too bad.\n  Problem: what if the app gets popular?\n\n\nIaaS Architecture on DigitalOcean\n\n\n\nContainerising: Docker\n\n\n  We want to make a “container” that includes Robojam and all necessary libraries to run on any Docker installation.\n  We’ll start with the tensorflow docker which includes a development environment for tensorflow.\n\n\nContainerising: Dockerfile\n\nFROM tensorflow/tensorflow:latest-py3\nMAINTAINER Charles Martin \"charlepm@ifi.uio.no\"\n\nCOPY requirements.txt /tmp/\nRUN pip install --requirement /tmp/requirements.txt\nCOPY . /tmp/\nWORKDIR /tmp\nCMD [ \"python\", \"./serve_tiny_performance_mdrnn.py\" ]\n\n\nContainerising: Building the container\n\nsudo docker build -t robojam:latest .\ndocker tag robojam:latest charlepm/robojam:latest\ndocker push charlepm/robojam:latest\n\n\nContainerising: Running the Container\n\ndocker run -d -p 5000:5000 robojam:latest\n\nContainerised Architecture with Docker\n\n\n\nDeploying to Kubenetes\n\n\n  Kubernetes is a system to run docker containers on multiple computers simultaneously.\n  Let’s set up a little cluster on Google Cloud Platform and deploy Robojam.\n  Need to set up computers through the web interface\n  Then use command interface to start Robojam.\n\n\nDeploying Robojam to Cluster\n\nkubectl run robojam-cluster --image=charlepm/robojam:latest --port 5000\nkubectl get pods\nkubectl expose deployment robojam-cluster --type=LoadBalancer --port 5000 --target-port 5000\nkubectl get service\n\n\nMicro-Service Architecture with Kubernetes\n\n\n\nConclusion\n\n\n  ML/AI isn’t just for research, we can make cool applications too!\n  Cloud resources very available:\n  Not too expensive to use powerful servers for short time (training)\n  can do a lot with cheap servers for production\n  Try out docker etc, makes life much easier.\n  Try out Jupyterhub for development. Might be way of future?\n\n\nExtra links\n\n\n  Maximise your GPU\nDollars\n\n",
        "url": "/presentations/04-deep-learning-on-cloud/"
      },
    
      {
        "title": "An Interactive Musical Prediction System with MDRNNs",
        "excerpt": "\n\n\n  An Interactive Musical Prediction System with MDRNNs\n  \n\n\nDr Charles Martin - The Australian National University\n\nweb: charlesmartin.com.au&nbsp;&nbsp;&nbsp;&nbsp; twitter/github: @cpmpercussion\n\n\n\nWhat is this?\n\nLearning to Predict Sequences\n\n\n\nInteracting with Musical Predictions\n\n\n\nWhy is this needed?\n\n\n\n\n  \n    \n      Creative Deep Learning Systems\n      NIMEs\n    \n  \n  \n    \n      Focus on MIDI data (e.g., Magenta Studio)\n      Yes MIDI, but also many custom sensors\n    \n    \n      Focus on digital audio\n      Focus on performer gestures\n    \n    \n      Focus on composition/artefact generation\n      Focus on interaction\n    \n    \n      Rhythm on 16th note grid\n      Complex or no rhythm\n    \n    \n      Focus on categorical data\n      Continuous data more interesting\n    \n  \n\n\n\n\n\n\n\n\nIMPS: Interactive Musial Prediction System\n\n\n\n\n  An opinionated deep learning model for NIMEs\n  An environment for making NIMEs that play themselves\n  “Wekinator” for deep learning?\n\n\nHow does it work?\n\nMixture Density Recurrent Neural Network\n\n\n\nMixture Density RNN\n\n\n\nGood at predicting creative, continuous, multi-dimensional data: handwriting, sketches… musical gestures?\n\nWhat to do with predictions?\n\n\n\n\n  Call-and-Response: Continue gestures when performer stops\n  Layered predictions: Always predict next move from current gesture\n  Duet: Two interdependent processes\n\n\nOk, how do I use it?\n\n\n  \n\n\nThree easy steps…\n\n\n\n\nCollect some data: IMPS logs interactions automatically to build up a dataset\n\n\n\nTrain an MDRNN: IMPS includes good presets, no need to train for days/weeks\n\n\n\nPerform! IMPS includes three interaction modes, scope to extend in future!\n\n\n\n\nIs this even practical?\n\nDeep Learning in NIMEs??\n\n\n  Is it practical for real-time use?\n  How do the MDRNN parameters affect time per prediction?\n  What are “good defaults” for training parameters?\n  Do you need a powerful/expensive computer?\n\n\nTest Systems\n\n\n\nResults: Time per prediction\n\n\n\nTime per prediction (ms) with different sizes of LSTM layers.\n\nResults: Time per prediction\n\n\n\nTime per prediction (ms) with different MDN output dimensions. (64\nLSTM units)\n\nResults: Training Error vs Validation Set Error\n\n\n\n12K sample dataset (15 minutes of performance)\n\nTakeaway: Smallest model best for small datasets. Don’t bother training for\ntoo long.\n\nResults: Training Error vs Validation Set Error\n\n\n\n100K sample dataset (120 minutes of performance)\n\nTakeaway: 64- and 128-unit model still best!\n\nResults: Exploring Generation\n\n\n\nTakeaway: Make Gaussians less diverse, make categorical more diverse.\n\nTry it out!\n\n\n\n\n  Available on GitHub\n  Try with your NIMEs!\n  Hack if you want!\n  Add an issue with problems/results!\n\n\nTwitter: @cpmpercussion\n\nWebsite: creativeprediction.xyz/imps\n",
        "content": "\n\n\n  An Interactive Musical Prediction System with MDRNNs\n  \n\n\nDr Charles Martin - The Australian National University\n\nweb: charlesmartin.com.au&nbsp;&nbsp;&nbsp;&nbsp; twitter/github: @cpmpercussion\n\n\n\nWhat is this?\n\nLearning to Predict Sequences\n\n\n\nInteracting with Musical Predictions\n\n\n\nWhy is this needed?\n\n\n\n\n  \n    \n      Creative Deep Learning Systems\n      NIMEs\n    \n  \n  \n    \n      Focus on MIDI data (e.g., Magenta Studio)\n      Yes MIDI, but also many custom sensors\n    \n    \n      Focus on digital audio\n      Focus on performer gestures\n    \n    \n      Focus on composition/artefact generation\n      Focus on interaction\n    \n    \n      Rhythm on 16th note grid\n      Complex or no rhythm\n    \n    \n      Focus on categorical data\n      Continuous data more interesting\n    \n  \n\n\n\n\n\n\n\n\nIMPS: Interactive Musial Prediction System\n\n\n\n\n  An opinionated deep learning model for NIMEs\n  An environment for making NIMEs that play themselves\n  “Wekinator” for deep learning?\n\n\nHow does it work?\n\nMixture Density Recurrent Neural Network\n\n\n\nMixture Density RNN\n\n\n\nGood at predicting creative, continuous, multi-dimensional data: handwriting, sketches… musical gestures?\n\nWhat to do with predictions?\n\n\n\n\n  Call-and-Response: Continue gestures when performer stops\n  Layered predictions: Always predict next move from current gesture\n  Duet: Two interdependent processes\n\n\nOk, how do I use it?\n\n\n  \n\n\nThree easy steps…\n\n\n\n\nCollect some data: IMPS logs interactions automatically to build up a dataset\n\n\n\nTrain an MDRNN: IMPS includes good presets, no need to train for days/weeks\n\n\n\nPerform! IMPS includes three interaction modes, scope to extend in future!\n\n\n\n\nIs this even practical?\n\nDeep Learning in NIMEs??\n\n\n  Is it practical for real-time use?\n  How do the MDRNN parameters affect time per prediction?\n  What are “good defaults” for training parameters?\n  Do you need a powerful/expensive computer?\n\n\nTest Systems\n\n\n\nResults: Time per prediction\n\n\n\nTime per prediction (ms) with different sizes of LSTM layers.\n\nResults: Time per prediction\n\n\n\nTime per prediction (ms) with different MDN output dimensions. (64\nLSTM units)\n\nResults: Training Error vs Validation Set Error\n\n\n\n12K sample dataset (15 minutes of performance)\n\nTakeaway: Smallest model best for small datasets. Don’t bother training for\ntoo long.\n\nResults: Training Error vs Validation Set Error\n\n\n\n100K sample dataset (120 minutes of performance)\n\nTakeaway: 64- and 128-unit model still best!\n\nResults: Exploring Generation\n\n\n\nTakeaway: Make Gaussians less diverse, make categorical more diverse.\n\nTry it out!\n\n\n\n\n  Available on GitHub\n  Try with your NIMEs!\n  Hack if you want!\n  Add an issue with problems/results!\n\n\nTwitter: @cpmpercussion\n\nWebsite: creativeprediction.xyz/imps\n",
        "url": "/presentations/imps/"
      },
    
      {
        "title": "Creative Prediction Practicalities",
        "excerpt": "\n\n\n\n    Creative Prediction with Neural Networks\n    A course in ML/AI for creative expression\n\n\nCreative Prediction Practicalities\n\nCharles Martin - The Australian National University\n\n\n\t\n\n\n\n\nPython is powerful.\n\n\n\nWe can all be Python superheroes! \n\nPython setup can be hard.\n\n\n\n(Sorry) \n\nMachine Learning Systems\n\n\n\n\n  \n  GPUs; not just fun for games!\n  \n\n\nColab to the Rescue?\n\n\n\nOn Premises: Workstations\n\n\n\n\n  Workstations\n  Pro: fun to play with\n  Pro: good for small number of users\n  Pro: one-time cost\n  Con: not practical for many users\n  Con: have to keep setting them up\n  Con: I don’t like sharing?\n\n\nInfrastructure-as-a-Service\n\n\n\n\n  Virtual servers\n  Set up server, access via Linux shell\n  Amazon Web Services (AWS)\n  Google Cloud Platform (GCP)\n  DigitalOcean (DO)\n  UH Cloud (UiO)\n\n\nPlatform-as-a-Service\n\n\n\n\n  Google Cloud Kubenetes Engine\n  Deploy “Containerised” application to servers.\n  (Deploy DL to Kubernetes)\n  Sigma2 (UiO)\n\n\nget your tools together\n\nCheck out the setup page on creativeprediction.xyz.\n\nExercise: Try out a Google Colab Notebook\n",
        "content": "\n\n\n\n    Creative Prediction with Neural Networks\n    A course in ML/AI for creative expression\n\n\nCreative Prediction Practicalities\n\nCharles Martin - The Australian National University\n\n\n\t\n\n\n\n\nPython is powerful.\n\n\n\nWe can all be Python superheroes! \n\nPython setup can be hard.\n\n\n\n(Sorry) \n\nMachine Learning Systems\n\n\n\n\n  \n  GPUs; not just fun for games!\n  \n\n\nColab to the Rescue?\n\n\n\nOn Premises: Workstations\n\n\n\n\n  Workstations\n  Pro: fun to play with\n  Pro: good for small number of users\n  Pro: one-time cost\n  Con: not practical for many users\n  Con: have to keep setting them up\n  Con: I don’t like sharing?\n\n\nInfrastructure-as-a-Service\n\n\n\n\n  Virtual servers\n  Set up server, access via Linux shell\n  Amazon Web Services (AWS)\n  Google Cloud Platform (GCP)\n  DigitalOcean (DO)\n  UH Cloud (UiO)\n\n\nPlatform-as-a-Service\n\n\n\n\n  Google Cloud Kubenetes Engine\n  Deploy “Containerised” application to servers.\n  (Deploy DL to Kubernetes)\n  Sigma2 (UiO)\n\n\nget your tools together\n\nCheck out the setup page on creativeprediction.xyz.\n\nExercise: Try out a Google Colab Notebook\n",
        "url": "/presentations/crepreprac/"
      },
    
      {
        "title": "How to be a Project Student in the Intelligent Music Lab",
        "excerpt": "\n\n\n\n    Creative Prediction with Neural Networks\n    A course in ML/AI for creative expression\n\n\nHow to be a Project Student in the Intelligent Music Lab\n\nCharles Martin - The Australian National University\n\n\n\t\n\n\n\n\n\n  \n  Ngunnawal &amp; Ngambri Country\n  \n\n\nWelcome! Great to have you here!\n\nYou’re now part of a research lab (woo hoo!)\n\nIt’s a loose network of academics, HDR students, and Master/Undergrad project students!\n\n\n  Sometimes we have a formal project (e.g., Charles is my supervisor).\n  Sometimes we have a collaboration (e.g., I’m working on a paper with Yichen and Charles).\n  Sometimes it’s part of a job (e.g., I’m a research assistant working for Henry).\n  Sometimes we just participate (e.g., I’m going to go to Ben’s seminar today to hear about live coding).\n\n\nWhat is this lecture about?\n\n\n  \n    A brief intro to how work on a project with Charles and others in the lab.\n  \n  \n    Expectations: what we expect of you, what you can expect from us.\n  \n  \n    Practicalities\n  \n  \n    How to plan research\n  \n  \n    Guide to weekly activities\n  \n  \n    Guide to the “shape” of a project.\n  \n\n\nProblems (for Charles):\n\n\n  \n    there are (up to) 10 of you and only one of me!\n  \n  \n    every student needs some of the same information\n  \n  \n    progress on projects is uneven over the semester\n  \n  \n    sometimes students don’t show up\n  \n\n\nProblems (for you):\n\n\n  you have to define a project, execute it, and write it up! (it’s a lot of work - luckily you have 1-2 semesters to do it!)\n  you might be new to research (for now!)\n  you might not have a lot of background knowledge (yet!)\n  research projects are tricky! sometimes ideas don’t work out, you have to be flexible but committed to reaching the goal (handing in a report/artefact/thesis)\n  you’re probably  busy! (other subjects, jobs, life!)\n\n\nThe Weekly Status Meeting\n\nHow it works:\n\nThis is a group meeting, online, once per week. In each meeting you will be asked three questions:\n\n\n  What progress have you made?\n  What obstacles have you encountered?\n  What is your next step?\n\n\nPurpose:\n\n\n  to gain an understanding of how everybody in the lab is going with their project\n  to determine if anybody requires individual meetings\n\n\nStatus meeting will be only 30 minutes! (approx 3 minutes each!)\n\nDoes this look like agile development? Ref: (Hicks and Foster, 2010)\n\nOn-demand Individual Meeting (&lt;= 30 minutes)\n\nHow it works:\n\nThis is an individual meeting, either online or in-person. Book a time through Microsoft Bookings.\n\nRequirements:\n\nWill have a specific goal e.g.:\n\n\n  talk about possible solutions to problem X\n  fix technical issue Y\n  discuss an ethics protocol\n  analyse results to an experiment\n  goal should be SMART (Specific, Measurable, Attainable, Realistic, Time-bound)\n\n\nStudent and supervisor should know exactly what we are going to discuss.\n\nSeminars\n\nHow it works:\n\nA group meeting for delivering information with relevance to multiple people. This will be dual-delivery! (both in-person and online).\n\nExamples:\n\n\n  how to structure a report/thesis in LaTeX\n  how to structure a git repository for project delivery\n  how to apply for an ethics protocol\n  how to run a user study\n  research topics in musical machine learning (mostly from Creative Prediction\n  research topics in sound and music computing (some overlap with COMP2710 Laptop Ensemble)\n  research topics in interactive media (some overlap with COMP1720 Art and Interaction Computing)\n\n\nRevision: Three types of meeting\n\n\n  Status meeting: just for updating status (not research)\n  On-demand meeting: just for discussing research (not status)\n  Seminar: just for 1-to-N information delivery (like this)\n\n\nThe shape of a project\n\n\n  \n    Getting started\n  \n  \n    The Work\n  \n  \n    The Write-up\n  \n\n\nLet’s go through these in detail…\n\nGetting started\n\n\n  Defining research topic and objectives\n  Background reading\n  Finding your “unsolved problem”\n  Discovering code frameworks and starting points.\n  If studying humans: applying for ethics approval.\n\n\nThe Work\n\n\n  Coding up a system\n  Iterating on a design\n  Running experiments\n\n\nThe Write-up\n\n\n  Expressing your process\n  Analysing and discussing results\n  Making images and plots\n  Coming up with your findings and contributions\n  Writing documentation and packaging code\n\n\nImportant Note!\n\nYour grade is based on a combination of your written report, artefact, and presentations.\n\nExaminers look for your ability to:\n\nunderstand and synthesise existing research\n\ncreate an artefact, or code project skillfully, and explain it\n\nperform experiments or critically engage with your research problem.\n\nYour grade is not based on your results (really!) (see Honours marking guidelines)\n\nIf you get “good” results (yay!) but don’t discuss them in detail, or critically engage in why they are good, you won’t do well. (and the converse)\n\nProject Planning\n\nA bad project plan\n\n\n  Getting started: Weeks 1-9\n  The Work: Weeks 10-11\n  The Write-up: Week 12\n\n\nA better project plan\n\n\n  Getting started: Weeks 1-6\n  The Work: Weeks 3-10\n  The Write-up: Week 6 and Weeks 9-12\n\n\nTo help, here’s some deadlines:\n\n1-Semester Project\n\n\n  End of week 6: Draft of report Introduction and Background sections (and draft code repository)\n  End of week 8: Draft of report System Design section (and draft code repository)\n  Start of week 11: Draft full report\n\n\n2-Semester Project.\n\n\n  S1: End of week 6: Draft of report Introduction\n  S1: End of week 8: Draft of report Background section\n  S1: End of week 11: Draft code repository and/or report System Design sections\n  S2: End of week 6: Draft of Experimental Methodology and Results sections\n  S2: End of week 8: Full report, draft 1\n  S2: End of week 11: Full report, draft 2\n\n\nEthics Approval\n\nIf you are studying humans (surveys, questionnaires, user studies) you need ethics approval.\n\n\n  All Australian National University researchers (staff or students) who intend on conducting research involving the collection of data from human participants need to apply for ANU Human Ethics approval before starting their data collection.\n\n\n\n  Gaining ethics approval is an important part of research work.\n  Many CS student projects do not need user studies to be successful.\n  Consider what “evaluation” looks like early in your project (esp. 1-semester projects)\n  Ethics approval should be done in first semester of 2-semester projects or before week 6 in 1-semester projects.\n\n\nConclusions\n\n\n  \n    communication and critical analysis of your project is most important aspect in grading\n  \n  \n    expect the “work” to take a long time, need to prioritise getting this started early in project\n  \n  \n    start write up as soon as your start your project\n  \n  \n    meetings are precious — most issues can be handled in team meetings. individual meetings only for critical issues / inflection points in project.\n  \n  \n    studies with humans require ethics approval—no exceptions.\n  \n\n",
        "content": "\n\n\n\n    Creative Prediction with Neural Networks\n    A course in ML/AI for creative expression\n\n\nHow to be a Project Student in the Intelligent Music Lab\n\nCharles Martin - The Australian National University\n\n\n\t\n\n\n\n\n\n  \n  Ngunnawal &amp; Ngambri Country\n  \n\n\nWelcome! Great to have you here!\n\nYou’re now part of a research lab (woo hoo!)\n\nIt’s a loose network of academics, HDR students, and Master/Undergrad project students!\n\n\n  Sometimes we have a formal project (e.g., Charles is my supervisor).\n  Sometimes we have a collaboration (e.g., I’m working on a paper with Yichen and Charles).\n  Sometimes it’s part of a job (e.g., I’m a research assistant working for Henry).\n  Sometimes we just participate (e.g., I’m going to go to Ben’s seminar today to hear about live coding).\n\n\nWhat is this lecture about?\n\n\n  \n    A brief intro to how work on a project with Charles and others in the lab.\n  \n  \n    Expectations: what we expect of you, what you can expect from us.\n  \n  \n    Practicalities\n  \n  \n    How to plan research\n  \n  \n    Guide to weekly activities\n  \n  \n    Guide to the “shape” of a project.\n  \n\n\nProblems (for Charles):\n\n\n  \n    there are (up to) 10 of you and only one of me!\n  \n  \n    every student needs some of the same information\n  \n  \n    progress on projects is uneven over the semester\n  \n  \n    sometimes students don’t show up\n  \n\n\nProblems (for you):\n\n\n  you have to define a project, execute it, and write it up! (it’s a lot of work - luckily you have 1-2 semesters to do it!)\n  you might be new to research (for now!)\n  you might not have a lot of background knowledge (yet!)\n  research projects are tricky! sometimes ideas don’t work out, you have to be flexible but committed to reaching the goal (handing in a report/artefact/thesis)\n  you’re probably  busy! (other subjects, jobs, life!)\n\n\nThe Weekly Status Meeting\n\nHow it works:\n\nThis is a group meeting, online, once per week. In each meeting you will be asked three questions:\n\n\n  What progress have you made?\n  What obstacles have you encountered?\n  What is your next step?\n\n\nPurpose:\n\n\n  to gain an understanding of how everybody in the lab is going with their project\n  to determine if anybody requires individual meetings\n\n\nStatus meeting will be only 30 minutes! (approx 3 minutes each!)\n\nDoes this look like agile development? Ref: (Hicks and Foster, 2010)\n\nOn-demand Individual Meeting (&lt;= 30 minutes)\n\nHow it works:\n\nThis is an individual meeting, either online or in-person. Book a time through Microsoft Bookings.\n\nRequirements:\n\nWill have a specific goal e.g.:\n\n\n  talk about possible solutions to problem X\n  fix technical issue Y\n  discuss an ethics protocol\n  analyse results to an experiment\n  goal should be SMART (Specific, Measurable, Attainable, Realistic, Time-bound)\n\n\nStudent and supervisor should know exactly what we are going to discuss.\n\nSeminars\n\nHow it works:\n\nA group meeting for delivering information with relevance to multiple people. This will be dual-delivery! (both in-person and online).\n\nExamples:\n\n\n  how to structure a report/thesis in LaTeX\n  how to structure a git repository for project delivery\n  how to apply for an ethics protocol\n  how to run a user study\n  research topics in musical machine learning (mostly from Creative Prediction\n  research topics in sound and music computing (some overlap with COMP2710 Laptop Ensemble)\n  research topics in interactive media (some overlap with COMP1720 Art and Interaction Computing)\n\n\nRevision: Three types of meeting\n\n\n  Status meeting: just for updating status (not research)\n  On-demand meeting: just for discussing research (not status)\n  Seminar: just for 1-to-N information delivery (like this)\n\n\nThe shape of a project\n\n\n  \n    Getting started\n  \n  \n    The Work\n  \n  \n    The Write-up\n  \n\n\nLet’s go through these in detail…\n\nGetting started\n\n\n  Defining research topic and objectives\n  Background reading\n  Finding your “unsolved problem”\n  Discovering code frameworks and starting points.\n  If studying humans: applying for ethics approval.\n\n\nThe Work\n\n\n  Coding up a system\n  Iterating on a design\n  Running experiments\n\n\nThe Write-up\n\n\n  Expressing your process\n  Analysing and discussing results\n  Making images and plots\n  Coming up with your findings and contributions\n  Writing documentation and packaging code\n\n\nImportant Note!\n\nYour grade is based on a combination of your written report, artefact, and presentations.\n\nExaminers look for your ability to:\n\nunderstand and synthesise existing research\n\ncreate an artefact, or code project skillfully, and explain it\n\nperform experiments or critically engage with your research problem.\n\nYour grade is not based on your results (really!) (see Honours marking guidelines)\n\nIf you get “good” results (yay!) but don’t discuss them in detail, or critically engage in why they are good, you won’t do well. (and the converse)\n\nProject Planning\n\nA bad project plan\n\n\n  Getting started: Weeks 1-9\n  The Work: Weeks 10-11\n  The Write-up: Week 12\n\n\nA better project plan\n\n\n  Getting started: Weeks 1-6\n  The Work: Weeks 3-10\n  The Write-up: Week 6 and Weeks 9-12\n\n\nTo help, here’s some deadlines:\n\n1-Semester Project\n\n\n  End of week 6: Draft of report Introduction and Background sections (and draft code repository)\n  End of week 8: Draft of report System Design section (and draft code repository)\n  Start of week 11: Draft full report\n\n\n2-Semester Project.\n\n\n  S1: End of week 6: Draft of report Introduction\n  S1: End of week 8: Draft of report Background section\n  S1: End of week 11: Draft code repository and/or report System Design sections\n  S2: End of week 6: Draft of Experimental Methodology and Results sections\n  S2: End of week 8: Full report, draft 1\n  S2: End of week 11: Full report, draft 2\n\n\nEthics Approval\n\nIf you are studying humans (surveys, questionnaires, user studies) you need ethics approval.\n\n\n  All Australian National University researchers (staff or students) who intend on conducting research involving the collection of data from human participants need to apply for ANU Human Ethics approval before starting their data collection.\n\n\n\n  Gaining ethics approval is an important part of research work.\n  Many CS student projects do not need user studies to be successful.\n  Consider what “evaluation” looks like early in your project (esp. 1-semester projects)\n  Ethics approval should be done in first semester of 2-semester projects or before week 6 in 1-semester projects.\n\n\nConclusions\n\n\n  \n    communication and critical analysis of your project is most important aspect in grading\n  \n  \n    expect the “work” to take a long time, need to prioritise getting this started early in project\n  \n  \n    start write up as soon as your start your project\n  \n  \n    meetings are precious — most issues can be handled in team meetings. individual meetings only for critical issues / inflection points in project.\n  \n  \n    studies with humans require ethics approval—no exceptions.\n  \n\n",
        "url": "/presentations/how-to-be-project-student/"
      },
    
      {
        "title": "References, Citations, Publishing, and Authorship",
        "excerpt": "\n\n\n\n    Creative Prediction with Neural Networks\n    A course in ML/AI for creative expression\n\n\nReferences, Citations, Publishing, and Authorship\n\nCharles Martin - The Australian National University\n\n\n\t\n\n\n\n\nWhat is a research paper?\n\n\n  \n    Written record of research discoveries\n  \n  \n    Communication to other researchers\n  \n  \n    Package up research for peer-review and criticism!\n  \n  \n    Preserve important knowledge forever (or at least for a long time).\n  \n\n\nHow do we know a paper isn’t lies?\n\nYou don’t!\n\nAfter all, anybody can write anything and put it on the internet.\n\nThis happens!\n\nAt least if a paper has been peer-reviewed, other researchers have also agreed that it is probably not lies.\n\nSee more about this\n\nWhy do we care?\n\nWhen you cite references in your own research, you are building on knowledge.\n\nYour references section provides information about where that knowledge comes from.\n\nExaminers will read your reference section! Why?\n\n\n  \n    shows student’s ability to critically engage with knowledge\n  \n  \n    shows student engaging with recent research, cutting edge, and “difficult” sources\n  \n\n\nFrom research to published paper…\n\n\n\nThere is a pathway from research to a published paper!\n\nAll research outputs are valuable, but they have different properties.\n\nAsk yourself: how do I know this output isn’t lies?\n\nCiting the right paper\n\nMultiple versions of a paper can exist! How do we cite the right one?\n\n\n  Pre-print version on arxiv.org\n  Publisher’s version on a journal website (behind paywall!)\n  “Post-print” version on university website (often required by law!)\n  Version on author’s own website (e.g., charlesmartin.com.au/publications)\n\n\nAs a first preference, always cite the publisher’s version! (this is the one that has been peer-reviewed).\n\nIf a paper is not published, cite the ArXiv or author’s version but remember: it might not be peer-reviewed.\n\nFinding the publisher’s version can take some research, particularly for CS conference publications!\n\ncitations\n\n\n\nA citation is an accurate way to find a reference.\n\nNeed to include: what (title), who (authors), when (date), where it came from (publisher, source), where to find it (DOI, URL)\n\nImage Credit: Author-Date Citation System (APA Style Guidelines)\n\nTotally valid to cite blog posts, youtube videos, tweets, etc etc, see the APA Style Blog for ideas.\n\nDOIs (Digital Object Identifier)\n\nThere’s a special kind of URL called a “DOI” available for many academic publications. The idea is that the DOI will be unique and never change (unlike URLs).\n\nAn example DOI is: 10.1000/182\n\nAnd we can follow it by going to: https://doi.org/10.1000/182\n\nThe prefix/suffix format identifies the publisher or registrant (prefix) and the specific object (suffix).\n\nDOIs aren’t just for papers! They can be for any piece of data (code, files, videos, images, figures, etc.)\n\nIf a DOI is available, use it instead of a URL!\n\nBibTeX\n\nI expect everybody to organise their references with a BibTeX database: here’s some info\n\n@inproceedings{McArthur2021vj,\n  author = {McArthur, Robert Neil and Martin, Charles Patrick},\n  title = {An Application for Evolutionary Music Composition Using Autoencoders},\n  year = {2021}\n  booktitle = {Artificial Intelligence in Music, Sound, Art and Design: 10th International Conference, EvoMUSART 2021},\n  pages = {443--458},\n  doi = {10.1007/978-3-030-72914-1_29},\n  publisher = {Springer International Publishing},\n}\n\n\nBibTeX for difficult items…\n\nSome things don’t fit well into BibTeX… you can use the unpublished or misc type and experiment with how to include appropriate information.\n\n@unpublished{Martin2017ap,\n  author = {Martin, Charles and Hunter, Alexander},\n  title = {Andromeda is Coming [Album]},\n  year = {2017}\n  month = jul,\n  note = {Music recording.},\n  url = {https://collectedresonances.bandcamp.com/album/andromeda-is-coming},\n}\n\n\n\n\nWriting your own papers…\n\nAuthoring your own papers!\n\nMany honours/Masters/research projects can be formulated in an academic paper.\n\n\n  Your final paper may not be quite acceptable for publication yet.\n  Your supervisor(s) can help you get it up to scratch.\n  We can also help you attend a conference to present your work.\n  ( We would expect to be co-authors of a paper after collaborating on these tasks! :-D )\n\n\nPublishing a Paper!\n\n\n  Academics publish research as part of their job!\n  Usually publishing is tied to important stuff: e.g., promotion, funding, number of PhD places, prestige.\n  We also LIKE to publish! We want to tell people about our work and want to read about what others are doing!\n  PhD students usually have to publish in order to get their degree!\n  Publishing can look a bit different between disciplines.\n  In computer science we have two main ways to publish:\n  Conferences: e.g., International Conference on Machine Learning\n  Journals: e.g., IEEE Transactions on Neural Networks and Learning Systems\n\n\nJournal\n\n\n\n\n  Send a manuscript to the journal when ready.\n  1+ rounds of reviewing and changes\n  Manuscript is turned into an article by the journal staff\n  Published online (sometimes print…)\n\n\nConference\n\n\n  Submit paper on conference website by a deadline.\n  All papers are reviewed at once.\n  Usually only one round of review! → accepted or rejected.\n  If accepted, polish up paper for “camera ready”\n  Attend conference, give talk (10-15mins) or present a poster. Meet other researchers, have a good time.\n  Paper appears in the “Proceedings of the Conference” which are published online.\n\n\nComputer Science\n\n\n  Conference papers have page limits\n  Journal articles tend to be a bit longer\n  Most CS academics treat conference papers and journal articles as being equally “worthy” of reading.\n  Other fields consider conference papers to be inconsequential or early-stage, will only look at articles and books for serious contributions.\n  Normally we would suggest honours/Master students go for a conference paper!\n\n\nQuality and Competition\n\nNot all conferences equal!\n\nAcceptance rates:\n\n\n  Difficult conferences &lt; 20% papers accepted\n  \n    Easier conferences &gt; 50% papers accepted → often focussed on a niche area.\n  \n  For “prestigious” conferences and journals, there are so many (1000s) submissions, that editors screen out papers that are unlikely to be accepted by reviewers.\n  Ranking for conferences tends to be anecdotal.\n  Journals often ranked by “Impact Factor”, roughly, “how many citations does the average paper get in one year”\n  Academics often consider too much focus on rankings to be harmful.\n  But we usually look anyway, as we want evidence that of excellence.\n\n\nFor you!\n\n\n  Do you want to take your research further?\n  Do you want to attend a conference?\n  \n    Submit your paper to a conference!\n  \n  Requires more revision after deadline.\n  Meetings with mentors/supervisors to make sure work is up to scratch.\n\n\nWho should be an author?\n\n\n  Who deserves credit for a work?\n  Movies do this well… everybody involved is listed with their role made very clear!\n  Academic Papers are not so clear…\n\n\n\n\n\nGiving Credit Where Credit is Due\n\n\n\nhttp://phdcomics.com/comics/archive.php?comicid=562\n\nProblems…\n\n\n\n\n\n\n\n\n\nDifferent Subjects Do this Differently\n\n\n\nVancouver Recommendations - 4 Criteria\n\n\n  Substantial contributions to the conception or design of the work; or the acquisition, analysis, or interpretation of data for the work; AND\n  Drafting the work or revising it critically for important intellectual content; AND\n  Final approval of the version to be published; AND\n  Agreement to be accountable for all aspects of the work in ensuring that questions related to the accuracy or integrity of any part of the work are appropriately investigated and resolved.\n\n\nAll those designated as authors should meet all four criteria for authorship, and all who meet the four criteria should be identified as authors. Those who do not meet all four criteria should be acknowledged.\n\n…\n\nThese authorship criteria are intended to reserve the status of authorship for those who deserve credit and can take responsibility for the work. The criteria are not intended for use as a means to disqualify colleagues from authorship who otherwise meet authorship criteria by denying them the opportunity to meet criterion #2 or #3.\n\nUiO’s 10 commandments for ethical practice in research\n\n\n  You shall be guided by the rights and duties spelled out in the Law of Academic Freedom.\n  You shall know that honesty is an absolute requisite for scientific work.\n  You shall act in accordance with ethical guidelines for your area of research.\n  You shall give due recognition to colleagues and sources that have aided your own investigation.\n  You shall, if at all possible, participate in collegial analysis and communication of methods and results.\n  You shall be prepared to account for all means and resources that you have received.\n  You shall ensure that your scientific results are solid enough to justify your conclusions and that the raw data/materials on which your publications are based remain intact and available.\n  You shall adhere to the rules that govern publications in your field.\n  You shall strive to be level-headed when you report your results; consequential considerations should include both potential benefits and would-be ethical dilemmas.\n  You shall maintain your scientific competency and also seek to improve it.\n\n\nhttps://www.uio.no/english/for-employees/support/research/ethics/10-commandements.html\n\nMy Policy (Charles)\n\n\n  Giving authorship credit is free, so I’m relaxed about giving credit to those who have contributed to the research process.\n  I tend to have papers with 2-4 authors involved, so it’s easy to make sure that all have a chance to revise the text before submission.\n  Could be hands-on editing of the tex file.\n  Or marking up a printed out copy of the paper.\n  I also write (a few) single author papers; for these, I make clear that it is just me before asking supervisors/mentors to read it.\n\n\nLinking to Code, Data, Video, Audio\n\nCan we include a video in a paper? How about code?\n\nIs it enough to include a Dropbox link? What about YouTube?\n\nWhat happens if YouTube goes out of business?\n\nWebsites change, papers should be forever (or at least a long time…)\n\nLinking to Code and Data\n\n\n\nBecoming more standard these days to link to code or data in a publication. Really good idea:\n\n\n  Reproducibility\n  More convincing for special application areas (e.g., music, robotics, video)\n  Some authors make a “web” version of their paper with videos and interactive stuff: e.g., https://worldmodels.github.io\n\n\nLinking to Data: Zenodo\n\n\n\nLinking to Data:\n\n\n  I usually put videos, audio samples on Zenodo\n  Include link with DOI in paper\n  Can also be “cited”, e.g.:\n\n\n@misc{martin_charles_patrick_2018_1215956,\n  author       = {Martin, Charles Patrick and  Jensenius, Alexander Refsum and\n                  Johnson, Victoria and Bjerkestrand , Kari Anne Vadstensvik},\n  title        = {Stillness Under Tension for Myo+Bela Quartet},\n  month        = apr,\n  year         = 2018,\n  doi          = {10.5281/zenodo.1215956},\n  url          = {https://doi.org/10.5281/zenodo.1215956}\n}\n\n\nLinking to Code: Zenodo as well!\n\n\n\n\n  You can connect Zenodo to a Github Repository!\n  Each release is given a new DOI.\n  A zipfile of the release is stored forever.\n\n\n\n\nMake a video!\n\n\n  Many conferences ask for a video demo or short explanation (e.g., 60 seconds)\n  Great idea for communicating research!\n\n\n\n\n",
        "content": "\n\n\n\n    Creative Prediction with Neural Networks\n    A course in ML/AI for creative expression\n\n\nReferences, Citations, Publishing, and Authorship\n\nCharles Martin - The Australian National University\n\n\n\t\n\n\n\n\nWhat is a research paper?\n\n\n  \n    Written record of research discoveries\n  \n  \n    Communication to other researchers\n  \n  \n    Package up research for peer-review and criticism!\n  \n  \n    Preserve important knowledge forever (or at least for a long time).\n  \n\n\nHow do we know a paper isn’t lies?\n\nYou don’t!\n\nAfter all, anybody can write anything and put it on the internet.\n\nThis happens!\n\nAt least if a paper has been peer-reviewed, other researchers have also agreed that it is probably not lies.\n\nSee more about this\n\nWhy do we care?\n\nWhen you cite references in your own research, you are building on knowledge.\n\nYour references section provides information about where that knowledge comes from.\n\nExaminers will read your reference section! Why?\n\n\n  \n    shows student’s ability to critically engage with knowledge\n  \n  \n    shows student engaging with recent research, cutting edge, and “difficult” sources\n  \n\n\nFrom research to published paper…\n\n\n\nThere is a pathway from research to a published paper!\n\nAll research outputs are valuable, but they have different properties.\n\nAsk yourself: how do I know this output isn’t lies?\n\nCiting the right paper\n\nMultiple versions of a paper can exist! How do we cite the right one?\n\n\n  Pre-print version on arxiv.org\n  Publisher’s version on a journal website (behind paywall!)\n  “Post-print” version on university website (often required by law!)\n  Version on author’s own website (e.g., charlesmartin.com.au/publications)\n\n\nAs a first preference, always cite the publisher’s version! (this is the one that has been peer-reviewed).\n\nIf a paper is not published, cite the ArXiv or author’s version but remember: it might not be peer-reviewed.\n\nFinding the publisher’s version can take some research, particularly for CS conference publications!\n\ncitations\n\n\n\nA citation is an accurate way to find a reference.\n\nNeed to include: what (title), who (authors), when (date), where it came from (publisher, source), where to find it (DOI, URL)\n\nImage Credit: Author-Date Citation System (APA Style Guidelines)\n\nTotally valid to cite blog posts, youtube videos, tweets, etc etc, see the APA Style Blog for ideas.\n\nDOIs (Digital Object Identifier)\n\nThere’s a special kind of URL called a “DOI” available for many academic publications. The idea is that the DOI will be unique and never change (unlike URLs).\n\nAn example DOI is: 10.1000/182\n\nAnd we can follow it by going to: https://doi.org/10.1000/182\n\nThe prefix/suffix format identifies the publisher or registrant (prefix) and the specific object (suffix).\n\nDOIs aren’t just for papers! They can be for any piece of data (code, files, videos, images, figures, etc.)\n\nIf a DOI is available, use it instead of a URL!\n\nBibTeX\n\nI expect everybody to organise their references with a BibTeX database: here’s some info\n\n@inproceedings{McArthur2021vj,\n  author = {McArthur, Robert Neil and Martin, Charles Patrick},\n  title = {An Application for Evolutionary Music Composition Using Autoencoders},\n  year = {2021}\n  booktitle = {Artificial Intelligence in Music, Sound, Art and Design: 10th International Conference, EvoMUSART 2021},\n  pages = {443--458},\n  doi = {10.1007/978-3-030-72914-1_29},\n  publisher = {Springer International Publishing},\n}\n\n\nBibTeX for difficult items…\n\nSome things don’t fit well into BibTeX… you can use the unpublished or misc type and experiment with how to include appropriate information.\n\n@unpublished{Martin2017ap,\n  author = {Martin, Charles and Hunter, Alexander},\n  title = {Andromeda is Coming [Album]},\n  year = {2017}\n  month = jul,\n  note = {Music recording.},\n  url = {https://collectedresonances.bandcamp.com/album/andromeda-is-coming},\n}\n\n\n\n\nWriting your own papers…\n\nAuthoring your own papers!\n\nMany honours/Masters/research projects can be formulated in an academic paper.\n\n\n  Your final paper may not be quite acceptable for publication yet.\n  Your supervisor(s) can help you get it up to scratch.\n  We can also help you attend a conference to present your work.\n  ( We would expect to be co-authors of a paper after collaborating on these tasks! :-D )\n\n\nPublishing a Paper!\n\n\n  Academics publish research as part of their job!\n  Usually publishing is tied to important stuff: e.g., promotion, funding, number of PhD places, prestige.\n  We also LIKE to publish! We want to tell people about our work and want to read about what others are doing!\n  PhD students usually have to publish in order to get their degree!\n  Publishing can look a bit different between disciplines.\n  In computer science we have two main ways to publish:\n  Conferences: e.g., International Conference on Machine Learning\n  Journals: e.g., IEEE Transactions on Neural Networks and Learning Systems\n\n\nJournal\n\n\n\n\n  Send a manuscript to the journal when ready.\n  1+ rounds of reviewing and changes\n  Manuscript is turned into an article by the journal staff\n  Published online (sometimes print…)\n\n\nConference\n\n\n  Submit paper on conference website by a deadline.\n  All papers are reviewed at once.\n  Usually only one round of review! → accepted or rejected.\n  If accepted, polish up paper for “camera ready”\n  Attend conference, give talk (10-15mins) or present a poster. Meet other researchers, have a good time.\n  Paper appears in the “Proceedings of the Conference” which are published online.\n\n\nComputer Science\n\n\n  Conference papers have page limits\n  Journal articles tend to be a bit longer\n  Most CS academics treat conference papers and journal articles as being equally “worthy” of reading.\n  Other fields consider conference papers to be inconsequential or early-stage, will only look at articles and books for serious contributions.\n  Normally we would suggest honours/Master students go for a conference paper!\n\n\nQuality and Competition\n\nNot all conferences equal!\n\nAcceptance rates:\n\n\n  Difficult conferences &lt; 20% papers accepted\n  \n    Easier conferences &gt; 50% papers accepted → often focussed on a niche area.\n  \n  For “prestigious” conferences and journals, there are so many (1000s) submissions, that editors screen out papers that are unlikely to be accepted by reviewers.\n  Ranking for conferences tends to be anecdotal.\n  Journals often ranked by “Impact Factor”, roughly, “how many citations does the average paper get in one year”\n  Academics often consider too much focus on rankings to be harmful.\n  But we usually look anyway, as we want evidence that of excellence.\n\n\nFor you!\n\n\n  Do you want to take your research further?\n  Do you want to attend a conference?\n  \n    Submit your paper to a conference!\n  \n  Requires more revision after deadline.\n  Meetings with mentors/supervisors to make sure work is up to scratch.\n\n\nWho should be an author?\n\n\n  Who deserves credit for a work?\n  Movies do this well… everybody involved is listed with their role made very clear!\n  Academic Papers are not so clear…\n\n\n\n\n\nGiving Credit Where Credit is Due\n\n\n\nhttp://phdcomics.com/comics/archive.php?comicid=562\n\nProblems…\n\n\n\n\n\n\n\n\n\nDifferent Subjects Do this Differently\n\n\n\nVancouver Recommendations - 4 Criteria\n\n\n  Substantial contributions to the conception or design of the work; or the acquisition, analysis, or interpretation of data for the work; AND\n  Drafting the work or revising it critically for important intellectual content; AND\n  Final approval of the version to be published; AND\n  Agreement to be accountable for all aspects of the work in ensuring that questions related to the accuracy or integrity of any part of the work are appropriately investigated and resolved.\n\n\nAll those designated as authors should meet all four criteria for authorship, and all who meet the four criteria should be identified as authors. Those who do not meet all four criteria should be acknowledged.\n\n…\n\nThese authorship criteria are intended to reserve the status of authorship for those who deserve credit and can take responsibility for the work. The criteria are not intended for use as a means to disqualify colleagues from authorship who otherwise meet authorship criteria by denying them the opportunity to meet criterion #2 or #3.\n\nUiO’s 10 commandments for ethical practice in research\n\n\n  You shall be guided by the rights and duties spelled out in the Law of Academic Freedom.\n  You shall know that honesty is an absolute requisite for scientific work.\n  You shall act in accordance with ethical guidelines for your area of research.\n  You shall give due recognition to colleagues and sources that have aided your own investigation.\n  You shall, if at all possible, participate in collegial analysis and communication of methods and results.\n  You shall be prepared to account for all means and resources that you have received.\n  You shall ensure that your scientific results are solid enough to justify your conclusions and that the raw data/materials on which your publications are based remain intact and available.\n  You shall adhere to the rules that govern publications in your field.\n  You shall strive to be level-headed when you report your results; consequential considerations should include both potential benefits and would-be ethical dilemmas.\n  You shall maintain your scientific competency and also seek to improve it.\n\n\nhttps://www.uio.no/english/for-employees/support/research/ethics/10-commandements.html\n\nMy Policy (Charles)\n\n\n  Giving authorship credit is free, so I’m relaxed about giving credit to those who have contributed to the research process.\n  I tend to have papers with 2-4 authors involved, so it’s easy to make sure that all have a chance to revise the text before submission.\n  Could be hands-on editing of the tex file.\n  Or marking up a printed out copy of the paper.\n  I also write (a few) single author papers; for these, I make clear that it is just me before asking supervisors/mentors to read it.\n\n\nLinking to Code, Data, Video, Audio\n\nCan we include a video in a paper? How about code?\n\nIs it enough to include a Dropbox link? What about YouTube?\n\nWhat happens if YouTube goes out of business?\n\nWebsites change, papers should be forever (or at least a long time…)\n\nLinking to Code and Data\n\n\n\nBecoming more standard these days to link to code or data in a publication. Really good idea:\n\n\n  Reproducibility\n  More convincing for special application areas (e.g., music, robotics, video)\n  Some authors make a “web” version of their paper with videos and interactive stuff: e.g., https://worldmodels.github.io\n\n\nLinking to Data: Zenodo\n\n\n\nLinking to Data:\n\n\n  I usually put videos, audio samples on Zenodo\n  Include link with DOI in paper\n  Can also be “cited”, e.g.:\n\n\n@misc{martin_charles_patrick_2018_1215956,\n  author       = {Martin, Charles Patrick and  Jensenius, Alexander Refsum and\n                  Johnson, Victoria and Bjerkestrand , Kari Anne Vadstensvik},\n  title        = {Stillness Under Tension for Myo+Bela Quartet},\n  month        = apr,\n  year         = 2018,\n  doi          = {10.5281/zenodo.1215956},\n  url          = {https://doi.org/10.5281/zenodo.1215956}\n}\n\n\nLinking to Code: Zenodo as well!\n\n\n\n\n  You can connect Zenodo to a Github Repository!\n  Each release is given a new DOI.\n  A zipfile of the release is stored forever.\n\n\n\n\nMake a video!\n\n\n  Many conferences ask for a video demo or short explanation (e.g., 60 seconds)\n  Great idea for communicating research!\n\n\n\n\n",
        "url": "/presentations/08-publications-and-references/"
      },
    
      {
        "title": "Recent Progress in Intelligent Musical Instruments",
        "excerpt": "\n\n\n\n\n  Recent Progress in Intelligent Musical Instruments\n  \n\n\nDr Charles Martin - The Australian National University\n\nweb: charlesmartin.com.au&nbsp;&nbsp;&nbsp;&nbsp; twitter/github: @cpmpercussion\n\n\n\n\n  \n  Ngunnawal &amp; Ngambri Country\n  \n\n\n\n  \n  vision\n  \n\n\nIntelligent Musical Instruments become a normal part of musical\nperformance and production.\n\n\n  \n  why?\n  \n\n\nAssist professional musicians &amp; composers\n\nEngage novice musicians &amp; students\n\nReveal creative interaction with intelligent systems\n\nCreate new kinds of music!\n\n\n  \n  making intelligent musical predictions\n  \n\n\nHistory\n\n\n  \n  Digital Musical Instruments (1979ish-)\n  \n\n\n\n  \n  Voyager - George E Lewis (1986-)\n  \n\n\n\n  \n  Continuator - François Pachet (2001)\n  \n\n\n\n  \n  Wekinator - Rebecca Fiebrink (2009-)\n  \n\n\n\n  \n  Magenta Project - Google (2016-)\n  \n\n\nwhere are all the intelligent musical instruments?\n\n\n\n\n  \n\n\nPerformance data is diverse\n\n\n\n\n  \n    \n      Music Systems\n      Data\n    \n  \n  \n    \n      Score / Notation\n      Symbolic Music, Image\n    \n    \n      Digital Instruments\n      MIDI\n    \n    \n      Recording &amp; Production\n      Digital  Audio\n    \n    \n      New Musical Interfaces\n      Gestural and Sensor Data\n    \n    \n      Show Control\n      Video, Audio, Lighting, Control Signals\n    \n  \n\n\nPredicting sequences\n\n\n\nInteracting with predictions\n\n\n\ncreating an orchestra of intelligent instruments…\n\n\n\nInteractive RNN Instrument\n\n\n\n\n  Generates endless music with a melody RNN.\n  Switchable Dataset.\n  Controls for sampling “temperature”.\n\n\n\n  \n  Physical Intelligent Instrument\n  \n\n\n\n\n\n  \n  GestureRNN\n  \n\n\nGestureRNN\n\n\n\n\n  Predicts 1 of 9 “gestures” for three AI performers.\n  Trained on labelled data from 5 hours of quartet performances.\n  Actual “sounds” are chunks of each gesture played back.\n\n\n\n  \n  RoboJam\n  \n\n\n\n\nRobojam and Microjam\n\n\n\n\n  Predicts next touch location in screen (x, y, dt).\n  Trained on ~1500 5s performances.\n  Produces duet “responses” to the user.\n\n\nMixture Density Network\n\n\n\n\n  \n\n\n\n\n\n  \n  IMPS System\n  \n\n\nIMPS System\n\n\n\n\n  Opinionated Neural Network for interacting with NIMES.\n  Automatically collects data and trains.\n  “Wekinator” for deep learning?\n\n\nThree easy steps…\n\n\n  Collect some data: IMPS logs interactions automatically to build up a dataset\n  Train an MDRNN: IMPS includes good presets, no need to train for days/weeks\n  Perform! IMPS includes three interaction modes, scope to extend in future!\n\n\n\n  \n\n\n\n  \n\n\n\n  \n\n\nExperiment: Is this practical?\n\n\n  Is it practical for real-time use?\n  How do the MDRNN parameters affect time per prediction?\n  What are “good defaults” for training parameters?\n  Do you need a powerful/expensive computer?\n\n\nTest Systems\n\n\n\nResults: Time per prediction\n\n\n\nTime per prediction (ms) with different sizes of LSTM layers.\n\nResults: Time per prediction\n\n\n\nTime per prediction (ms) with different MDN output dimensions. (64\nLSTM units)\n\nResults: Training Error vs Validation Set Error\n\n\n\n12K sample dataset (15 minutes of performance)\n\nTakeaway: Smallest model best for small datasets. Don’t bother training for\ntoo long.\n\nResults: Training Error vs Validation Set Error\n\n\n\n100K sample dataset (120 minutes of performance)\n\nTakeaway: 64- and 128-unit model still best!\n\nResults: Exploring Generation\n\n\n\nTakeaway: Make Gaussians less diverse, make categorical more diverse.\n\n\n\n\n  \n  Embodied Predictive Musical Instrument (EMPI)\n  \n\n\nEmbodied Predictive Musical Instrument (EMPI)\n\n\n\n\n  Predicts next movement and time, represents physically.\n  Experiments with interaction mappings; mainly focussed on call-response\n  Weird and confusing/fun?\n\n\nTraining Data\n\n\n\n\n\n\n\nGenerated Data\n\n\n\n\n\n\n  \n\n\n\n  \n\n\n\n  \n\n\n\n  \n  Improvisations with EMPI\n  \n\n\n\n  \n    12 participants\n  \n  \n    two independent factors: model and feedback\n  \n  \n    model: human, synthetic, noise\n  \n  \n    feedback: motor on, motor off\n  \n\n\nResults: Survey\n\n\n\nChange of ML model had significant effect: Q2, Q4, Q5, Q6, Q7\n\nResults: Survey\n\n\n  \n    human model most “related”, noise was least\n  \n  \n    human model most “musically creative”\n  \n  \n    human model easiest to “influence”\n  \n  \n    noise model not rated badly!\n  \n\n\nParticipants generally preferred human or synth, but not always!\n\nResults: Performance Length\n\n\n\nHuman and synth: more range of performance lengths with motor on.\n\nNoise: more range with motor off.\n\nTakeaways\n\nStudied self-contained intelligent instrument in genuine performance.\n\nPhysical representation could be polarising.\n\nPerformers work hard to understand and influence ML model.\n\nConstrained, intelligent instrument can produce a compelling experience.\n\n\n\n\n  \n  How can intelligent instruments help us make music?\n  \n\n\nEmulate or enhance ensemble experience\n\nEngage in call-and-response improvisation\n\nModel a performer’s personal style\n\nModify/improve performance actions in place\n\n\n  \n  Research questions...\n  \n\n\nAre ML models practical for musical prediction?\n\nAre intelligent instruments useful to musicians?\n\nWhat happens when musicians and instrument co-adapt?\n\nCan a musical practice be represented as a dataset?\n\nWhat does a intelligent instrument album / concert sound like?\n\n\n\n\n\n\n\n\n\n\nThanks!\n\n\n\n\n  IMPS on GitHub\n  creative ML: creativeprediction.xyz\n  Twitter/Github: @cpmpercussion\n  Homepage: charlesmartin.com.au\n\n\n",
        "content": "\n\n\n\n\n  Recent Progress in Intelligent Musical Instruments\n  \n\n\nDr Charles Martin - The Australian National University\n\nweb: charlesmartin.com.au&nbsp;&nbsp;&nbsp;&nbsp; twitter/github: @cpmpercussion\n\n\n\n\n  \n  Ngunnawal &amp; Ngambri Country\n  \n\n\n\n  \n  vision\n  \n\n\nIntelligent Musical Instruments become a normal part of musical\nperformance and production.\n\n\n  \n  why?\n  \n\n\nAssist professional musicians &amp; composers\n\nEngage novice musicians &amp; students\n\nReveal creative interaction with intelligent systems\n\nCreate new kinds of music!\n\n\n  \n  making intelligent musical predictions\n  \n\n\nHistory\n\n\n  \n  Digital Musical Instruments (1979ish-)\n  \n\n\n\n  \n  Voyager - George E Lewis (1986-)\n  \n\n\n\n  \n  Continuator - François Pachet (2001)\n  \n\n\n\n  \n  Wekinator - Rebecca Fiebrink (2009-)\n  \n\n\n\n  \n  Magenta Project - Google (2016-)\n  \n\n\nwhere are all the intelligent musical instruments?\n\n\n\n\n  \n\n\nPerformance data is diverse\n\n\n\n\n  \n    \n      Music Systems\n      Data\n    \n  \n  \n    \n      Score / Notation\n      Symbolic Music, Image\n    \n    \n      Digital Instruments\n      MIDI\n    \n    \n      Recording &amp; Production\n      Digital  Audio\n    \n    \n      New Musical Interfaces\n      Gestural and Sensor Data\n    \n    \n      Show Control\n      Video, Audio, Lighting, Control Signals\n    \n  \n\n\nPredicting sequences\n\n\n\nInteracting with predictions\n\n\n\ncreating an orchestra of intelligent instruments…\n\n\n\nInteractive RNN Instrument\n\n\n\n\n  Generates endless music with a melody RNN.\n  Switchable Dataset.\n  Controls for sampling “temperature”.\n\n\n\n  \n  Physical Intelligent Instrument\n  \n\n\n\n\n\n  \n  GestureRNN\n  \n\n\nGestureRNN\n\n\n\n\n  Predicts 1 of 9 “gestures” for three AI performers.\n  Trained on labelled data from 5 hours of quartet performances.\n  Actual “sounds” are chunks of each gesture played back.\n\n\n\n  \n  RoboJam\n  \n\n\n\n\nRobojam and Microjam\n\n\n\n\n  Predicts next touch location in screen (x, y, dt).\n  Trained on ~1500 5s performances.\n  Produces duet “responses” to the user.\n\n\nMixture Density Network\n\n\n\n\n  \n\n\n\n\n\n  \n  IMPS System\n  \n\n\nIMPS System\n\n\n\n\n  Opinionated Neural Network for interacting with NIMES.\n  Automatically collects data and trains.\n  “Wekinator” for deep learning?\n\n\nThree easy steps…\n\n\n  Collect some data: IMPS logs interactions automatically to build up a dataset\n  Train an MDRNN: IMPS includes good presets, no need to train for days/weeks\n  Perform! IMPS includes three interaction modes, scope to extend in future!\n\n\n\n  \n\n\n\n  \n\n\n\n  \n\n\nExperiment: Is this practical?\n\n\n  Is it practical for real-time use?\n  How do the MDRNN parameters affect time per prediction?\n  What are “good defaults” for training parameters?\n  Do you need a powerful/expensive computer?\n\n\nTest Systems\n\n\n\nResults: Time per prediction\n\n\n\nTime per prediction (ms) with different sizes of LSTM layers.\n\nResults: Time per prediction\n\n\n\nTime per prediction (ms) with different MDN output dimensions. (64\nLSTM units)\n\nResults: Training Error vs Validation Set Error\n\n\n\n12K sample dataset (15 minutes of performance)\n\nTakeaway: Smallest model best for small datasets. Don’t bother training for\ntoo long.\n\nResults: Training Error vs Validation Set Error\n\n\n\n100K sample dataset (120 minutes of performance)\n\nTakeaway: 64- and 128-unit model still best!\n\nResults: Exploring Generation\n\n\n\nTakeaway: Make Gaussians less diverse, make categorical more diverse.\n\n\n\n\n  \n  Embodied Predictive Musical Instrument (EMPI)\n  \n\n\nEmbodied Predictive Musical Instrument (EMPI)\n\n\n\n\n  Predicts next movement and time, represents physically.\n  Experiments with interaction mappings; mainly focussed on call-response\n  Weird and confusing/fun?\n\n\nTraining Data\n\n\n\n\n\n\n\nGenerated Data\n\n\n\n\n\n\n  \n\n\n\n  \n\n\n\n  \n\n\n\n  \n  Improvisations with EMPI\n  \n\n\n\n  \n    12 participants\n  \n  \n    two independent factors: model and feedback\n  \n  \n    model: human, synthetic, noise\n  \n  \n    feedback: motor on, motor off\n  \n\n\nResults: Survey\n\n\n\nChange of ML model had significant effect: Q2, Q4, Q5, Q6, Q7\n\nResults: Survey\n\n\n  \n    human model most “related”, noise was least\n  \n  \n    human model most “musically creative”\n  \n  \n    human model easiest to “influence”\n  \n  \n    noise model not rated badly!\n  \n\n\nParticipants generally preferred human or synth, but not always!\n\nResults: Performance Length\n\n\n\nHuman and synth: more range of performance lengths with motor on.\n\nNoise: more range with motor off.\n\nTakeaways\n\nStudied self-contained intelligent instrument in genuine performance.\n\nPhysical representation could be polarising.\n\nPerformers work hard to understand and influence ML model.\n\nConstrained, intelligent instrument can produce a compelling experience.\n\n\n\n\n  \n  How can intelligent instruments help us make music?\n  \n\n\nEmulate or enhance ensemble experience\n\nEngage in call-and-response improvisation\n\nModel a performer’s personal style\n\nModify/improve performance actions in place\n\n\n  \n  Research questions...\n  \n\n\nAre ML models practical for musical prediction?\n\nAre intelligent instruments useful to musicians?\n\nWhat happens when musicians and instrument co-adapt?\n\nCan a musical practice be represented as a dataset?\n\nWhat does a intelligent instrument album / concert sound like?\n\n\n\n\n\n\n\n\n\n\nThanks!\n\n\n\n\n  IMPS on GitHub\n  creative ML: creativeprediction.xyz\n  Twitter/Github: @cpmpercussion\n  Homepage: charlesmartin.com.au\n\n\n",
        "url": "/presentations/2021-recent-progress-intelligent-musical-instruments/"
      },
    
      {
        "title": "Embedding Embodied Music Generation",
        "excerpt": "\n\n\n\n\n  Embedding Embodied Music Generation\n  \n\n\nDr Charles Martin - The Australian National University\n\nweb: charlesmartin.com.au&nbsp;&nbsp;&nbsp;&nbsp; twitter/github: @cpmpercussion\n\n\n\n\n  \n  Ngunnawal &amp; Ngambri Country\n  \n\n\n\n  \n\n\n\n  \n  Embodied Music Generation\n  \n\n\n\n  \n    note generation: generate “symbolic” music—notes (A, B, C, half-note, quaver, etc.). Abstract version of sounds created by some musical instruments.\n  \n  \n    embodied gesture generation: generate the movements a performer makes to operate a particular musical instrument.\n  \n\n\nthis project explores embodied gesture generation in an improvised electronic music context!\n\n\n  \n  Why do this?\n  \n\n\n\n  \n    Lots of musical instruments don’t use “notes”\n  \n  \n    e.g., turntable, mixer, modular synthesiser, effects pedal, etc\n  \n  \n    what does “intelligence” and “co-creation” look like in these instruments?\n  \n  \n    can we incorporate generative AI into a longer-term performance practice?\n  \n\n\n\n\n\n  \n  Embodied Predictive Musical Instrument (EMPI)\n  \n\n\nEmbodied Predictive Musical Instrument (EMPI)\n\n\n\n\n  Predicts next movement and time, represents physically.\n  Experiments with interaction mappings; mainly focussed on call-response\n  Weird and confusing/fun?\n\n\nTraining Data\n\n\n\n\n\n\n\nGenerated Data\n\n\n\n\n\n\n  \n\n\n\n  \n\n\n\n  \n\n\n\n  \n  Improvisations with EMPI\n  \n\n\n\n  \n    12 participants\n  \n  \n    two independent factors: model and feedback\n  \n  \n    model: human, synthetic, noise\n  \n  \n    feedback: motor on, motor off\n  \n\n\nResults: Survey\n\n\n\nChange of ML model had significant effect: Q2, Q4, Q5, Q6, Q7\n\nResults: Survey\n\n\n  \n    human model most “related”, noise was least\n  \n  \n    human model most “musically creative”\n  \n  \n    human model easiest to “influence”\n  \n  \n    noise model not rated badly!\n  \n\n\nParticipants generally preferred human or synth, but not always!\n\nResults: Performance Length\n\n\n\nHuman and synth: more range of performance lengths with motor on.\n\nNoise: more range with motor off.\n\nTakeaways\n\nStudied self-contained intelligent instrument in genuine performance.\n\nPhysical representation could be polarising.\n\nPerformers work hard to understand and influence ML model.\n\nConstrained, intelligent instrument can produce a compelling experience.\n\n\n\n\n  \n\n\nGenerative AI System\n\n\n  \n    gestural predictions are made by a Mixture Density Recurrent Neural Network (implemented using “Interactive Music Prediction System”—IMPS)\n  \n  \n    MDRNN: an extension of common LSTM/RNN designs to allow expressive predictions of multiple continuous variables.\n  \n  \n    MDRNN specs: 2 32-unit LSTM layers, 9-dimensional mixture density layer (8 knobs + time)\n  \n  \n    IMPS: A CLI Python program that provides MDRNN, data collection, training and interaction features.\n  \n  \n    communicates with music software over OSC (Open Sound Control)\n  \n  \n    in this case, MDRNN is configured for “call-and-response” interaction (or “continuation”)\n  \n\n\n\n  \n\n\n\n  \n  Performances and Experiences\n  \n\n\n\n  \n    deployed in performance since 2019\n  \n  \n    so it works! and it’s practical!\n  \n  \n    but is it better than a random walk generator?\n  \n\n\n\n  \n\n\n\n  \n\n\n\n  \n\n\n\n  \n  Influence and Co-Creation\n  \n\n\n\n  \n    can be steered (a little bit) by performer’s gestures\n  \n  \n    tends to continue adjusting knobs the performer last used\n  \n  \n    learns interesting behaviours from data (moving one vs multiple knobs, pauses, continuous changes)\n  \n  \n    good to for performer to have a different task to work on.\n  \n  \n    also important to allow performer to “just listen”\n  \n\n\n\n  \n  Small Data and Co-Adaptation\n  \n\n\n\n  \n    interactions from each performance are saved\n  \n  \n    some of these have been incorporated into training datasets\n  \n  \n    co-adaptive: system grows and changes along with the performer (yet to be studied rigorously)\n  \n\n\n\n  \n  Thanks!\n  \n\n\n\n  IMPS on GitHub\n  creative ML: creativeprediction.xyz\n  Twitter/Github: @cpmpercussion\n  Homepage: charlesmartin.com.au\n\n\n",
        "content": "\n\n\n\n\n  Embedding Embodied Music Generation\n  \n\n\nDr Charles Martin - The Australian National University\n\nweb: charlesmartin.com.au&nbsp;&nbsp;&nbsp;&nbsp; twitter/github: @cpmpercussion\n\n\n\n\n  \n  Ngunnawal &amp; Ngambri Country\n  \n\n\n\n  \n\n\n\n  \n  Embodied Music Generation\n  \n\n\n\n  \n    note generation: generate “symbolic” music—notes (A, B, C, half-note, quaver, etc.). Abstract version of sounds created by some musical instruments.\n  \n  \n    embodied gesture generation: generate the movements a performer makes to operate a particular musical instrument.\n  \n\n\nthis project explores embodied gesture generation in an improvised electronic music context!\n\n\n  \n  Why do this?\n  \n\n\n\n  \n    Lots of musical instruments don’t use “notes”\n  \n  \n    e.g., turntable, mixer, modular synthesiser, effects pedal, etc\n  \n  \n    what does “intelligence” and “co-creation” look like in these instruments?\n  \n  \n    can we incorporate generative AI into a longer-term performance practice?\n  \n\n\n\n\n\n  \n  Embodied Predictive Musical Instrument (EMPI)\n  \n\n\nEmbodied Predictive Musical Instrument (EMPI)\n\n\n\n\n  Predicts next movement and time, represents physically.\n  Experiments with interaction mappings; mainly focussed on call-response\n  Weird and confusing/fun?\n\n\nTraining Data\n\n\n\n\n\n\n\nGenerated Data\n\n\n\n\n\n\n  \n\n\n\n  \n\n\n\n  \n\n\n\n  \n  Improvisations with EMPI\n  \n\n\n\n  \n    12 participants\n  \n  \n    two independent factors: model and feedback\n  \n  \n    model: human, synthetic, noise\n  \n  \n    feedback: motor on, motor off\n  \n\n\nResults: Survey\n\n\n\nChange of ML model had significant effect: Q2, Q4, Q5, Q6, Q7\n\nResults: Survey\n\n\n  \n    human model most “related”, noise was least\n  \n  \n    human model most “musically creative”\n  \n  \n    human model easiest to “influence”\n  \n  \n    noise model not rated badly!\n  \n\n\nParticipants generally preferred human or synth, but not always!\n\nResults: Performance Length\n\n\n\nHuman and synth: more range of performance lengths with motor on.\n\nNoise: more range with motor off.\n\nTakeaways\n\nStudied self-contained intelligent instrument in genuine performance.\n\nPhysical representation could be polarising.\n\nPerformers work hard to understand and influence ML model.\n\nConstrained, intelligent instrument can produce a compelling experience.\n\n\n\n\n  \n\n\nGenerative AI System\n\n\n  \n    gestural predictions are made by a Mixture Density Recurrent Neural Network (implemented using “Interactive Music Prediction System”—IMPS)\n  \n  \n    MDRNN: an extension of common LSTM/RNN designs to allow expressive predictions of multiple continuous variables.\n  \n  \n    MDRNN specs: 2 32-unit LSTM layers, 9-dimensional mixture density layer (8 knobs + time)\n  \n  \n    IMPS: A CLI Python program that provides MDRNN, data collection, training and interaction features.\n  \n  \n    communicates with music software over OSC (Open Sound Control)\n  \n  \n    in this case, MDRNN is configured for “call-and-response” interaction (or “continuation”)\n  \n\n\n\n  \n\n\n\n  \n  Performances and Experiences\n  \n\n\n\n  \n    deployed in performance since 2019\n  \n  \n    so it works! and it’s practical!\n  \n  \n    but is it better than a random walk generator?\n  \n\n\n\n  \n\n\n\n  \n\n\n\n  \n\n\n\n  \n  Influence and Co-Creation\n  \n\n\n\n  \n    can be steered (a little bit) by performer’s gestures\n  \n  \n    tends to continue adjusting knobs the performer last used\n  \n  \n    learns interesting behaviours from data (moving one vs multiple knobs, pauses, continuous changes)\n  \n  \n    good to for performer to have a different task to work on.\n  \n  \n    also important to allow performer to “just listen”\n  \n\n\n\n  \n  Small Data and Co-Adaptation\n  \n\n\n\n  \n    interactions from each performance are saved\n  \n  \n    some of these have been incorporated into training datasets\n  \n  \n    co-adaptive: system grows and changes along with the performer (yet to be studied rigorously)\n  \n\n\n\n  \n  Thanks!\n  \n\n\n\n  IMPS on GitHub\n  creative ML: creativeprediction.xyz\n  Twitter/Github: @cpmpercussion\n  Homepage: charlesmartin.com.au\n\n\n",
        "url": "/presentations/2022-NIME-embedding-embodied-music-generation/"
      },
    
      {
        "title": "Generative AI and Music",
        "excerpt": "\n\n\n\n\n  Generative AI and Music\n  \n\n\nDr Charles Martin - The Australian National University\n\nweb: charlesmartin.com.au&nbsp;&nbsp;&nbsp;&nbsp; twitter/github: @cpmpercussion\n\n\n\n\n  \n  Generative AI\n  \n\n\n…creates new content.\n\nCreate an image about…\n\n“a person giving a seminar in a baroque hall in black and white.”\n\nWhat can AI do?\n\n\n\nWhat can AI do?\n\n\n\nHow does it work?\n\n\n\nFast forward 50 years.\n\n\n\nUnderstanding “context”\n\n\n\nGenerating sequences\n\n\n\n\n  \n  Generative AI for Making Music\n  \n\n\n\n  \n  Voyager - George E Lewis (1986-)\n  \n\n\n\n  \n  Wekinator - Rebecca Fiebrink (2009-)\n  \n\n\n\n  \n  Magenta Project - Google (2016-)\n  \n\n\nwhere are all the generative AI musical instruments?\n\n\n\n  \n\n\n\n  \n\n\n\n\n  \n\n\n\n  \n\n\nMixture Density Network\n\n\n\n\n  \n  IMPS System\n  \n\n\n\n\n  \n\n\n\n  \n\n\n\n\n  \n  Embodied Musical Predictions\n  \n\n\n\n  \n\n\n\n  \n\n\n\n  \n\n\n\n\n  \n  What is an authentic generative AI system?\n  \n\n\n\n\n  \n  Where will the data come from?\n  \n\n\n\n\n  \n  Hand-Crafted Boutique Artisanal Datasets\n  \n\n\n\n  \n\n\n\n  \n\n\nThanks!\n\n\n\n\n  IMPS on GitHub\n  creative ML: creativeprediction.xyz\n  Twitter/Github: @cpmpercussion\n  Homepage: charlesmartin.com.au\n\n\n\n\n\n\n\n\n\n\n\n",
        "content": "\n\n\n\n\n  Generative AI and Music\n  \n\n\nDr Charles Martin - The Australian National University\n\nweb: charlesmartin.com.au&nbsp;&nbsp;&nbsp;&nbsp; twitter/github: @cpmpercussion\n\n\n\n\n  \n  Generative AI\n  \n\n\n…creates new content.\n\nCreate an image about…\n\n“a person giving a seminar in a baroque hall in black and white.”\n\nWhat can AI do?\n\n\n\nWhat can AI do?\n\n\n\nHow does it work?\n\n\n\nFast forward 50 years.\n\n\n\nUnderstanding “context”\n\n\n\nGenerating sequences\n\n\n\n\n  \n  Generative AI for Making Music\n  \n\n\n\n  \n  Voyager - George E Lewis (1986-)\n  \n\n\n\n  \n  Wekinator - Rebecca Fiebrink (2009-)\n  \n\n\n\n  \n  Magenta Project - Google (2016-)\n  \n\n\nwhere are all the generative AI musical instruments?\n\n\n\n  \n\n\n\n  \n\n\n\n\n  \n\n\n\n  \n\n\nMixture Density Network\n\n\n\n\n  \n  IMPS System\n  \n\n\n\n\n  \n\n\n\n  \n\n\n\n\n  \n  Embodied Musical Predictions\n  \n\n\n\n  \n\n\n\n  \n\n\n\n  \n\n\n\n\n  \n  What is an authentic generative AI system?\n  \n\n\n\n\n  \n  Where will the data come from?\n  \n\n\n\n\n  \n  Hand-Crafted Boutique Artisanal Datasets\n  \n\n\n\n  \n\n\n\n  \n\n\nThanks!\n\n\n\n\n  IMPS on GitHub\n  creative ML: creativeprediction.xyz\n  Twitter/Github: @cpmpercussion\n  Homepage: charlesmartin.com.au\n\n\n\n\n\n\n\n\n\n\n\n",
        "url": "/presentations/2022-generative-ai-arts-music/"
      },
    
      {
        "title": "Performing with a Generative Electronic Music Controller",
        "excerpt": "\n\n\n\n\n  Performing with a Generative Electronic Music Controller\n  \n\n\nDr Charles Martin - The Australian National University\n\nweb: charlesmartin.com.au&nbsp;&nbsp;&nbsp;&nbsp; twitter/github: @cpmpercussion\n\n\n\n\n  \n  Ngunnawal &amp; Ngambri Country\n  \n\n\n\n  \n\n\n\n  \n  Embodied Music Generation\n  \n\n\n\n  \n    note generation: generate “symbolic” music—notes (A, B, C, half-note, quaver, etc.). Abstract version of sounds created by some musical instruments.\n  \n  \n    embodied gesture generation: generate the movements a performer makes to operate a particular musical instrument.\n  \n\n\nthis project explores embodied gesture generation in an improvised electronic music context!\n\n\n  \n  Why do this?\n  \n\n\n\n  \n    Lots of musical instruments don’t use “notes”\n  \n  \n    e.g., turntable, mixer, modular synthesiser, effects pedal, etc\n  \n  \n    what does “intelligence” and “co-creation” look like in these instruments?\n  \n  \n    can we incorporate generative AI into a longer-term performance practice?\n  \n\n\n\n  \n\n\nGenerative AI System\n\n\n  \n    gestural predictions are made by a Mixture Density Recurrent Neural Network (implemented using “Interactive Music Prediction System”—IMPS)\n  \n  \n    MDRNN: an extension of common LSTM/RNN designs to allow expressive predictions of multiple continuous variables.\n  \n  \n    MDRNN specs: 2 32-unit LSTM layers, 9-dimensional mixture density layer (8 knobs + time)\n  \n  \n    IMPS: A CLI Python program that provides MDRNN, data collection, training and interaction features.\n  \n  \n    communicates with music software over OSC (Open Sound Control)\n  \n  \n    in this case, MDRNN is configured for “call-and-response” interaction (or “continuation”)\n  \n\n\n\n  \n\n\n\n  \n  Performances and Experiences\n  \n\n\n\n  \n    deployed in performance since 2019\n  \n  \n    so it works! and it’s practical!\n  \n  \n    but is it better than a random walk generator?\n  \n\n\n\n  \n\n\n\n  \n\n\n\n  \n\n\n\n  \n  Influence and Co-Creation\n  \n\n\n\n  \n    can be steered (a little bit) by performer’s gestures\n  \n  \n    tends to continue adjusting knobs the performer last used\n  \n  \n    learns interesting behaviours from data (moving one vs multiple knobs, pauses, continuous changes)\n  \n  \n    good to for performer to have a different task to work on.\n  \n  \n    also important to allow performer to “just listen”\n  \n\n\n\n  \n  Small Data and Co-Adaptation\n  \n\n\n\n  \n    interactions from each performance are saved\n  \n  \n    some of these have been incorporated into training datasets\n  \n  \n    co-adaptive: system grows and changes along with the performer (yet to be studied rigorously)\n  \n\n\n\n  \n  Thanks!\n  \n\n\n\n  IMPS on GitHub\n  creative ML: creativeprediction.xyz\n  Twitter/Github: @cpmpercussion\n  Homepage: charlesmartin.com.au\n\n\n",
        "content": "\n\n\n\n\n  Performing with a Generative Electronic Music Controller\n  \n\n\nDr Charles Martin - The Australian National University\n\nweb: charlesmartin.com.au&nbsp;&nbsp;&nbsp;&nbsp; twitter/github: @cpmpercussion\n\n\n\n\n  \n  Ngunnawal &amp; Ngambri Country\n  \n\n\n\n  \n\n\n\n  \n  Embodied Music Generation\n  \n\n\n\n  \n    note generation: generate “symbolic” music—notes (A, B, C, half-note, quaver, etc.). Abstract version of sounds created by some musical instruments.\n  \n  \n    embodied gesture generation: generate the movements a performer makes to operate a particular musical instrument.\n  \n\n\nthis project explores embodied gesture generation in an improvised electronic music context!\n\n\n  \n  Why do this?\n  \n\n\n\n  \n    Lots of musical instruments don’t use “notes”\n  \n  \n    e.g., turntable, mixer, modular synthesiser, effects pedal, etc\n  \n  \n    what does “intelligence” and “co-creation” look like in these instruments?\n  \n  \n    can we incorporate generative AI into a longer-term performance practice?\n  \n\n\n\n  \n\n\nGenerative AI System\n\n\n  \n    gestural predictions are made by a Mixture Density Recurrent Neural Network (implemented using “Interactive Music Prediction System”—IMPS)\n  \n  \n    MDRNN: an extension of common LSTM/RNN designs to allow expressive predictions of multiple continuous variables.\n  \n  \n    MDRNN specs: 2 32-unit LSTM layers, 9-dimensional mixture density layer (8 knobs + time)\n  \n  \n    IMPS: A CLI Python program that provides MDRNN, data collection, training and interaction features.\n  \n  \n    communicates with music software over OSC (Open Sound Control)\n  \n  \n    in this case, MDRNN is configured for “call-and-response” interaction (or “continuation”)\n  \n\n\n\n  \n\n\n\n  \n  Performances and Experiences\n  \n\n\n\n  \n    deployed in performance since 2019\n  \n  \n    so it works! and it’s practical!\n  \n  \n    but is it better than a random walk generator?\n  \n\n\n\n  \n\n\n\n  \n\n\n\n  \n\n\n\n  \n  Influence and Co-Creation\n  \n\n\n\n  \n    can be steered (a little bit) by performer’s gestures\n  \n  \n    tends to continue adjusting knobs the performer last used\n  \n  \n    learns interesting behaviours from data (moving one vs multiple knobs, pauses, continuous changes)\n  \n  \n    good to for performer to have a different task to work on.\n  \n  \n    also important to allow performer to “just listen”\n  \n\n\n\n  \n  Small Data and Co-Adaptation\n  \n\n\n\n  \n    interactions from each performance are saved\n  \n  \n    some of these have been incorporated into training datasets\n  \n  \n    co-adaptive: system grows and changes along with the performer (yet to be studied rigorously)\n  \n\n\n\n  \n  Thanks!\n  \n\n\n\n  IMPS on GitHub\n  creative ML: creativeprediction.xyz\n  Twitter/Github: @cpmpercussion\n  Homepage: charlesmartin.com.au\n\n\n",
        "url": "/presentations/2022-haigen-performing-generative-controller/"
      },
    
      {
        "title": "Creating Intelligent Musical Instruments",
        "excerpt": "\n\n\n  Creating Intelligent Musical Instruments\n  \n\n\nDr Charles Martin - The Australian National University\n\nweb: charlesmartin.com.au&nbsp;&nbsp;&nbsp;&nbsp; twitter/github: @cpmpercussion\n\n\n\n\n  \n  Ngunnawal &amp; Ngambri &amp; Ngarigu Country\n  \n\n\n\n  \n  vision\n  \n\n\nIntelligent Musical Instruments become a normal part of musical\nperformance and production.\n\n\n  \n  why?\n  \n\n\nAssist professional musicians &amp; composers\n\nEngage novice musicians &amp; students\n\nCreate new kinds of music!\n\n\n  \n  making intelligent musical predictions\n  \n\n\n\n  \n  predicting sequences\n  \n\n\nInteracting with predictions\n\n\n\nHistory\n\n\n  “Experiments in Musical Intelligence” (1987)\n  Neural Networks for recognising musical gestures (1991)\n  LSTM RNNs for generating music (2002)\n  OMax Musical Agent (2006)\n  Wekinator (2009)\n  Google Magenta MelodyRNN (2016)\n  Magenta Studio (Ableton Plugins) (2019)\n\n\nPerformance data is diverse\n\n\n\n\n  \n    \n      Music Systems\n      Data\n    \n  \n  \n    \n      Score / Notation\n      Symbolic Music, Image\n    \n    \n      Digital Instruments\n      MIDI\n    \n    \n      Recording &amp; Production\n      Digital  Audio\n    \n    \n      New Musical Interfaces\n      Gestural and Sensor Data\n    \n    \n      Show Control\n      Video, Audio, Lighting, Control Signals\n    \n  \n\n\nInteractive RNN Instrument\n\n\n\n\n  Generates endless music with a melody RNN.\n  Switchable Dataset.\n  Controls for sampling “temperature”.\n\n\n\n  \n  Physical Intelligent Instrument\n  \n\n\n\n  \n  GestureRNN\n  \n\n\n\n\n\n\n\nGestureRNN\n\n\n\n\n  Predicts 1 of 9 “gestures” for three AI performers.\n  Trained on labelled data from 5 hours of quartet performances.\n  Actual “sounds” are chunks of each gesture played back.\n\n\n\n  \n  RoboJam\n  \n\n\nRobojam and Microjam\n\n\n\n\n  Predicts next touch location in screen (x, y, dt).\n  Trained on ~1500 5s performances.\n  Produces duet “responses” to the user.\n\n\nMixture Density Network\n\n\n\n\n  \n\n\n\n\n\n  \n  IMPS System\n  \n\n\nIMPS System\n\n\n\n\n  Opinionated Neural Network for interacting with NIMES.\n  Automatically collects data and trains.\n  “Wekinator” for deep learning?\n\n\nThree easy steps…\n\n\n  Collect some data: IMPS logs interactions automatically to build up a dataset\n  Train an MDRNN: IMPS includes good presets, no need to train for days/weeks\n  Perform! IMPS includes three interaction modes, scope to extend in future!\n\n\n\n  \n\n\n\n  \n\n\n\n  \n\n\n\n\n\n  \n  Embodied Predictive Musical Instrument (EMPI)\n  \n\n\nEmbodied Predictive Musical Instrument (EMPI)\n\n\n\n\n  Predicts next movement and time, represents physically.\n  Experiments with interaction mappings; mainly focussed on call-response\n  Weird and confusing/fun?\n\n\n\n  \n\n\nHow to build one\n\n\n  Brain: Raspberry Pi 3/4\n  Interface: Arduino Pro Mini or similar\n  Amplifier: Adafruit Mono 2.5W (PAM8302)\n  Speaker: scavenged from monitor?\n  Case: custom 3D print\n  Software: https://github.com/cpmpercussion/empi\n\n\nSoftware\n\n\n  Sound: Pure Data (pd) running in headless mode\n  Predictions: IMPS (running on RPi)\n  Interface to MCU: MIDI over USB\n\n\nSoftware starts on boot on the RPi, can configure over a network.\n\n\n  \n\n\n\n  \n\n\n\n  \n\n\n\n  \n\n\n\n  \n\n\n\n  \n\n\nTraining Data\n\n\n\n\n\n\n\nGenerated Data\n\n\n\n\n\n\n  \n\n\n\n  \n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  Using Predictions to Make Music\n  \n\n\nEmulate or enhance ensemble experience\n\nEngage in call-and-response improvisation\n\nModel a performer’s personal style\n\nModify/improve performance actions in place\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  Evaluating Predictive Instruments?\n  \n\n\nDoes the ML model make good predictions?\n\nIs this computationally practical?\n\nIs this useful to musicians?\n\nTry out IMPS or EMPI!\n\n\n\n\n  Available on GitHub\n  creativeprediction.xyz/imps\n  Try with your digital musical instruments!\n  Hack if you want!\n  Add an issue with problems/results!\n  Twitter/Github: @cpmpercussion\n  Homepage: charlesmartin.com.au\n\n\n",
        "content": "\n\n\n  Creating Intelligent Musical Instruments\n  \n\n\nDr Charles Martin - The Australian National University\n\nweb: charlesmartin.com.au&nbsp;&nbsp;&nbsp;&nbsp; twitter/github: @cpmpercussion\n\n\n\n\n  \n  Ngunnawal &amp; Ngambri &amp; Ngarigu Country\n  \n\n\n\n  \n  vision\n  \n\n\nIntelligent Musical Instruments become a normal part of musical\nperformance and production.\n\n\n  \n  why?\n  \n\n\nAssist professional musicians &amp; composers\n\nEngage novice musicians &amp; students\n\nCreate new kinds of music!\n\n\n  \n  making intelligent musical predictions\n  \n\n\n\n  \n  predicting sequences\n  \n\n\nInteracting with predictions\n\n\n\nHistory\n\n\n  “Experiments in Musical Intelligence” (1987)\n  Neural Networks for recognising musical gestures (1991)\n  LSTM RNNs for generating music (2002)\n  OMax Musical Agent (2006)\n  Wekinator (2009)\n  Google Magenta MelodyRNN (2016)\n  Magenta Studio (Ableton Plugins) (2019)\n\n\nPerformance data is diverse\n\n\n\n\n  \n    \n      Music Systems\n      Data\n    \n  \n  \n    \n      Score / Notation\n      Symbolic Music, Image\n    \n    \n      Digital Instruments\n      MIDI\n    \n    \n      Recording &amp; Production\n      Digital  Audio\n    \n    \n      New Musical Interfaces\n      Gestural and Sensor Data\n    \n    \n      Show Control\n      Video, Audio, Lighting, Control Signals\n    \n  \n\n\nInteractive RNN Instrument\n\n\n\n\n  Generates endless music with a melody RNN.\n  Switchable Dataset.\n  Controls for sampling “temperature”.\n\n\n\n  \n  Physical Intelligent Instrument\n  \n\n\n\n  \n  GestureRNN\n  \n\n\n\n\n\n\n\nGestureRNN\n\n\n\n\n  Predicts 1 of 9 “gestures” for three AI performers.\n  Trained on labelled data from 5 hours of quartet performances.\n  Actual “sounds” are chunks of each gesture played back.\n\n\n\n  \n  RoboJam\n  \n\n\nRobojam and Microjam\n\n\n\n\n  Predicts next touch location in screen (x, y, dt).\n  Trained on ~1500 5s performances.\n  Produces duet “responses” to the user.\n\n\nMixture Density Network\n\n\n\n\n  \n\n\n\n\n\n  \n  IMPS System\n  \n\n\nIMPS System\n\n\n\n\n  Opinionated Neural Network for interacting with NIMES.\n  Automatically collects data and trains.\n  “Wekinator” for deep learning?\n\n\nThree easy steps…\n\n\n  Collect some data: IMPS logs interactions automatically to build up a dataset\n  Train an MDRNN: IMPS includes good presets, no need to train for days/weeks\n  Perform! IMPS includes three interaction modes, scope to extend in future!\n\n\n\n  \n\n\n\n  \n\n\n\n  \n\n\n\n\n\n  \n  Embodied Predictive Musical Instrument (EMPI)\n  \n\n\nEmbodied Predictive Musical Instrument (EMPI)\n\n\n\n\n  Predicts next movement and time, represents physically.\n  Experiments with interaction mappings; mainly focussed on call-response\n  Weird and confusing/fun?\n\n\n\n  \n\n\nHow to build one\n\n\n  Brain: Raspberry Pi 3/4\n  Interface: Arduino Pro Mini or similar\n  Amplifier: Adafruit Mono 2.5W (PAM8302)\n  Speaker: scavenged from monitor?\n  Case: custom 3D print\n  Software: https://github.com/cpmpercussion/empi\n\n\nSoftware\n\n\n  Sound: Pure Data (pd) running in headless mode\n  Predictions: IMPS (running on RPi)\n  Interface to MCU: MIDI over USB\n\n\nSoftware starts on boot on the RPi, can configure over a network.\n\n\n  \n\n\n\n  \n\n\n\n  \n\n\n\n  \n\n\n\n  \n\n\n\n  \n\n\nTraining Data\n\n\n\n\n\n\n\nGenerated Data\n\n\n\n\n\n\n  \n\n\n\n  \n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  Using Predictions to Make Music\n  \n\n\nEmulate or enhance ensemble experience\n\nEngage in call-and-response improvisation\n\nModel a performer’s personal style\n\nModify/improve performance actions in place\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  Evaluating Predictive Instruments?\n  \n\n\nDoes the ML model make good predictions?\n\nIs this computationally practical?\n\nIs this useful to musicians?\n\nTry out IMPS or EMPI!\n\n\n\n\n  Available on GitHub\n  creativeprediction.xyz/imps\n  Try with your digital musical instruments!\n  Hack if you want!\n  Add an issue with problems/results!\n  Twitter/Github: @cpmpercussion\n  Homepage: charlesmartin.com.au\n\n\n",
        "url": "/presentations/creating-intelligent-instruments-45/"
      },
    
      {
        "title": "Creating Intelligent Instruments with Machine Learning",
        "excerpt": "\n\n\n# Creating Intelligent Instruments with Machine Learning\n\nCharles P Martin\n\nANU Research School of Computer Science \n\nweb: [charlesmartin.com.au](https://charlesmartin.com.au)   twitter/github: @cpmpercussion\n\n![](/assets/predictive-models-in-interactive-music/anu-logo.png) \n\n\n\n\n  Welcome\n\n\n\n\n\n  \n  Ngunnawal &amp; Ngambri &amp; Ngarigu Country\n  \n\n\n&lt;section id=”vision”\n         data-background-image=”/assets/predictive-models-in-interactive-music/rohan-performance-2-performance.JPG”\n         data-background=”#000000”\n         data-background-opacity=0.5&gt;\n\nVision:\n\nIntelligent Musical Instruments become a normal\n  part of musical performance and production.\n&lt;/section&gt;\n\n&lt;section id=”why”\n         data-background-image=”/assets/predictive-models-in-interactive-music/2015-11-28-Electrofringe-Workshop-2.jpg”\n         data-background=”#000000”\n         data-beackground-opacity=0.5&gt;\nWhy?\n&lt;p&gt;Assist professional musicians &amp; composers&lt;/p&gt;\n&lt;p&gt;Engage novice musicians &amp; students&lt;/p&gt;\n&lt;p&gt;Create &lt;i&gt;new kinds of music!&lt;/i&gt;&lt;/p&gt;\n\n&lt;/section&gt;\n\n&lt;section id=”what”\n         data-background-image=”/assets/predictive-models-in-interactive-music/musical-performance-predictions.jpg”\n         data-background=”#000000”\n         data-background-opacity=0.6&gt;\nmaking intelligent musical predictions\n&lt;/section&gt;\n\n\n\n## predicting sequences\n\n![](/assets/predictive-models-in-interactive-music/sequence-learning.png)\n\n\n  So this is sequence learning where we train a model to predict the\n  next element of a sequence.\n  RNNs have long history.\n  Can now generate new data.\n\n\n\n\n\n\n## interacting with predictions\n\n![](/assets/predictive-models-in-interactive-music/predictive-interaction-motivation.png)\n\n\n  Idea here is to embed this model into the interaction loop of NIME\n  so that it can predict future control data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n### Performance data is diverse\n\n![](/assets/predictive-models-in-interactive-music/imps-nimes-examples.jpg)\n\n| Music Systems                   | Data                               |\n|---------------------------------|------------------------------------|\n| Score / Notation                | Symbolic Music, Image              |\n| Digital Instruments             | MIDI                               |\n| Recording &amp; Production          | Digital  Audio                     |\n| New Musical Interfaces          | Gestural and Sensor Data           |\n| Show Control                    | Video, Audio, Lighting, Control Signals |\n\n\n\n\n\n\n\n\n## Interactive RNN Instrument\n\n![Physical Intelligent\nInstrument](/assets/predictive-models-in-interactive-music/physical-intelligent-instrument2.png) \n\n- Generates endless music with a melody RNN.\n- Switchable Dataset.\n- Controls for sampling \"temperature\".\n\n  - Is this a performance instrument? Something between instrument and\n  playback device.\n  - Switchable dataset is a nice idea (Bach, Massive MIDI, Final\n  Fantasy 7)\n  - \"Temperature\" is a good user parameter for experimenting in\n  performance, althought users focussed on the synth change knob.\n\n\n\n\n&lt;section id=”rpiinsttitle”\n         data-background-video=”/assets/predictive-models-in-interactive-music/physical-intelligent-instrument.mp4”\n         data-background-video-loop\n         data-background-opacity=0.7\n         data-background=”#000000”&gt;\nPhysical Intelligent Instrument\n&lt;/section&gt;\n\n\n\n\n\n\n\n&lt;section id=”gesture-rnn-title”\n  id=”whatisthis”\n  data-background-video=”/assets/predictive-models-in-interactive-music/neural-ensemble-interaction.mp4”\n  data-background-video-loop\n  data-background-opacity=0.7\n  data-background=”#000000”&gt;\nGestureRNN\n&lt;/section&gt;\n\n\n\n\n\n  ## GestureRNN\n\n  ![Gesture RNN](/assets/predictive-models-in-interactive-music/gesture-rnn.png) \n\n  - Predicts 1 of 9 \"gestures\" for three AI performers.\n  - Trained on labelled data from 5 hours of quartet performances.\n  - Actual \"sounds\" are chunks of each gesture played back.\n\n  \n    Lessons:\n    - fun to play\n    - music needs beginning and end\n    - limited sonic material\n    - lacks a bit of coherence.\n\n\n\n\n&lt;section\n  id=”robojamtitle”\n  data-background-video=”/assets/predictive-models-in-interactive-music/robojam-drum-response.mp4”\n  data-background-video-loop\n  data-background-opacity=0.7\n  data-background=”#000000”&gt;\nRoboJam\n&lt;/section&gt;\n\n\n\n\n\n## Robojam and Microjam\n\n![Robojam Interaction](/assets/predictive-models-in-interactive-music/robojam-interaction.png) \n\n- Predicts next touch location in screen (x, y, dt).\n- Trained on ~1500 5s performances.\n- Produces duet \"responses\" to the user.\n\n\n  - is this a performance instrument\n  - took a long time to get this to work (MDNs are hard)\n  - tuning of the samplig here is important.\n  - a lot relied on the \"reply\" interaction, but this was a bit of a\n  conceit.\n  - still not sure if this is a \"useful\" performance idea, only\n  performed a few times with microjam\n  - try out the app? contribute to my dataset!\n\n\n\n\n\n  Mixture Density Network\n\n  \n\n\n\n\n\n\n\n&lt;section id=”slide”\n  id=”empititle”\n  data-background-video=”/assets/empi/empi-short-demo.mp4”\n  data-background-video-loop\n  data-background-opacity=0.7\n  data-background=”#000000”&gt;\n Embodied Predictive Musical Instrument (EMPI)\n&lt;/section&gt;\n\n\n\n\n\n## Embodied Predictive Musical Instrument (EMPI)\n\n![Physical Intelligent\nInstrument](/assets/empi/EMPI-system-diagram.jpg) \n\n- Predicts next movement and time, represents physically.\n- Experiments with interaction mappings; mainly focussed on call-response\n- Weird and confusing/fun?\n\n  - neural networks can run on a raspberry pi.\n  - physical output really draws audiences in\n  - difficult to work out how to extend interaction patterns here\n  - this instrument seems to lend itself to synthetic or one-track\n  datasets (need way to switch between them)_\n\n\n\n\n\n\n&lt;section\n  id=”imps”\n  data-background-video=”/assets/predictive-models-in-interactive-music/imps-lightpad-loop.mp4”\n  data-background-video-loop\n  data-background-opacity=0.7\n  data-background=”#000000”&gt;\nIMPS system\n&lt;/section&gt;\n\n\n\n\n\n  ## IMPS System\n\n  ![](/assets/predictive-models-in-interactive-music/IMPS_connection_example.png) \n\n  - Opinionated Neural Network for interacting with NIMES.\n  - Automatically collects data and trains.\n  - \"Wekinator\" for deep learning?\n  \n  \n    - very small datasets work! (for something)\n    - training on regular computers works! (up to a point)\n    - automatic data recording leads to a kind of \"practice\" for the\n    neural network\n    - do we need data curation as well?\n    - looking for users!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;section id=”whattodo”\n         data-background-image=”/assets/predictive-models-in-interactive-music/ipad-ensemble-2015.jpg”\n         data-background-opacity=0.5\n         data-background=”#000000”&gt;\nUsing Predictions to Make Music\n\n  Emulate or enhance ensemble experience\n  Engage in call-and-response improvisation\n  Model a performer's personal style\n  Modify/improve performance actions in place\n\n&lt;/section&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;section id=”evaluation”\n         data-background-image=”/assets/predictive-models-in-interactive-music/metatone-20170529-ifi.jpg”\n         data-background-opacity=0.5\n         data-background=”#000000”&gt;\nEvaluating Predictive Instruments?\n\n    Does the ML model make good predictions?\n    Is this computationally practical?\n    Is this useful to musicians?\n  \n&lt;/section&gt;\n\n\n  IMPS works with traditional instruments too!\n\n\n\n\n## Try out IMPS!\n\n\n\n- Available on [GitHub](https://github.com/cpmpercussion/imps)\n- [creativeprediction.xyz/imps](https://creativeprediction.xyz/imps)\n- Try with your digital musical instruments!\n- Hack if you want!\n- Add an issue with problems/results!\n- Twitter/Github: [@cpmpercussion](https://www.twitter.com/cpmpercussion)\n- Homepage: [charlesmartin.com.au](https://charlesmartin.com.au)\n\n\n\n",
        "content": "\n\n\n# Creating Intelligent Instruments with Machine Learning\n\nCharles P Martin\n\nANU Research School of Computer Science \n\nweb: [charlesmartin.com.au](https://charlesmartin.com.au)   twitter/github: @cpmpercussion\n\n![](/assets/predictive-models-in-interactive-music/anu-logo.png) \n\n\n\n\n  Welcome\n\n\n\n\n\n  \n  Ngunnawal &amp; Ngambri &amp; Ngarigu Country\n  \n\n\n&lt;section id=”vision”\n         data-background-image=”/assets/predictive-models-in-interactive-music/rohan-performance-2-performance.JPG”\n         data-background=”#000000”\n         data-background-opacity=0.5&gt;\n\nVision:\n\nIntelligent Musical Instruments become a normal\n  part of musical performance and production.\n&lt;/section&gt;\n\n&lt;section id=”why”\n         data-background-image=”/assets/predictive-models-in-interactive-music/2015-11-28-Electrofringe-Workshop-2.jpg”\n         data-background=”#000000”\n         data-beackground-opacity=0.5&gt;\nWhy?\n&lt;p&gt;Assist professional musicians &amp; composers&lt;/p&gt;\n&lt;p&gt;Engage novice musicians &amp; students&lt;/p&gt;\n&lt;p&gt;Create &lt;i&gt;new kinds of music!&lt;/i&gt;&lt;/p&gt;\n\n&lt;/section&gt;\n\n&lt;section id=”what”\n         data-background-image=”/assets/predictive-models-in-interactive-music/musical-performance-predictions.jpg”\n         data-background=”#000000”\n         data-background-opacity=0.6&gt;\nmaking intelligent musical predictions\n&lt;/section&gt;\n\n\n\n## predicting sequences\n\n![](/assets/predictive-models-in-interactive-music/sequence-learning.png)\n\n\n  So this is sequence learning where we train a model to predict the\n  next element of a sequence.\n  RNNs have long history.\n  Can now generate new data.\n\n\n\n\n\n\n## interacting with predictions\n\n![](/assets/predictive-models-in-interactive-music/predictive-interaction-motivation.png)\n\n\n  Idea here is to embed this model into the interaction loop of NIME\n  so that it can predict future control data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n### Performance data is diverse\n\n![](/assets/predictive-models-in-interactive-music/imps-nimes-examples.jpg)\n\n| Music Systems                   | Data                               |\n|---------------------------------|------------------------------------|\n| Score / Notation                | Symbolic Music, Image              |\n| Digital Instruments             | MIDI                               |\n| Recording &amp; Production          | Digital  Audio                     |\n| New Musical Interfaces          | Gestural and Sensor Data           |\n| Show Control                    | Video, Audio, Lighting, Control Signals |\n\n\n\n\n\n\n\n\n## Interactive RNN Instrument\n\n![Physical Intelligent\nInstrument](/assets/predictive-models-in-interactive-music/physical-intelligent-instrument2.png) \n\n- Generates endless music with a melody RNN.\n- Switchable Dataset.\n- Controls for sampling \"temperature\".\n\n  - Is this a performance instrument? Something between instrument and\n  playback device.\n  - Switchable dataset is a nice idea (Bach, Massive MIDI, Final\n  Fantasy 7)\n  - \"Temperature\" is a good user parameter for experimenting in\n  performance, althought users focussed on the synth change knob.\n\n\n\n\n&lt;section id=”rpiinsttitle”\n         data-background-video=”/assets/predictive-models-in-interactive-music/physical-intelligent-instrument.mp4”\n         data-background-video-loop\n         data-background-opacity=0.7\n         data-background=”#000000”&gt;\nPhysical Intelligent Instrument\n&lt;/section&gt;\n\n\n\n\n\n\n\n&lt;section id=”gesture-rnn-title”\n  id=”whatisthis”\n  data-background-video=”/assets/predictive-models-in-interactive-music/neural-ensemble-interaction.mp4”\n  data-background-video-loop\n  data-background-opacity=0.7\n  data-background=”#000000”&gt;\nGestureRNN\n&lt;/section&gt;\n\n\n\n\n\n  ## GestureRNN\n\n  ![Gesture RNN](/assets/predictive-models-in-interactive-music/gesture-rnn.png) \n\n  - Predicts 1 of 9 \"gestures\" for three AI performers.\n  - Trained on labelled data from 5 hours of quartet performances.\n  - Actual \"sounds\" are chunks of each gesture played back.\n\n  \n    Lessons:\n    - fun to play\n    - music needs beginning and end\n    - limited sonic material\n    - lacks a bit of coherence.\n\n\n\n\n&lt;section\n  id=”robojamtitle”\n  data-background-video=”/assets/predictive-models-in-interactive-music/robojam-drum-response.mp4”\n  data-background-video-loop\n  data-background-opacity=0.7\n  data-background=”#000000”&gt;\nRoboJam\n&lt;/section&gt;\n\n\n\n\n\n## Robojam and Microjam\n\n![Robojam Interaction](/assets/predictive-models-in-interactive-music/robojam-interaction.png) \n\n- Predicts next touch location in screen (x, y, dt).\n- Trained on ~1500 5s performances.\n- Produces duet \"responses\" to the user.\n\n\n  - is this a performance instrument\n  - took a long time to get this to work (MDNs are hard)\n  - tuning of the samplig here is important.\n  - a lot relied on the \"reply\" interaction, but this was a bit of a\n  conceit.\n  - still not sure if this is a \"useful\" performance idea, only\n  performed a few times with microjam\n  - try out the app? contribute to my dataset!\n\n\n\n\n\n  Mixture Density Network\n\n  \n\n\n\n\n\n\n\n&lt;section id=”slide”\n  id=”empititle”\n  data-background-video=”/assets/empi/empi-short-demo.mp4”\n  data-background-video-loop\n  data-background-opacity=0.7\n  data-background=”#000000”&gt;\n Embodied Predictive Musical Instrument (EMPI)\n&lt;/section&gt;\n\n\n\n\n\n## Embodied Predictive Musical Instrument (EMPI)\n\n![Physical Intelligent\nInstrument](/assets/empi/EMPI-system-diagram.jpg) \n\n- Predicts next movement and time, represents physically.\n- Experiments with interaction mappings; mainly focussed on call-response\n- Weird and confusing/fun?\n\n  - neural networks can run on a raspberry pi.\n  - physical output really draws audiences in\n  - difficult to work out how to extend interaction patterns here\n  - this instrument seems to lend itself to synthetic or one-track\n  datasets (need way to switch between them)_\n\n\n\n\n\n\n&lt;section\n  id=”imps”\n  data-background-video=”/assets/predictive-models-in-interactive-music/imps-lightpad-loop.mp4”\n  data-background-video-loop\n  data-background-opacity=0.7\n  data-background=”#000000”&gt;\nIMPS system\n&lt;/section&gt;\n\n\n\n\n\n  ## IMPS System\n\n  ![](/assets/predictive-models-in-interactive-music/IMPS_connection_example.png) \n\n  - Opinionated Neural Network for interacting with NIMES.\n  - Automatically collects data and trains.\n  - \"Wekinator\" for deep learning?\n  \n  \n    - very small datasets work! (for something)\n    - training on regular computers works! (up to a point)\n    - automatic data recording leads to a kind of \"practice\" for the\n    neural network\n    - do we need data curation as well?\n    - looking for users!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;section id=”whattodo”\n         data-background-image=”/assets/predictive-models-in-interactive-music/ipad-ensemble-2015.jpg”\n         data-background-opacity=0.5\n         data-background=”#000000”&gt;\nUsing Predictions to Make Music\n\n  Emulate or enhance ensemble experience\n  Engage in call-and-response improvisation\n  Model a performer's personal style\n  Modify/improve performance actions in place\n\n&lt;/section&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;section id=”evaluation”\n         data-background-image=”/assets/predictive-models-in-interactive-music/metatone-20170529-ifi.jpg”\n         data-background-opacity=0.5\n         data-background=”#000000”&gt;\nEvaluating Predictive Instruments?\n\n    Does the ML model make good predictions?\n    Is this computationally practical?\n    Is this useful to musicians?\n  \n&lt;/section&gt;\n\n\n  IMPS works with traditional instruments too!\n\n\n\n\n## Try out IMPS!\n\n\n\n- Available on [GitHub](https://github.com/cpmpercussion/imps)\n- [creativeprediction.xyz/imps](https://creativeprediction.xyz/imps)\n- Try with your digital musical instruments!\n- Hack if you want!\n- Add an issue with problems/results!\n- Twitter/Github: [@cpmpercussion](https://www.twitter.com/cpmpercussion)\n- Homepage: [charlesmartin.com.au](https://charlesmartin.com.au)\n\n\n\n",
        "url": "/presentations/creating-intelligent-instruments/"
      },
    
      {
        "title": "Creative Machine Learning with ml5.js",
        "excerpt": "\n\n\n\n    Creative Prediction with Neural Networks\n    A course in ML/AI for creative expression\n\n\nCreative Machine Learning with ml5.js\n\nCharles Martin - The Australian National University\n\n\n\t\n\n\n\n\n\n  \n  Ngunnawal &amp; Ngambri Country\n  \n\n\nCreative Machine Learning with ml5.js\n\nWhat is machine learning (ML)?\n\nHow can I use ML in p5?\n\nHow can we make art with this?\n\nWhat is Machine Learning Anyway?\n\nCreating computer programs without explicitly programming them.\n\nAlgorithms that learn by example.\n\nAlgorithms that learn through experience.\n\nKind of a big deal ($$$)\n\nKind of problematic (!!!)\n\nLet’s Solve a Problem\n\nSuppose the boss wants a program where the screen colour changes to red when the mouse moves to certain locations.\n\n\n  \n    \n      mouseX\n      red background\n    \n  \n  \n    \n      15\n      no\n    \n    \n      75\n      no\n    \n    \n      173\n      no\n    \n    \n      250\n      yes\n    \n    \n      312\n      yes\n    \n    \n      375\n      yes\n    \n  \n\n\n(N.B.: the screen is 400 pixels wide)\n\nHow would you do it?\n\nLet’s write a configurable algorithm!\n\nif (mouseX &gt; ??) {\n    background(255,0,0);\n} else {\n    background(0,0,0);\n}\n\n\none decision (red or black background)\n\none input (mouseX)\n\nWhat if we had more inputs?\n\nMaybe we could make more complicated decisions?\n\nLikely to get more complicated to configure the algorithm.\n\n\n  \n  Pictures as Inputs\n  \n\n\nPictures are 2D arrays of colours! (represented as numbers)\n\nSo we had enough ifs and elses then maybe we could make a doggo classifier!\n\n(Photo by Ashleigh Robertson on Unsplash)\n\nSimplifying inputs\n\n\n\nOne trick we often use is to design a configurable algorithm which can:\n\n\n  take lots of numbers as inputs\n  boil this all down to just one number as output.\n\n\nThe “configuration” would be “choosing how much of each input to listen to”\n\nOne example is a “perceptron” (1958)\n\nFast forward 50 years.\n\n\n\nTrick 1: feed the outputs of perceptrons into more perceptrons in a kind of network\n\nTrick 2: tricky algorithms to choose the configuration\n\nTrick 3: big fast computers with lots of data to learn from\n\nBy the way, another name for a perceptron is an artificial neuron. So the above is a… neural network…\n\nSome terminology\n\nModel: an instance of a trainable algorithm\n\nPre-Trained Model: a trainable algorithm which has… already been trained.\n\nTraining/Optimising: using training data to make a trained model.\n\nPrediction/Inference: using a trained model to generate an output using  unseen data\n\nClassification: an ML task for choosing a “class” (or description) for a  piece of data\n\nml5.js: Friendly Machine Learning\n\nml5.js is a JavaScript library that provides access to machine learning models in a web browser.\n\nYou can load up pre-trained models and start doing prediction right away!\n\nRelated to and inspired by p5.js.\n\nGet started\n\nJust need to load it in our index.html:\n\n&lt;script src=\"https://unpkg.com/ml5@0.5.0/dist/ml5.min.js\" type=\"text/javascript\"&gt;&lt;/script&gt;\n\n\nOpen up a p5 web editor sketch with ml5 already loaded.\n\nClassifying Images\n\nLet’s classify some doggos. We’ll use a pretrained model called MobileNet\n\n// load the classifier\nclassifier = ml5.imageClassifier('MobileNet');\n// classify an image\nclassifier.classify(img, gotResult)\n\nWhere does the result go? Need to define a callback function gotResult(error, results).\n\nClassifying Video\n\nWe can access a webcam in our sketch:\nvideo = createCapture(VIDEO);\nvideo.size(320, 240);\nvideo.hide();\n\n\nAnd we can just ask the classifier to only make predictions from this video stream:\nclassifier = ml5.imageClassifier('MobileNet', video);\nclassifier.classify(gotResult); // classify one frame\n\n\n\n  \n  What is problematic about doggos?\n  \n\n\n\n  \n  What is MobileNet?\n  \n\n\nPhoto by Alina Grubnyak on Unsplash\n\n\n  \n  \n  \n\n\nWhat was the training data?\n\n\n  \n  Are there hidden costs?\n  \n\n\nArtistinal Bespoke Machine Learning\n\nLet’s make our own custom image classifier with Teachable Machine\n\n\n\n\n  \n\n\n\n\n  \n    \n\n  Allison Parrish\n\n\t\n  \n    \n\n  Compasses\n\n\t\n  \n    \n\n  2019\n\n\t\n  \n\n\n\n\n  \n\n\n\n\n  \n    \n\n  Dadabots\n\n\t\n  \n    \n\n  Relentless Doppelganger\n\n\t\n  \n    \n\n  2019\n\n\t\n  \n\n\n\n\n  \n\n\n\n\n  \n    \n\n  Memo Atken\n\n\t\n  \n    \n\n  Learning to See\n\n\t\n  \n    \n\n  2019\n\n\t\n  \n\n\n\n\n  \n\n\n\n\n  \n    \n\n  Dilpreet Singh\n\n\t\n  \n    \n\n  Art or Not App\n\n\t\n  \n    \n\n  2019\n\n\t\n  \n\n\n\n\n  \n\n\n\n\n  \n    \n\n  Benedikte Wallace\n\n\t\n  \n    \n\n  Dance Generation Neural Network\n\n\t\n  \n    \n\n  2020\n\n\t\n  \n\n\n\n",
        "content": "\n\n\n\n    Creative Prediction with Neural Networks\n    A course in ML/AI for creative expression\n\n\nCreative Machine Learning with ml5.js\n\nCharles Martin - The Australian National University\n\n\n\t\n\n\n\n\n\n  \n  Ngunnawal &amp; Ngambri Country\n  \n\n\nCreative Machine Learning with ml5.js\n\nWhat is machine learning (ML)?\n\nHow can I use ML in p5?\n\nHow can we make art with this?\n\nWhat is Machine Learning Anyway?\n\nCreating computer programs without explicitly programming them.\n\nAlgorithms that learn by example.\n\nAlgorithms that learn through experience.\n\nKind of a big deal ($$$)\n\nKind of problematic (!!!)\n\nLet’s Solve a Problem\n\nSuppose the boss wants a program where the screen colour changes to red when the mouse moves to certain locations.\n\n\n  \n    \n      mouseX\n      red background\n    \n  \n  \n    \n      15\n      no\n    \n    \n      75\n      no\n    \n    \n      173\n      no\n    \n    \n      250\n      yes\n    \n    \n      312\n      yes\n    \n    \n      375\n      yes\n    \n  \n\n\n(N.B.: the screen is 400 pixels wide)\n\nHow would you do it?\n\nLet’s write a configurable algorithm!\n\nif (mouseX &gt; ??) {\n    background(255,0,0);\n} else {\n    background(0,0,0);\n}\n\n\none decision (red or black background)\n\none input (mouseX)\n\nWhat if we had more inputs?\n\nMaybe we could make more complicated decisions?\n\nLikely to get more complicated to configure the algorithm.\n\n\n  \n  Pictures as Inputs\n  \n\n\nPictures are 2D arrays of colours! (represented as numbers)\n\nSo we had enough ifs and elses then maybe we could make a doggo classifier!\n\n(Photo by Ashleigh Robertson on Unsplash)\n\nSimplifying inputs\n\n\n\nOne trick we often use is to design a configurable algorithm which can:\n\n\n  take lots of numbers as inputs\n  boil this all down to just one number as output.\n\n\nThe “configuration” would be “choosing how much of each input to listen to”\n\nOne example is a “perceptron” (1958)\n\nFast forward 50 years.\n\n\n\nTrick 1: feed the outputs of perceptrons into more perceptrons in a kind of network\n\nTrick 2: tricky algorithms to choose the configuration\n\nTrick 3: big fast computers with lots of data to learn from\n\nBy the way, another name for a perceptron is an artificial neuron. So the above is a… neural network…\n\nSome terminology\n\nModel: an instance of a trainable algorithm\n\nPre-Trained Model: a trainable algorithm which has… already been trained.\n\nTraining/Optimising: using training data to make a trained model.\n\nPrediction/Inference: using a trained model to generate an output using  unseen data\n\nClassification: an ML task for choosing a “class” (or description) for a  piece of data\n\nml5.js: Friendly Machine Learning\n\nml5.js is a JavaScript library that provides access to machine learning models in a web browser.\n\nYou can load up pre-trained models and start doing prediction right away!\n\nRelated to and inspired by p5.js.\n\nGet started\n\nJust need to load it in our index.html:\n\n&lt;script src=\"https://unpkg.com/ml5@0.5.0/dist/ml5.min.js\" type=\"text/javascript\"&gt;&lt;/script&gt;\n\n\nOpen up a p5 web editor sketch with ml5 already loaded.\n\nClassifying Images\n\nLet’s classify some doggos. We’ll use a pretrained model called MobileNet\n\n// load the classifier\nclassifier = ml5.imageClassifier('MobileNet');\n// classify an image\nclassifier.classify(img, gotResult)\n\nWhere does the result go? Need to define a callback function gotResult(error, results).\n\nClassifying Video\n\nWe can access a webcam in our sketch:\nvideo = createCapture(VIDEO);\nvideo.size(320, 240);\nvideo.hide();\n\n\nAnd we can just ask the classifier to only make predictions from this video stream:\nclassifier = ml5.imageClassifier('MobileNet', video);\nclassifier.classify(gotResult); // classify one frame\n\n\n\n  \n  What is problematic about doggos?\n  \n\n\n\n  \n  What is MobileNet?\n  \n\n\nPhoto by Alina Grubnyak on Unsplash\n\n\n  \n  \n  \n\n\nWhat was the training data?\n\n\n  \n  Are there hidden costs?\n  \n\n\nArtistinal Bespoke Machine Learning\n\nLet’s make our own custom image classifier with Teachable Machine\n\n\n\n\n  \n\n\n\n\n  \n    \n\n  Allison Parrish\n\n\t\n  \n    \n\n  Compasses\n\n\t\n  \n    \n\n  2019\n\n\t\n  \n\n\n\n\n  \n\n\n\n\n  \n    \n\n  Dadabots\n\n\t\n  \n    \n\n  Relentless Doppelganger\n\n\t\n  \n    \n\n  2019\n\n\t\n  \n\n\n\n\n  \n\n\n\n\n  \n    \n\n  Memo Atken\n\n\t\n  \n    \n\n  Learning to See\n\n\t\n  \n    \n\n  2019\n\n\t\n  \n\n\n\n\n  \n\n\n\n\n  \n    \n\n  Dilpreet Singh\n\n\t\n  \n    \n\n  Art or Not App\n\n\t\n  \n    \n\n  2019\n\n\t\n  \n\n\n\n\n  \n\n\n\n\n  \n    \n\n  Benedikte Wallace\n\n\t\n  \n    \n\n  Dance Generation Neural Network\n\n\t\n  \n    \n\n  2020\n\n\t\n  \n\n\n\n",
        "url": "/presentations/creml5js/"
      },
    
      {
        "title": "The future of predictive NIMEs?",
        "excerpt": "\n\n\n\n    Creative Prediction with Neural Networks\n    A course in ML/AI for creative expression\n\n\nThe future of predictive NIMEs?\n\nCharles Martin - The Australian National University\n\n\n\t\n\n\n\n\n\n\n\n\n- What even is a predictive NIME?\n- What NIMEs could benefit from prediction?\n- What tools are needed?\n- Who can help?\n- What resources are necessary?\n- What's going on with this \"Creative AI\" field anyway?\n\n\n\n\n\n\n\n## Design a Predictive NIME\n\n- Design a new NIME that relies on prediction \n- Or extend and older NIME with predictive models (studied today)\n\n\n\n\n",
        "content": "\n\n\n\n    Creative Prediction with Neural Networks\n    A course in ML/AI for creative expression\n\n\nThe future of predictive NIMEs?\n\nCharles Martin - The Australian National University\n\n\n\t\n\n\n\n\n\n\n\n\n- What even is a predictive NIME?\n- What NIMEs could benefit from prediction?\n- What tools are needed?\n- Who can help?\n- What resources are necessary?\n- What's going on with this \"Creative AI\" field anyway?\n\n\n\n\n\n\n\n## Design a Predictive NIME\n\n- Design a new NIME that relies on prediction \n- Or extend and older NIME with predictive models (studied today)\n\n\n\n\n",
        "url": "/presentations/future/"
      },
    
      {
        "title": "Presentations",
        "excerpt": "Here’s some presentations about neural networks and creative prediction:\n\nTeaching\n\n\n  \n    Making Predictive NIMEs with Neural Networks\n  \n  \n    Deep Dive on RNNs\n  \n  \n    Mixture Density Networks\n  \n  \n    Deep Learning in the Cloud\n  \n  \n    Creative Prediction Practicalities\n  \n  \n    How to be a Project Student in the Intelligent Music Lab\n  \n  \n    References, Citations, Publishing, and Authorship\n  \n  \n    Creative Machine Learning with ml5.js\n  \n  \n    Creative Prediction Projects\n  \n\n\nResearch\n\n\n  \n    An Interactive Musical Prediction System with MDRNNs\n  \n  \n    Recent Progress in Intelligent Musical Instruments\n  \n  \n    Embedding Embodied Music Generation\n  \n  \n    Generative AI and Music\n  \n  \n    Performing with a Generative Electronic Music Controller\n  \n  \n    Creating Intelligent Musical Instruments\n  \n\n\n",
        "content": "Here’s some presentations about neural networks and creative prediction:\n\nTeaching\n\n\n  \n    Making Predictive NIMEs with Neural Networks\n  \n  \n    Deep Dive on RNNs\n  \n  \n    Mixture Density Networks\n  \n  \n    Deep Learning in the Cloud\n  \n  \n    Creative Prediction Practicalities\n  \n  \n    How to be a Project Student in the Intelligent Music Lab\n  \n  \n    References, Citations, Publishing, and Authorship\n  \n  \n    Creative Machine Learning with ml5.js\n  \n  \n    Creative Prediction Projects\n  \n\n\nResearch\n\n\n  \n    An Interactive Musical Prediction System with MDRNNs\n  \n  \n    Recent Progress in Intelligent Musical Instruments\n  \n  \n    Embedding Embodied Music Generation\n  \n  \n    Generative AI and Music\n  \n  \n    Performing with a Generative Electronic Music Controller\n  \n  \n    Creating Intelligent Musical Instruments\n  \n\n\n",
        "url": "/presentations/"
      },
    
      {
        "title": "Predictive Models in Interactive Music",
        "excerpt": "\n\n\n\n    Creative Prediction with Neural Networks\n    A course in ML/AI for creative expression\n\n\nPredictive Models in Interactive Music\n\nCharles Martin - The Australian National University\n\n\n\t\n\n\n\n\n\n  \n  Ngunnawal &amp; Ngambri &amp; Ngarigu Country\n  \n\n\n&lt;section id=”vision”\n         data-background-image=”/assets/predictive-models-in-interactive-music/rohan-performance-2-performance.JPG”\n         data-background=”#000000”\n         data-background-opacity=0.5&gt;\n\nVision:\n\nPredictive Musical Instruments become a normal\n  part of musical performance and production.\n&lt;/section&gt;\n\n&lt;section id=”why”\n         data-background-image=”/assets/predictive-models-in-interactive-music/2015-11-28-Electrofringe-Workshop-2.jpg”\n         data-background=”#000000”\n         data-beackground-opacity=0.5&gt;\nWhy?\n&lt;p&gt;Assist professional musicians &amp; composers&lt;/p&gt;\n&lt;p&gt;Engage novice musicians &amp; students&lt;/p&gt;\n&lt;p&gt;Create &lt;i&gt;new kinds of music!&lt;/i&gt;&lt;/p&gt;\n\n&lt;/section&gt;\n\n&lt;section id=”what”\n         data-background-image=”/assets/predictive-models-in-interactive-music/musical-performance-predictions.jpg”\n         data-background=”#000000”\n         data-background-opacity=0.6&gt;\nMusical Predictions?\n&lt;/section&gt;\n\n\n\n## predicting sequences\n\n![](/assets/predictive-models-in-interactive-music/sequence-learning.png)\n\n\n  So this is sequence learning where we train a model to predict the\n  next element of a sequence.\n  RNNs have long history.\n  Can now generate new data.\n\n\n\n\n\n\n## interacting with predictions\n\n![](/assets/predictive-models-in-interactive-music/predictive-interaction-motivation.png)\n\n\n  Idea here is to embed this model into the interaction loop of NIME\n  so that it can predict future control data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n### Performance data is diverse\n\n![](/assets/predictive-models-in-interactive-music/imps-nimes-examples.jpg)\n\n| Music Systems                   | Data                               |\n|---------------------------------|------------------------------------|\n| Score / Notation                | Symbolic Music, Image              |\n| Digital Instruments             | MIDI                               |\n| Recording &amp; Production          | Digital  Audio                     |\n| New Musical Interfaces          | Gestural and Sensor Data           |\n| Show Control                    | Video, Audio, Lighting, Control Signals |\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;section id=”gesture-rnn-title”\n  id=”whatisthis”\n  data-background-video=”/assets/predictive-models-in-interactive-music/neural-ensemble-interaction.mp4”\n  data-background-video-loop\n  data-background-opacity=0.7\n  data-background=”#000000”&gt;\nGestureRNN\n&lt;/section&gt;\n\n\n\n\n\n  ## GestureRNN\n\n  ![Gesture RNN](/assets/predictive-models-in-interactive-music/gesture-rnn.png) \n\n  - Predicts 1 of 9 \"gestures\" for three AI performers.\n  - Trained on labelled data from 5 hours of quartet performances.\n  - Actual \"sounds\" are chunks of each gesture played back.\n\n  \n    Lessons:\n    - fun to play\n    - music needs beginning and end\n    - limited sonic material\n    - lacks a bit of coherence.\n\n\n\n\n\n\n&lt;section\n  id=”robojamtitle”\n  data-background-video=”/assets/predictive-models-in-interactive-music/robojam-drum-response.mp4”\n  data-background-video-loop\n  data-background-opacity=0.7\n  data-background=”#000000”&gt;\nRoboJam\n&lt;/section&gt;\n\n\n\n\n\n## Robojam and Microjam\n\n![Robojam Interaction](/assets/predictive-models-in-interactive-music/robojam-interaction.png) \n\n- Predicts next touch location in screen (x, y, dt).\n- Trained on ~1500 5s performances.\n- Configured to produce duet \"responses\" to performances in the app.\n\n\n  - is this a performance instrument\n  - took a long time to get this to work (MDNs are hard)\n  - tuning of the samplig here is important.\n  - a lot relied on the \"reply\" interaction, but this was a bit of a\n  conceit.\n  - still not sure if this is a \"useful\" performance idea, only\n  performed a few times with microjam\n  - try out the app? contribute to my dataset!\n\n\n\n\n\n\n&lt;section id=”rpiinsttitle”\n         data-background-video=”/assets/predictive-models-in-interactive-music/physical-intelligent-instrument.mp4”\n         data-background-video-loop\n         data-background-opacity=0.7\n         data-background=”#000000”&gt;\nPhysical Intelligent Instrument\n&lt;/section&gt;\n\n\n\n## Interactive RNN Instrument\n\n![Physical Intelligent\nInstrument](/assets/predictive-models-in-interactive-music/physical-intelligent-instrument2.png) \n\n- Generates endless music with a melody RNN.\n- Switchable Dataset.\n- Controls for sampling \"temperature\".\n\n  - Is this a performance instrument? Something between instrument and\n  playback device.\n  - Switchable dataset is a nice idea (Bach, Massive MIDI, Final\n  Fantasy 7)\n  - \"Temperature\" is a good user parameter for experimenting in\n  performance, althought users focussed on the synth change knob.\n\n\n\n\n\n\n&lt;section id=”slide”\n  id=”empititle”\n  data-background-video=”/assets/empi/empi-short-demo.mp4”\n  data-background-video-loop\n  data-background-opacity=0.7\n  data-background=”#000000”&gt;\n Embodied Predictive Musical Instrument (EMPI)\n&lt;/section&gt;\n\n\n\n\n\n## Embodied Predictive Musical Instrument (EMPI)\n\n![Physical Intelligent\nInstrument](/assets/empi/EMPI-system-diagram.jpg) \n\n- Predicts next movement and time, represents physically.\n- Experiments with interaction mappings; mainly focussed on call-response\n- Weird and confusing/fun?\n\n  - neural networks can run on a raspberry pi.\n  - physical output really draws audiences in\n  - difficult to work out how to extend interaction patterns here\n  - this instrument seems to lend itself to synthetic or one-track\n  datasets (need way to switch between them)_\n\n\n\n\n&lt;section\n  id=”imps”\n  data-background-video=”/assets/predictive-models-in-interactive-music/imps-lightpad-loop.mp4”\n  data-background-video-loop\n  data-background-opacity=0.7\n  data-background=”#000000”&gt;\nIMPS system\n&lt;/section&gt;\n\n\n\n  ## IMPS System\n\n  ![](/assets/predictive-models-in-interactive-music/IMPS_connection_example.png) \n\n  - Opinionated Neural Network for interacting with NIMES.\n  - Automatically collects data and trains.\n  - \"Wekinator\" for deep learning?\n  \n  \n    - very small datasets work! (for something)\n    - training on regular computers works! (up to a point)\n    - automatic data recording leads to a kind of \"practice\" for the\n    neural network\n    - do we need data curation as well?\n    - looking for users!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;section id=”whattodo”\n         data-background-image=”/assets/predictive-models-in-interactive-music/ipad-ensemble-2015.jpg”\n         data-background-opacity=0.5\n         data-background=”#000000”&gt;\nUsing Predictions to Make Music\n\n  Emulate or enhance ensemble experience\n  Engage in call-and-response improvisation\n  Model a performer's personal style\n  Modify/improve performance actions in place\n\n&lt;/section&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;section id=”evaluation”\n         data-background-image=”/assets/predictive-models-in-interactive-music/metatone-20170529-ifi.jpg”\n         data-background-opacity=0.5\n         data-background=”#000000”&gt;\nEvaluating Predictive Instruments?\n\n    Does the ML model make good predictions?\n    Is this computationally practical?\n    Is this useful to musicians?\n  \n&lt;/section&gt;\n\n\n  IMPS works with traditional instruments too!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
        "content": "\n\n\n\n    Creative Prediction with Neural Networks\n    A course in ML/AI for creative expression\n\n\nPredictive Models in Interactive Music\n\nCharles Martin - The Australian National University\n\n\n\t\n\n\n\n\n\n  \n  Ngunnawal &amp; Ngambri &amp; Ngarigu Country\n  \n\n\n&lt;section id=”vision”\n         data-background-image=”/assets/predictive-models-in-interactive-music/rohan-performance-2-performance.JPG”\n         data-background=”#000000”\n         data-background-opacity=0.5&gt;\n\nVision:\n\nPredictive Musical Instruments become a normal\n  part of musical performance and production.\n&lt;/section&gt;\n\n&lt;section id=”why”\n         data-background-image=”/assets/predictive-models-in-interactive-music/2015-11-28-Electrofringe-Workshop-2.jpg”\n         data-background=”#000000”\n         data-beackground-opacity=0.5&gt;\nWhy?\n&lt;p&gt;Assist professional musicians &amp; composers&lt;/p&gt;\n&lt;p&gt;Engage novice musicians &amp; students&lt;/p&gt;\n&lt;p&gt;Create &lt;i&gt;new kinds of music!&lt;/i&gt;&lt;/p&gt;\n\n&lt;/section&gt;\n\n&lt;section id=”what”\n         data-background-image=”/assets/predictive-models-in-interactive-music/musical-performance-predictions.jpg”\n         data-background=”#000000”\n         data-background-opacity=0.6&gt;\nMusical Predictions?\n&lt;/section&gt;\n\n\n\n## predicting sequences\n\n![](/assets/predictive-models-in-interactive-music/sequence-learning.png)\n\n\n  So this is sequence learning where we train a model to predict the\n  next element of a sequence.\n  RNNs have long history.\n  Can now generate new data.\n\n\n\n\n\n\n## interacting with predictions\n\n![](/assets/predictive-models-in-interactive-music/predictive-interaction-motivation.png)\n\n\n  Idea here is to embed this model into the interaction loop of NIME\n  so that it can predict future control data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n### Performance data is diverse\n\n![](/assets/predictive-models-in-interactive-music/imps-nimes-examples.jpg)\n\n| Music Systems                   | Data                               |\n|---------------------------------|------------------------------------|\n| Score / Notation                | Symbolic Music, Image              |\n| Digital Instruments             | MIDI                               |\n| Recording &amp; Production          | Digital  Audio                     |\n| New Musical Interfaces          | Gestural and Sensor Data           |\n| Show Control                    | Video, Audio, Lighting, Control Signals |\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;section id=”gesture-rnn-title”\n  id=”whatisthis”\n  data-background-video=”/assets/predictive-models-in-interactive-music/neural-ensemble-interaction.mp4”\n  data-background-video-loop\n  data-background-opacity=0.7\n  data-background=”#000000”&gt;\nGestureRNN\n&lt;/section&gt;\n\n\n\n\n\n  ## GestureRNN\n\n  ![Gesture RNN](/assets/predictive-models-in-interactive-music/gesture-rnn.png) \n\n  - Predicts 1 of 9 \"gestures\" for three AI performers.\n  - Trained on labelled data from 5 hours of quartet performances.\n  - Actual \"sounds\" are chunks of each gesture played back.\n\n  \n    Lessons:\n    - fun to play\n    - music needs beginning and end\n    - limited sonic material\n    - lacks a bit of coherence.\n\n\n\n\n\n\n&lt;section\n  id=”robojamtitle”\n  data-background-video=”/assets/predictive-models-in-interactive-music/robojam-drum-response.mp4”\n  data-background-video-loop\n  data-background-opacity=0.7\n  data-background=”#000000”&gt;\nRoboJam\n&lt;/section&gt;\n\n\n\n\n\n## Robojam and Microjam\n\n![Robojam Interaction](/assets/predictive-models-in-interactive-music/robojam-interaction.png) \n\n- Predicts next touch location in screen (x, y, dt).\n- Trained on ~1500 5s performances.\n- Configured to produce duet \"responses\" to performances in the app.\n\n\n  - is this a performance instrument\n  - took a long time to get this to work (MDNs are hard)\n  - tuning of the samplig here is important.\n  - a lot relied on the \"reply\" interaction, but this was a bit of a\n  conceit.\n  - still not sure if this is a \"useful\" performance idea, only\n  performed a few times with microjam\n  - try out the app? contribute to my dataset!\n\n\n\n\n\n\n&lt;section id=”rpiinsttitle”\n         data-background-video=”/assets/predictive-models-in-interactive-music/physical-intelligent-instrument.mp4”\n         data-background-video-loop\n         data-background-opacity=0.7\n         data-background=”#000000”&gt;\nPhysical Intelligent Instrument\n&lt;/section&gt;\n\n\n\n## Interactive RNN Instrument\n\n![Physical Intelligent\nInstrument](/assets/predictive-models-in-interactive-music/physical-intelligent-instrument2.png) \n\n- Generates endless music with a melody RNN.\n- Switchable Dataset.\n- Controls for sampling \"temperature\".\n\n  - Is this a performance instrument? Something between instrument and\n  playback device.\n  - Switchable dataset is a nice idea (Bach, Massive MIDI, Final\n  Fantasy 7)\n  - \"Temperature\" is a good user parameter for experimenting in\n  performance, althought users focussed on the synth change knob.\n\n\n\n\n\n\n&lt;section id=”slide”\n  id=”empititle”\n  data-background-video=”/assets/empi/empi-short-demo.mp4”\n  data-background-video-loop\n  data-background-opacity=0.7\n  data-background=”#000000”&gt;\n Embodied Predictive Musical Instrument (EMPI)\n&lt;/section&gt;\n\n\n\n\n\n## Embodied Predictive Musical Instrument (EMPI)\n\n![Physical Intelligent\nInstrument](/assets/empi/EMPI-system-diagram.jpg) \n\n- Predicts next movement and time, represents physically.\n- Experiments with interaction mappings; mainly focussed on call-response\n- Weird and confusing/fun?\n\n  - neural networks can run on a raspberry pi.\n  - physical output really draws audiences in\n  - difficult to work out how to extend interaction patterns here\n  - this instrument seems to lend itself to synthetic or one-track\n  datasets (need way to switch between them)_\n\n\n\n\n&lt;section\n  id=”imps”\n  data-background-video=”/assets/predictive-models-in-interactive-music/imps-lightpad-loop.mp4”\n  data-background-video-loop\n  data-background-opacity=0.7\n  data-background=”#000000”&gt;\nIMPS system\n&lt;/section&gt;\n\n\n\n  ## IMPS System\n\n  ![](/assets/predictive-models-in-interactive-music/IMPS_connection_example.png) \n\n  - Opinionated Neural Network for interacting with NIMES.\n  - Automatically collects data and trains.\n  - \"Wekinator\" for deep learning?\n  \n  \n    - very small datasets work! (for something)\n    - training on regular computers works! (up to a point)\n    - automatic data recording leads to a kind of \"practice\" for the\n    neural network\n    - do we need data curation as well?\n    - looking for users!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;section id=”whattodo”\n         data-background-image=”/assets/predictive-models-in-interactive-music/ipad-ensemble-2015.jpg”\n         data-background-opacity=0.5\n         data-background=”#000000”&gt;\nUsing Predictions to Make Music\n\n  Emulate or enhance ensemble experience\n  Engage in call-and-response improvisation\n  Model a performer's personal style\n  Modify/improve performance actions in place\n\n&lt;/section&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;section id=”evaluation”\n         data-background-image=”/assets/predictive-models-in-interactive-music/metatone-20170529-ifi.jpg”\n         data-background-opacity=0.5\n         data-background=”#000000”&gt;\nEvaluating Predictive Instruments?\n\n    Does the ML model make good predictions?\n    Is this computationally practical?\n    Is this useful to musicians?\n  \n&lt;/section&gt;\n\n\n  IMPS works with traditional instruments too!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
        "url": "/presentations/predictive-models-in-interactive-music/"
      },
    
      {
        "title": "Creative Prediction Projects",
        "excerpt": "\n\n\n\n    Creative Prediction with Neural Networks\n    A course in ML/AI for creative expression\n\n\nCreative Prediction Projects\n\nCharles Martin - The Australian National University\n\n\n\t\n\n\n\n\nSupervisors\n\nCharles Martin. Lecturer in Computer Science, Australian\nNational University. charles.martin@anu.edu.au\n\nBenedikte Wallace. PhD Researcher, University of Oslo.\nbenediwa@ifi.uio.no\n\n\n\n\n\nCreative Predictions\n\n\n\nLearning to Predict Sequences\n\n\n\nMelody to Harmony in MicroJam\n\n\n\n\n  Gain an overview of DL for music generation.\n  Develop a melody to harmony sequence to sequence model\n  Train the model on matched melody/harmony sequences\n  Use MicroJam-sourced data as input and see if the generated harmonies make sense!\n\n\nSeq-to-Seq Music Generation\n\n\n\n\n  Understand the Transformer architecture.\n  Implement your own Transformer (e.g., in Keras).\n  Find a musical dataset that could be trained.\n  Train your model, listen to the results and find a way to evaluate them.\n\n\nGenerating colour palettes from audio data\n\n\n\n\n  Gain an overview of DL for audio processing.\n  Obtain a dataset of audio and video (or colour) data.\n  Try different neural network designs and evaluate the results. (Even a simple fully-connected ANN might work well!)\n\n\nMotion-to-Motion Generators\n\n\n\n\n  Gain an overview of the main DL methods used for motion generation including RNNs, MDRNNs, and world models.\n  Find a dataset of motion capture or other movement data (or capture one yourself!)\n  Train the ANN and evaluate its generative abilities.\n\n\n",
        "content": "\n\n\n\n    Creative Prediction with Neural Networks\n    A course in ML/AI for creative expression\n\n\nCreative Prediction Projects\n\nCharles Martin - The Australian National University\n\n\n\t\n\n\n\n\nSupervisors\n\nCharles Martin. Lecturer in Computer Science, Australian\nNational University. charles.martin@anu.edu.au\n\nBenedikte Wallace. PhD Researcher, University of Oslo.\nbenediwa@ifi.uio.no\n\n\n\n\n\nCreative Predictions\n\n\n\nLearning to Predict Sequences\n\n\n\nMelody to Harmony in MicroJam\n\n\n\n\n  Gain an overview of DL for music generation.\n  Develop a melody to harmony sequence to sequence model\n  Train the model on matched melody/harmony sequences\n  Use MicroJam-sourced data as input and see if the generated harmonies make sense!\n\n\nSeq-to-Seq Music Generation\n\n\n\n\n  Understand the Transformer architecture.\n  Implement your own Transformer (e.g., in Keras).\n  Find a musical dataset that could be trained.\n  Train your model, listen to the results and find a way to evaluate them.\n\n\nGenerating colour palettes from audio data\n\n\n\n\n  Gain an overview of DL for audio processing.\n  Obtain a dataset of audio and video (or colour) data.\n  Try different neural network designs and evaluate the results. (Even a simple fully-connected ANN might work well!)\n\n\nMotion-to-Motion Generators\n\n\n\n\n  Gain an overview of the main DL methods used for motion generation including RNNs, MDRNNs, and world models.\n  Find a dataset of motion capture or other movement data (or capture one yourself!)\n  Train the ANN and evaluate its generative abilities.\n\n\n",
        "url": "/presentations/creprepro/"
      },
    
  
  
  
  {
    "title": "IMPS The Interactive Musical Prediction System",
    "excerpt": "\n",
    "content": "IMPS is a system for predicting musical control data in live performance. It uses a mixture density recurrent neural network (MDRNN) to observe control inputs over multiple time steps, predicting the next value of each step, and the time that expects the next value to occur. It provides an input and output interface over OSC and can work with musical interfaces with any number of real valued inputs (we’ve tried from 1-8). Several interactive paradigms are supported for call-response improvisation, as well as independent operation, and “filtering” of the performer’s input. Whenever you use IMPS, your input data is logged to build up a training corpus and a script is provided to train new versions of your model.\n\n\n\n\n\n\nHere’s a demonstration video showing how IMPS can be used with different musical interfaces.\n\n\n\nInstallation\n\nIMPS is written in Python with Keras and TensorFlow Probability, so it should work on any platform where Tensorflow can be installed. Python 3 is required.\n\nFirst you should clone this repository or download it to your computer:\n\ngit clone https://github.com/cpmpercussion/imps.git\ncd imps\n\n\nThe python requirements can be installed as follows:\n\npip install -r requirements.txt\n\n\n\n\n\n\nSome people like to keep Python packages separate in virtual environments, if that’s you, here’s some terminal commands to install:\n\nvirtualenv --system-site-packages -p python3 venv\nsource venv/bin/activate\npip install -r requirements.txt\n\n\nHow to use\n\nThere are four steps for using IMPS. First, you’ll need to setup your musical interface to send it OSC data and receive predictions the same way. Then you can log data, train the MDRNN, and make predictions using our provided scripts.\n\n1. Connect music interface and synthesis software\n\nYou’ll need:\n\n\n  A music interface that can output data as OSC.\n  Some synthesiser software that can take OSC as input.\n\n\nThese could be the same piece of software or hardware!\n\nYou need to decide on the number of inputs (or dimension) for your predictive model. This is the number of continuous outputs from your interface plus one (for time). So for an interface with 8 faders, the dimension will be 9.\n\nNow you need your music interface to send messages to IMPS over OSC. The default address for IMPS is: localhost:5001. The messages to IMPS should have the OSC address /interface, and then a float between 0 and 1 for each continuous output on your interface, e.g.:\n\n/interface 0 0.5 0.23 0.87 0.9 0.7 0.45 0.654\n\n\nFor an 8-dimensional interface.\n\nYour synthesiser software or interface needs to listen for messages from the IMPS system as well. These have the same format with the OSC address /prediction. You can interpret these as interactions predicted to occur right when the message is sent.\n\nHere’s an example diagram for our 8-controller example, the xtouch mini controller.\n\n\n\nIn this example we’ve used Pd to connect the xtouch mini to IMPS and to synthesis sounds. Our Pd mapping patch takes data from the xtouch and sends /interface OSC messages to IMPS, it also receives /prediction OSC message back from IMPS whenever they occur. Of course, whenever the user performs with the controller, the mapping patch sends commands to the synthesiser patch to make sound. Whenever /prediction messages are received, these also trigger changes in the synth patch, and we also send MIDI messages back to the xtouch controller to update its lights so that the performer knows what IMPS is predicting.\n\nSo what happens if IMPS and the performer play at the same time? In this example, it doesn’t make sense for both to control the synthesiser at the same time, so we set IMPS to run in “call and response” mode, so that it only makes predictions when the human has stopped performing. We could also set up our mapping patch to use prediction messages for a different synth and use one of the simultaneous performance modes of IMPS.\n\n2. Log some training data\n\nYou use the predictive_music_model command to log training data. If your interface has N inputs the dimension is N+1:\n\npython predictive_music_model.py --dimension=(N+1) --log\n\n\nThis command creates files in the logs directory with data like this:\n\n2019-01-17T12:37:38.109979,interface,0.3359375,0.296875,0.5078125\n2019-01-17T12:37:38.137938,interface,0.359375,0.296875,0.53125\n2019-01-17T12:37:38.160842,interface,0.375,0.3046875,0.1953125\n\n\nThese CSV files have the format: timestamp, source of message (interface or rnn), x_1, x_2, …,  x_N.\n\nYou can log training data without using the RNN with the o switch (user only) if you like, or use a partially trained RNN and then collect more data.\n\npython predictive_music_model.py --dimension=(N+1) --log -o\n\n\nEvery time you run the predictive_music_model, a new log file is created so that you can build up a significant dataset!\n\n3. Train an MDRNN\n\nThere’s two steps for training: Generate a dataset file, and train the predictive model.\n\nUse the generate_dataset command:\n\npython generate_dataset --dimension=(N+1)\n\n\nThis command collates all logs of dimension N+1 from the logs directory and saves the data in a compressed .npz file in the datasets directory. It will also print out some information about your dataset, in particular the total number of individual interactions. To have a useful dataset, it’s good to start with more than 10,000 individual interactions but YMMV.\n\nTo train the model, use the train_predictive_music_model command—this can take a while on a normal computer, so be prepared to let your computer sit and think for a few hours! You’ll have to decide what size model to try to train: xs, s, m, l, xl. The size refers to the number of LSTM units in each layer of your model and roughly corresponds to “learning capacity” at a cost of slower training and predictions.\nIt’s a good idea to start with an xs or s model, and the larger models are more relevant for quite large datasets (e.g., &gt;1M individual interactions).\n\npython train_predictive_music_model.py --dimension=(N+1) --modelsize=xs --earlystopping\n\n\nIt’s a good idea to use the “earlystopping” parameter to stop training after the model stops improving for 10 epochs.\n\n4. Perform with your predictive model\n\nNow that you have a trained model, you can run this command to start making predictions:\n\npython predictive_music_model.py -d=(N+1) --modelsize=xs --log -c\n\n\nTHe --log switch logs all of your interactions as well as predictions for later re-training. (The dataset generator filters out RNN records so that you only train on human sourced data).\n\nPS: the three scripts respond to the --help switch to show command line options. If there’s something not documented or working, it would be great if you add an issue above to let me know or get in touch on Twitter at @cpmpercussion.\n\nMore about Mixture Density Recurrent Neural Networks\n\nIMPS uses a mixture density recurrent neural network MDRNN to make predictions. This machine learning architecture is set up to predict the next in a sequence of multi-valued elements. The recurrent neural network uses LSTM units to remember information about past inputs and use this to help make decisions. The mixture density model at the end of the network allows continuous multi-valued elements to be sampled from a rich probability distribution.\n\nThe network is illustrated here—every time IMPS receives an interaction message from your interface, it is sent to thorugh the LSTM layers to produce the parameters of a Gaussian mixture model. The predicted next interaction is sampled from this probability model.\n\n\n\nThe MDRNN is written in Keras and uses the keras-mdn-layer package. There’s more info and tutorials about MDNs on that github repo.\n",
    "url": "/imps/"
  },
  
  {
    "title": "Creative Prediction",
    "excerpt": "This is the website for the Creative Prediction project – a collection of walkthroughs and examples for applying machine learning to creative data in interactive creativity systems such as interactive artworks and musical instruments.\n",
    "content": "This is the website for the Creative Prediction project – a collection of walkthroughs and examples by Charles Martin and others for applying machine learning to creative data in interactive creativity systems such as interactive artworks and musical instruments.\n\nThe main content is a list of  tutorials and exercises that work on your computer or in Google Colab, and some presentations and lectures about using machine learning with creative data. There’s also page about setting up your computer with Python and the right libraries if you want to run the code examples.\n\nIf you want to (or already) use machine learning in a creative artistic setting; this is the place for you!\n\n\nvia GIPHY\n\nAbout CrePre\n\nCreative Prediction is about applying predictive machine learning models to creative data. The focus is on recurrent neural networks (RNNs), deep learning models that can be used to generate sequential and temporal data. RNNs can be applied to many kinds of creative data including text and music. They can learn the long-range structure from a corpus of data and “create” new sequences by predicting one element at a time. When embedded in a creative interface, they can be used for “predictive interaction” where a human collaborates with, influences, and is influenced by a generative neural network.\n\nThis site includes walkthroughs of the fundamental steps for training creative RNNs using live-coded demonstrations with Python code in Jupyter Notebooks.\n\nWorkshops\n\nCreative Prediction workshops have been presented in Oslo, Tokyo, and Porto Alegre. The workshops involve lectures in machine learning for creative arts, and collaborative hacking sessions to start creating artistic prediction projects!\n\n\n\n\n\n",
    "url": "/"
  },
  
  {
    "title": "Mixture Density Networks",
    "excerpt": "\n",
    "content": "\n\nSo far; RNNs that Model Categorical Data\n\n::: columns\n\n:::: column\n\n  Remember that most RNNs (and most deep learning models) end with a softmax layer.\n  This layer outputs a probability distribution for a set of categorical predictions.\n  E.g.:\n    \n      image labels,\n      letters, words,\n      musical notes,\n      robot commands,\n      moves in chess.\n::::\n    \n  \n\n\n::::column\n{ width=95% }\n{ width=80% }\n::::\n\n:::\n\n\n\nExpressive Data is Often Continuous\n\n::: columns\n\n:::: column\n{ width=60% }\n{ width=60% }\n::::\n\n::::column\n{ width=60% }\n\n::::\n\n:::\n\n\n\nSo are Bio-Signals\n\n::: columns\n\n:::: column\n{ width=60% }\n::::\n\n::::column\n{ width=60% }\n{ width=60% }\n::::\n\n:::\n\nImage Credit: Wikimedia\n\n\n\nCategorical vs. Continuous Models\n\n::: columns\n\n:::: column\n\n::::\n\n::::column\n\n::::\n\n:::\n\n\n\nNormal (Gaussian) Distribution\n\n::: columns\n\n:::: column\n\n  “Standard” probability distribution\n  Has two parameters:\n    \n      mean ($\\mu$) and\n      standard deviation ($\\sigma$)\n    \n  \n  Probability Density Function:\n    \n      \n\\[\\mathcal{N}(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2} } e^{ -\\frac{(x-\\mu)^2}{2\\sigma^2} }\\]\n      \n    \n  \n\n\n::::\n\n::::column\n{ width=100% }\n\n::::\n\n:::\n\n\n\nProblem: Normal distribution might not fit data\n\n::: columns\n\n:::: column\nWhat if the data is complicated?\n\n\n  It’s easy to “fit” a normal model to any data.\n    \n      Just calculate $\\mu$ and $\\sigma$\n    \n  \n  But this might not fit the data well.\n::::\n\n\n::::column\n{ width=100% }\n::::\n\n:::\n\n\n\nMixture of Normals\n\n::: columns\n\n:::: column\nThree groups of parameters:\n\n\n  means ($\\boldsymbol\\mu$): location of each component\n  standard deviations ($\\boldsymbol\\sigma$): width of each component\n  Weight ($\\boldsymbol\\pi$): height of each curve\n  Probability Density Function:\n    \n      \n\\[p(x) = \\sum_{i=1}^K \\pi_i\\mathcal{N}(x \\mid \\mu, \\sigma^2)\\]\n      \n    \n  \n\n\n::::\n\n::::column\n{ width=100% }\n::::\n\n:::\n\n\n\nThis solves our problem:\n\n::: columns\n\n:::: column\nReturning to our modelling problem, let’s plot the PDF of a evenly-weighted mixture of the two sample normal models.\n\nWe set:\n\n\n  $K = 2$\n  $\\boldsymbol\\pi = [0.5, 0.5]$\n  $\\boldsymbol\\mu = [-5, 5]$\n  $\\boldsymbol\\sigma = [2, 3]$\n  (bold used to indicate the vector of parameters for each component)\n\n\nIn this case, I knew the right parameters, but normally you would have to estimate, or learn, these somehow…\n::::\n\n::::column\n{ width=100% }\n::::\n\n:::\n\n\n\nMixture Density Networks\n\n\n\n\n  Neural networks used to model complicated real-valued data.\n    \n      i.e., data that might not be very “normal”\n    \n  \n  Usual approach: use a neuron with linear activation to make predictions.\n    \n      Training function could be MSE (mean squared error).\n    \n  \n  Problem! This is equivalent to fitting to a single normal model! 😱\n  (See Bishop, C (1994) for proof and more details)\n\n\n\n\nMixture Density Networks\n\n\n\n\n  Idea: output parameters of a mixture model instead!\n  Rather than MSE for training, use the PDF of the mixture model.\n  Now network can model complicated distributions! 😌\n\n\n\n\nSimple Example in Keras\n\n{ width=30% }\n\n\n  Difficult data is not hard to find! Think about modelling an inverse sine (arcsine) function.\n    \n      Each input value takes multiple outputs…\n      This is not going to go well for a single normal model.\n    \n  \n\n\n\n\nFeedforward MSE Network\n\n{ width=30% }\n{ width=40% }\n\nHere’s a simple two-hidden-layer network (286 parameters), trained to produce the above result.\n\n~~~~~{.python .numberLines}\nmodel = Sequential()\nmodel.add(Dense(15, batch_input_shape=(None, 1), activation=’tanh’))\nmodel.add(Dense(15, activation=’tanh’))\nmodel.add(Dense(1, activation=’linear’))\nmodel.compile(loss=’mse’, optimizer=’rmsprop’)\nmodel.fit(x=x_data, y=y_data, batch_size=128, epochs=200, validation_split=0.15)\n\n\n---\n\n\n## MDN Architecture:\n\n![](./mdn-network.png)\n\n- Loss function for MDN is negative log of likelihood function $\\mathcal{L}$.\n- $\\mathcal{L}$ measures likelihood of $t$ being drawn from a mixture parametrised by $\\mu$, $\\sigma$, and $\\pi$ which are generated by the network inputs $x$:\n\n$$\\mathcal{L} = \\sum_{i=1}^K\\pi_i(\\mathbf{x})\\mathcal{N}\\bigl(\\mu_i(\\mathbf{x}), \\sigma_i^2(\\mathbf{x}); \\mathbf{t} \\bigr)$$\n\n\n---\n\n\n### Feedforward MDN Solution\n\n![](./arcsine-feedforward-mdn-predictions.png){ width=30% }\n![](./arcsine-feedforward-mdn-loss.png){ width=40% }\n\nAnd, here's a simple two-hidden-layer MDN (510 parameters), that achieves the above result! Much better!\n\n~~~~~{.python .numberLines}\nN_MIXES = 5\n\nmodel = Sequential()\nmodel.add(Dense(15, batch_input_shape=(None, 1), activation='relu'))\nmodel.add(Dense(15, activation='relu'))\nmodel.add(mdn.MDN(1, N_MIXES)) # here's the MDN layer!\nmodel.compile(loss=mdn.get_mixture_loss_func(1,N_MIXES), optimizer='rmsprop')\nmodel.summary()\n\n\n\n\nGetting inside the MDN layer\n\nHere’s the same network wihtout using the MDN layer abstraction (this is with Keras’ functional API):\n\n~~~~~{.python .numberLines}\ndef elu_plus_one_plus_epsilon(x):\n    “\"”ELU activation with a very small addition to help prevent NaN in loss.”””\n    return (K.elu(x) + 1 + 1e-8)\n\nN_HIDDEN = 15\nN_MIXES = 5\n\ninputs = Input(shape=(1,), name=’inputs’)\nhidden1 = Dense(N_HIDDEN, activation=’relu’, name=’hidden1’)(inputs)\nhidden2 = Dense(N_HIDDEN, activation=’relu’, name=’hidden2’)(hidden1)\n\nmdn_mus = Dense(N_MIXES, name=’mdn_mus’)(hidden2)\nmdn_sigmas = Dense(N_MIXES, activation=elu_plus_one_plus_epsilon, name=’mdn_sigmas’)(hidden2)\nmdn_pi = Dense(N_MIXES, name=’mdn_pi’)(hidden2)\n\nmdn_out = Concatenate(name=’mdn_outputs’)([mdn_mus, mdn_sigmas, mdn_pi])\n\nmodel = Model(inputs=inputs, outputs=mdn_out)\nmodel.summary()\n\n\n---\n\n\n## Loss Function: The Tricky Bit.\n\nLoss function for the MDN should be the negative log likelihood:\n\n~~~~~{.python .numberLines}\ndef mdn_loss(y_true, y_pred):\n    # Split the inputs into paramaters\n    out_mu, out_sigma, out_pi = tf.split(y_pred, num_or_size_splits=[N_MIXES, N_MIXES, N_MIXES],\n                                         axis=-1, name='mdn_coef_split')\n    mus = tf.split(out_mu, num_or_size_splits=N_MIXES, axis=1)\n    sigs = tf.split(out_sigma, num_or_size_splits=N_MIXES, axis=1)\n    # Construct the mixture models\n    cat = tfd.Categorical(logits=out_pi)\n    coll = [tfd.MultivariateNormalDiag(loc=loc, scale_diag=scale) for loc, scale\n            in zip(mus, sigs)]\n    mixture = tfd.Mixture(cat=cat, components=coll)\n    # Calculate the loss function\n    loss = mixture.log_prob(y_true)\n    loss = tf.negative(loss)\n    loss = tf.reduce_mean(loss)\n    return loss\n\nmodel.compile(loss=mdn_loss, optimizer='rmsprop')\n\n\nLet’s go through bit by bit…\n\n\n\nLoss Function: Part 1:\n\nFirst we have to extract the mixture paramaters.\n\n~~~~~{.python .numberLines}\nSplit the inputs into paramaters\nout_mu, out_sigma, out_pi = tf.split(y_pred, num_or_size_splits=[N_MIXES, N_MIXES, N_MIXES],\n                                     axis=-1, name=’mdn_coef_split’)\nmus = tf.split(out_mu, num_or_size_splits=N_MIXES, axis=1)\nsigs = tf.split(out_sigma, num_or_size_splits=N_MIXES, axis=1)\n\n- Split up the parameters $\\boldsymbol\\mu$, $\\boldsymbol\\sigma$, and $\\boldsymbol\\pi$, remember that there are N_MIXES $= K$ of each of these.\n- $\\boldsymbol\\mu$ and $\\boldsymbol\\sigma$ have to be split _again_ so that we can iterate over them (you can't iterate over an axis of a tensor...)\n\n\n---\n\n\n## Loss Function: Part 2:\n\nNow we have to construct the mixture model's PDF. \n\n~~~~~{.python .numberLines}\n# Construct the mixture models\ncat = tfd.Categorical(logits=out_pi) \ncoll = [tfd.Normal(loc=loc, scale=scale) for loc, scale\n        in zip(mus, sigs)]\nmixture = tfd.Mixture(cat=cat, components=coll)\n\n\n\n  For this, we’re using the Mixture abstraction provided in tensorflow-probability.distributions.\n  This takes a categorical (a.k.a. softmax, a.k.a. generalized Bernoulli distribution) model, and a list the component distributions.\n  Each normal PDF is contructed using tfd.Normal.\n  Can do this from first principles as well, but good to use abstractions that are available (?)\n\n\n\n\nLoss Function: Part 3:\n\nFinally, we calculate the loss:\n\n~~~~~{.python .numberLines}\nloss = mixture.log_prob(y_true)\nloss = tf.negative(loss)\nloss = tf.reduce_mean(loss)\n~~~~~\n\n\n  mixture.log_prob(y_true) means “the log-likelihood of sampling y_true from the distribution called mixture.”\n\n\n\n\nSome more details….\n\n{ width=40% }\n\n\n  This “version” of a mixture model works for a mixture of 1D normal distributions.\n  Not too hard to extend to multivariate normal distributions, which are useful for lots of problems.\n  This is how it actually works in my Keras MDN layer, have a look at the code for more details…\n\n\n\n\nMDN-RNNs\n\n\n\nMDNs can be handy at the end of an RNN! Imagine a robot calculating moves forward through space, it might have to choose from a number of valid positions, each of which could be modelled by a 2D Normal model.\n\n\n\nMDN-RNN Architecture\n\n\n\nCan be as simple as putting an MDN layer after recurrent layers!\n\n\n\nUse Cases: Handwriting Generation\n\n{ width=40% }\n{ width=40% }\n\n\n  Handwriting Generation RNN (Graves, 2013).\n  Trained on handwriting data.\n  Predicts the next location of the pen ($dx$, $dy$, and up/down)\n  Network takes text to write as an extra input, RNN learns to decide what character to write next.\n\n\n\n\nUse Cases: SketchRNN\n\n{ width=40% }\n{ width=40% }\n\n\n  SketchRNN Kanji (Ha, 2015); similar to handwriting generation, trained on kanji and then generates new “fake” characters\n  SketchRNN VAE (Ha et al., 2017); similar again, but trained on human-sourced sketches. VAE architecture with bidirectional RNN encoder and MDN in the decoder part.\n\n\n\n\nUse Cases: RoboJam\n\n::: columns\n\n:::: column\n\n::::\n\n:::: column\n\n  RoboJam (Martin et al., 2018); similar to the kanji RNN, but trained on touchscreen musical performances\n  Extra complexity: have to model touch position ($x$, $y$) and time ($dt$).\n  Implemented in my MicroJam app (have a go: microjam.info)\n::::\n\n\n:::\n\n\n\nUse Cases: World Models\n\n::: columns\n\n:::: column\n\n  World Models (Ha &amp; Schmidhuber, 2018)\n  Train a VAE for visual perception an environment (e.g., VizDoom), now each frame from the environment can be represented by a vector $z$\n  Train MDN to predict next $z$, use this to help train an agent to operate in the environment.\n::::\n\n\n:::: column\n{ width=80% }\n{ width=80% }\n::::\n\n:::\n\n\n\nReferences\n\n\n  Christopher M. Bishop. 1994. Mixture Density Networks. Technical Report NCRG/94/004. Neural Computing Research Group, Aston University.\n  Axel Brando. 2017. Mixture Density Networks (MDN) for distribution and uncertainty estimation. Master’s thesis. Universitat Politècnica de Catalunya.\n  A. Graves. 2013. Generating Sequences With Recurrent Neural Networks. ArXiv e-prints (Aug. 2013). ArXiv:1308.0850\n  David Ha and Douglas Eck. 2017. A Neural Representation of Sketch Drawings. ArXiv e-prints (April 2017). ArXiv:1704.03477\n  Charles P. Martin and Jim Torresen. 2018. RoboJam: A Musical Mixture Density Network for Collaborative Touchscreen Interaction. In Evolutionary and Biologically Inspired Music, Sound, Art and Design: EvoMUSART ’18, A. Liapis et al. (Ed.). Lecture Notes in Computer Science, Vol. 10783. Springer International Publishing. DOI:10.1007/9778-3-319-77583-8_11\n  D. Ha and J. Schmidhuber. 2018. Recurrent World Models Facilitate Policy Evolution. ArXiv e-prints (Sept. 2018). ArXiv:1809.01999\n\n",
    "url": "/assets/mdn/mdn-lecture/"
  },
  
  {
    "title": "Making Predictive NIMEs with Neural Networks",
    "excerpt": "\n",
    "content": "\n\n\nDo you want apply machine learning or AI in creative applications, but don’t know where to start? Do you want to make NIMEs that play themselves? Do you want your computer to compose endless video game soundtracks? Then this is the workshop for you!\n\nWorkshop Description\n\nIn this workshop, we’ll apply predictive machine learning models to creative data and use them in interactive music applications. We will show you how to create and train neural networks and how to use our new toolkit for interactive musical prediction (IMPS). Join us to define new predictive NIME prototypes and future research directions!\n\nThe first half of the workshop will focus on recurrent neural networks (RNNs), deep learning models that can be used to generate sequences and mixture density networks (MDNs) that can creatively predict multivariate data.  We will walk through the steps for training creative RNNs using live-coded demonstrations with Python code in Jupyter Notebooks.\n\nThe second half of the workshop will focus on the Interactive Musical Prediction System (IMP), an end-to-end solution for adding an MDRNN to a musical interface with communication over OSC. This system allows you to focus on defining new predictive  NIMEs that can “fill in” parts of a musical performance or to “dream” new performances and accompaniments.\n\nSession Plan\n\n\n  9:00 Meeting, Intro, and Python Setup\n  9:15 Overview of Deep Learning and Creativity\n  9:45 Generating Text and Music with RNNs (practical)\n  10:30 (coffee break)\n  10:45 Hack: inventing Star Trek episode titles\n  11:30 Using Mixture Density Networks (MDNs)\n  13:00 Lunch (90 minutes)\n  14:30 Making Predictive Musical Interactions with the IMPS system\n  16:00 (coffee break)\n  16:15 Future directions for creative neural networks at NIME and beyond\n\n\nGetting Started\n\nWelcome to the Creative Prediction workshop at NIME! Thanks for coming along!\n\nThis workshop will introduce the basics of deep learning generation of creative sequences (e.g., text, music, videos, movements, etc!). We will cover a bit of the theory behind recurrent neural networks, and mixture density networks, and show you how to construct your own with Python and Keras.\n\nAll of the demonstration code for the workshop is contained in Jupyter Notebooks, an open standard for mixing code, text, and visualisations in a document that can be opened in web browser. We will display this code on the screen for you to follow along and see how it works, but for maximum fun, you’ll want to install Jupyter, Python, and Keras on your own computer\n\nAnother way to try out the Jupyter Notebooks is with Google Colaboratory, a free-to-use Jupyter notebook environment that has most of the necessary Python libraries pre-installed. It even works on a tablet! If you want to get started quickly without slowing down to get your Python install right, Colab is a great way to go.\n\nColab has some amazing features:\n\n\n  You can load all the notebooks for this workshop straight from the GitHub repo.\n  You can use a GPU for free to train a biggish RNN\n\n\nThere are some downsides though:\n\n\n  uploading and downloading data is a bit fiddly.\n  you can’t use music21’s playback features to hear generated MIDI scores, or visualise them with musescore.\n  you can’t run IMPS in Colab (it’s not a jupyter notebook).\n\n\nThe notebooks have some sections with a comment like “Use this if on Colab!” to work around some of the limitations.\n\nFeel free to download all examples (and this website) from our GitHub repository if you want :-D\n\nOverview of Deep Learning and Creativity\n\nSlides\n\n(30 minutes talk).\n\n\n  sequence learning with RNNs and Long Short-Term Memory (LSTM) in Keras and Python\n  Exercise: Set up Python, Keras, and Jupyter on your computer\n  Exercise: Try out a Google Colab Notebook\n\n\nGenerating Text and Music with RNNs\n\nSlides\n\n(30m talk, 60m hack).\n\n\n  Predicting sequences with LSTM networks\n  Generating text character-by-character\n  Generating music note-by-note\n  Demo: Star Trek Episode Title Generator\n  Demo: Zeldic Music Generator\n  Exercise: Make your own text generator\n  \n    Exercise: Make your own melody generator\n  \n  Demo and Tutorial Code\n    \n      \n        Generating Text with a CharRNN; inventing Star Trek episode titles. github colab\n      \n      \n        Advanced CharRNN techniques. github colab\n      \n      \n        Continuing musical sequences. github colab\n      \n      \n        Combining multiple sequences in a CharRNN with “Gesture-RNN” (not working right now..). github colab\n      \n    \n  \n\n\nUsing Mixture Density Networks (MDNs) to predict NIME data with RoboJam\n\nSlides\n\n(45m talk, 45m hack).\n\n\n  Predicting continuous data from NIMEs\n  Gaussian Mixture Models\n  Mixture Density RNNs\n  Demo: Kanji Generation\n  \n    Exercise: Making a RoboJam model with touchscreen data\n  \n  Demo and Tutorial Code\n    \n      \n        Mixture distribution examples github colab\n      \n      \n        Introduction to Mixture Density Layers  github colab\n      \n      \n        Predicting sketching: Kanji generation with a Mixture Density RNN. github colab\n      \n      \n        Predicting time and place - musical scribbles with RoboJam. github colab\n      \n    \n  \n\n\nMaking Predictive Musical Interactions with the IMPS system\n\n\n\nSlides\n\nIMPS information\n\n(30m talk, 90m hack).\n\n\n  The “Interactive Music Prediction System”: IMPS\n  Setup and interaction scheme\n  Demo: IMPS with Lightpad block and Behringer Xtouch Mini\n  Exercise: Set up IMPS with 3D prediction (x, y, t) with Processing and Pd\n\n\nFuture directions for creative neural networks at NIME and beyond\n\nSlides\n\nLet’s define the future of predictive NIMEs… together.\n\n\n  What even is a predictive NIME?\n  What NIMEs could benefit from prediction?\n  What tools are needed?\n  Who can help?\n  What resources are necessary?\n  What’s going on with this “Creative AI” field anyway?\n\n\n(60m discussion and brainstorming).\n",
    "url": "/workshop/"
  }
  
]

