{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A VAE to Generate Doom Screens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Flatten, Reshape, Layer\n",
    "from keras.layers import Conv2D, Conv2DTranspose\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from keras.datasets import cifar10\n",
    "from keras import layers\n",
    "import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-08-28 23:09:41--  http://folk.uio.no/charlepm/datasets/doom_images.npz\n",
      "Resolving folk.uio.no (folk.uio.no)... 129.240.121.81, 2001:700:100:121::81\n",
      "Connecting to folk.uio.no (folk.uio.no)|129.240.121.81|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 74242603 (71M)\n",
      "Saving to: ‘doom_images.npz’\n",
      "\n",
      "doom_images.npz     100%[===================>]  70,80M  4,42MB/s    in 20s     \n",
      "\n",
      "2019-08-28 23:10:01 (3,48 MB/s) - ‘doom_images.npz’ saved [74242603/74242603]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Get some Data...\n",
    "!wget http://folk.uio.no/charlepm/datasets/doom_images.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23874, 64, 64, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=300x300 at 0x7F194817B278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test loading new file.\n",
    "with np.load('doom_images.npz') as data:\n",
    "    doom_arrays = data['arr_0']\n",
    "display(doom_arrays.shape)\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from keras.preprocessing import image\n",
    "img = image.array_to_img(doom_arrays[0], scale=False)\n",
    "display(img.resize((300, 300)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (23874, 64, 64, 3)\n",
      "23874 train samples\n"
     ]
    }
   ],
   "source": [
    "# The data, split between train and test sets:\n",
    "x_train = doom_arrays\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "#print(x_test.shape[0], 'test samples')\n",
    "\n",
    "x_train = x_train.astype('float32') / 255\n",
    "#x_test = x_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Hyperparameters\n",
    "img_rows, img_cols, img_chns = 64, 64, 3\n",
    "latent_dim = 64\n",
    "intermediate_dim = 128\n",
    "epsilon_std = 1.0\n",
    "filters = 32\n",
    "num_conv = 3\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 50\n",
    "\n",
    "img_size = (img_rows, img_cols, img_chns)\n",
    "original_dim = img_rows * img_cols * img_chns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before flattening: (None, 16, 16, 32)\n"
     ]
    }
   ],
   "source": [
    "# Encoder\n",
    "input_img = Input(shape=img_size, name='encoder_input')\n",
    "x = Conv2D(img_chns, kernel_size=(2, 2), padding='same', activation='relu')(input_img)\n",
    "x = Conv2D(filters, kernel_size=(2, 2), padding='same', activation='relu', strides=(2, 2))(x)\n",
    "x = Conv2D(filters, kernel_size=(2, 2), padding='same', activation='relu', strides=(2, 2))(x)\n",
    "# x = keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='same')(x) # try a max pooling layer here instead of the previous stride\n",
    "x = Conv2D(filters, kernel_size=num_conv, padding='same', activation='relu', strides=1)(x)\n",
    "shape_before_flattening = K.int_shape(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(intermediate_dim, activation='relu', name='latent_project')(x)\n",
    "\n",
    "print(\"Shape before flattening:\", shape_before_flattening)\n",
    "\n",
    "# mean and var\n",
    "z_mean = Dense(latent_dim, name='Z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='Z_var')(x)\n",
    "\n",
    "# make an encoder model (not used until after training)\n",
    "encoder = Model(input_img, z_mean)\n",
    "\n",
    "# sampling layer\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., stddev=1.)\n",
    "    return z_mean + K.exp(z_log_var) * epsilon\n",
    "\n",
    "z = layers.Lambda(sampling, name=\"Z_sample\")([z_mean, z_log_var])\n",
    "\n",
    "# decoder\n",
    "decoder_input = layers.Input(K.int_shape(z)[1:])\n",
    "y = Dense(intermediate_dim, activation='relu')(decoder_input)  # (z)\n",
    "y = Dense(np.prod(shape_before_flattening[1:]), activation='relu')(y)\n",
    "y = Reshape(shape_before_flattening[1:])(y)\n",
    "y = Conv2DTranspose(filters, kernel_size=num_conv, padding='same', strides=1, activation='relu',\n",
    "                    name='deconv_1')(y)  # deconv 1\n",
    "y = Conv2DTranspose(filters, kernel_size=num_conv, padding='same', strides=(2, 2), activation='relu',\n",
    "                    name='deconv_2')(y)  # deconv 2\n",
    "y = Conv2DTranspose(filters, kernel_size=(3, 3), strides=(2, 2), padding='valid', activation='relu',\n",
    "                    name='deconv_3')(y)  # deconv 3, upsamp\n",
    "y = Conv2D(img_chns, kernel_size=2, padding='valid', activation='sigmoid', name=\"mean_squash\")(y)  # mean squash\n",
    "decoder = Model(decoder_input, y, name=\"Decoder\")\n",
    "z_decoded = decoder(z)  # y\n",
    "\n",
    "def xent(y_true, y_pred):\n",
    "    return keras.metrics.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "def kl_measure(loc, log_var):\n",
    "    return -0.5 * K.mean(1 + log_var - K.square(loc) - K.exp(log_var), axis=-1)\n",
    "\n",
    "def kl_custom_metric(y_true, y_pred):\n",
    "    # Ignore input and take from z tensors.\n",
    "    return kl_measure(z_mean, z_log_var)\n",
    "\n",
    "class VAELayer(keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(VAELayer, self).__init__(**kwargs)\n",
    "\n",
    "    def vae_loss(self, x, z_decoded):\n",
    "        x = K.flatten(x)\n",
    "        z_decoded = K.flatten(z_decoded)\n",
    "        r_loss = original_dim * xent(x, z_decoded)\n",
    "        kl_loss = kl_measure(z_mean, z_log_var)\n",
    "        print(\"KL Shape:\", K.int_shape(kl_loss))\n",
    "        print(\"Xent shape:\", K.int_shape(r_loss))\n",
    "        return K.mean(r_loss + kl_loss)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        z_decoded = inputs[1]\n",
    "        loss = self.vae_loss(x, z_decoded)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        return x\n",
    "\n",
    "y = VAELayer()([input_img, z_decoded])\n",
    "\n",
    "vae = Model(input_img, y, name=\"VAE\")\n",
    "vae.compile(optimizer='adam', metrics=['mse', 'binary_crossentropy'])\n",
    "\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node conv2d_1/convolution}} = Conv2D[T=DT_FLOAT, _class=[\"loc:@training/Adam/gradients/conv2d_1/convolution_grad/Conv2DBackpropFilter\"], data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training/Adam/gradients/conv2d_1/convolution_grad/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer, conv2d_1/kernel/read)]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-c95a1a2c83d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# try different params here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"full_vae_weights.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"encoder_only_weights.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"decoder_only_weights.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnknownError\u001b[0m: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node conv2d_1/convolution}} = Conv2D[T=DT_FLOAT, _class=[\"loc:@training/Adam/gradients/conv2d_1/convolution_grad/Conv2DBackpropFilter\"], data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training/Adam/gradients/conv2d_1/convolution_grad/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer, conv2d_1/kernel/read)]]"
     ]
    }
   ],
   "source": [
    "history = vae.fit(x_train, shuffle=True, epochs=50, batch_size=128) # try different params here\n",
    "\n",
    "vae.save_weights(filepath+\"full_vae_weights.h5\")\n",
    "encoder.save_weights(filepath+\"encoder_only_weights.h5\")\n",
    "decoder.save_weights(filepath+\"decoder_only_weights.h5\")\n",
    "\n",
    "# Generates latent z-values for all pictures in one rollout.\n",
    "def generate_latent_variables(input):\n",
    "    z_mean = encoder.predict(input, batch_size=1)\n",
    "    return z_mean\n",
    "\n",
    "# Regenerates pictures based on latent values.\n",
    "def generate_picture_from_latent(latent_variables):\n",
    "    decoded_images = decoder.predict(latent_variables)\n",
    "    return decoded_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder part only:\n",
    "ex = x_train[np.random.randint(len(x_train))]\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(ex) # cmap ignored if input is 3D (as it should be here)\n",
    "plt.show()\n",
    "\n",
    "enc_z = encoder.predict(np.array([ex]))\n",
    "print(enc_z[0])\n",
    "\n",
    "ex_dec = decoder.predict(np.array([enc_z[0]]))\n",
    "# Plot output\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(ex_dec[0]) # cmap ignored if input is 3D (as it should be here)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input\n",
    "ex = x_train[np.random.randint(len(x_train))]\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(ex) # cmap ignored if input is 3D (as it should be here)\n",
    "plt.show()\n",
    "\n",
    "# Predict output\n",
    "ex_dec = vae.predict(np.array([ex]))\n",
    "\n",
    "# Plot output\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(ex_dec[0]) # cmap ignored if input is 3D (as it should be here)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# display images generated from randomly sampled latent vector\n",
    "n = 10 # num images\n",
    "img_size = 64\n",
    "figure = np.zeros((img_size * n, img_size * n, img_chns))\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        z_sample = np.array([np.random.uniform(-1,1 ,size=latent_dim)])\n",
    "        x_decoded = decoder.predict(z_sample)\n",
    "        img = x_decoded[0].reshape(img_size, img_size, img_chns)\n",
    "        figure[i * img_size: (i + 1) * img_size,j * img_size: (j + 1) * img_size] = img\n",
    "\n",
    "        #plt.figure(figsize=(5, 5))\n",
    "        #plt.imshow(img, cmap='Greys_r')\n",
    "        #plt.show()\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(figure) # cmap ignored if input is 3D (as it should be here)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and download models\n",
    "\n",
    "def save_model_three_ways(model, name=\"model\"):\n",
    "  # Save the weights\n",
    "  model.save(\"./models/doom_\" + name + ('_ld_%d_conv_%d_id_%d_e_%d.h5' % (latent_dim, num_conv, intermediate_dim, epochs)))\n",
    "  model.save_weights(\"./models/doom_\" + name + '_weights.h5')\n",
    "  # Save the model architecture\n",
    "  with open(\"./models/doom_\" + name + '_architecture.json', 'w') as f:\n",
    "    f.write(model.to_json())\n",
    "\n",
    "save_model_three_ways(vae, name=\"vae\")\n",
    "save_model_three_ways(encoder, name=\"encoder\")\n",
    "save_model_three_ways(encoder, name=\"decoder\")\n",
    "\n",
    "!tar -czvf doom_models.tar.gz models\n",
    "\n",
    "from google.colab import files\n",
    "files.download('doom_models.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder part only:\n",
    "ex = x_train[1]\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(ex) # cmap ignored if input is 3D (as it should be here)\n",
    "plt.show()\n",
    "\n",
    "enc_z = encoder.predict(np.array([ex]))[0]\n",
    "print(enc_z)\n",
    "z_1 = enc_z[0]\n",
    "z_2 = enc_z[1]\n",
    "z_3 = enc_z[2]\n",
    "z_4 = enc_z[3]\n",
    "z_5 = enc_z[4]\n",
    "z_6 = enc_z[5]\n",
    "z_7 = enc_z[6]\n",
    "z_8 = enc_z[7]\n",
    "z_9 = enc_z[8]\n",
    "z_10 = enc_z[9]\n",
    "z_11 = enc_z[10]\n",
    "z_12 = enc_z[11]\n",
    "z_13 = enc_z[12]\n",
    "z_14 = enc_z[13]\n",
    "z_15 = enc_z[14]\n",
    "z_16 = enc_z[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Number fields { run: \"auto\", vertical-output: true, form-width: \"50px\" }\n",
    "z_1 = -0.86 #@param {type:\"slider\", min:-1, max:1, step:0.01}\n",
    "z_2 = -0.93 #@param {type:\"slider\", min:-1, max:1, step:0.01}\n",
    "z_3 = 0.39 #@param {type:\"slider\", min:-1, max:1, step:0.01}\n",
    "z_4 = -0.83 #@param {type:\"slider\", min:-1, max:1, step:0.01}\n",
    "z_5 = 0.15 #@param {type:\"slider\", min:-1, max:1, step:0.01}\n",
    "z_6 = 0.23 #@param {type:\"slider\", min:-1, max:1, step:0.01}\n",
    "z_7 = 0.51 #@param {type:\"slider\", min:-1, max:1, step:0.01}\n",
    "z_8 = -0.8 #@param {type:\"slider\", min:-1, max:1, step:0.01}\n",
    "z_9 = 0.77 #@param {type:\"slider\", min:-1, max:1, step:0.01}\n",
    "z_10 = -0.69 #@param {type:\"slider\", min:-1, max:1, step:0.01}\n",
    "z_11 = 0 #@param {type:\"slider\", min:-1, max:1, step:0.01}\n",
    "z_12 = -0.73 #@param {type:\"slider\", min:-1, max:1, step:0.01}\n",
    "z_13 = -0.48 #@param {type:\"slider\", min:-1, max:1, step:0.01}\n",
    "z_14 = -0.6 #@param {type:\"slider\", min:-1, max:1, step:0.01}\n",
    "z_15 = 0.56 #@param {type:\"slider\", min:-1, max:1, step:0.01}\n",
    "z_16 = 0.52 #@param {type:\"slider\", min:-1, max:1, step:0.01}\n",
    "\n",
    "new_z= np.array([z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_10,z_11,z_12,z_13,z_14,z_15,z_16])\n",
    "print(new_z)\n",
    "dec = decoder.predict(np.array([new_z]))\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(dec[0]) # cmap ignored if input is 3D (as it should be here)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
