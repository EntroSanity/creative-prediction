{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A VAE to Generate Doom Screens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Flatten, Reshape, Layer\n",
    "from keras.layers import Conv2D, Conv2DTranspose\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from keras.datasets import cifar10\n",
    "from keras import layers\n",
    "import keras\n",
    "\n",
    "\n",
    "img_rows, img_cols, img_chns = 64, 64, 3\n",
    "latent_dim = 64\n",
    "intermediate_dim = 128\n",
    "epsilon_std = 1.0\n",
    "epochs = 50\n",
    "filters = 32\n",
    "num_conv = 3\n",
    "batch_size = 100\n",
    "\n",
    "img_size = (img_rows, img_cols, img_chns)\n",
    "original_dim = img_rows * img_cols * img_chns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get some Data...\n",
    "!wget http://folk.uio.no/charlepm/datasets/doom_images.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doom_corpus = np.load('doom_images.npz')\n",
    "doom_arrays = doom_corpus['arr_0']\n",
    "display(doom_arrays.shape)\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from keras.preprocessing import image\n",
    "img = image.array_to_img(doom_arrays[0], scale=False)\n",
    "display(img.resize((300, 300)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "input_img = Input(shape=img_size, name='encoder_input')\n",
    "x = Conv2D(img_chns, kernel_size=(2, 2), padding='same', activation='relu')(input_img)\n",
    "x = Conv2D(filters, kernel_size=(2, 2), padding='same', activation='relu', strides=(2, 2))(x)\n",
    "x = Conv2D(filters, kernel_size=(2, 2), padding='same', activation='relu', strides=(2, 2))(x)\n",
    "# x = keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='same')(x) # try a max pooling layer here instead of the previous stride\n",
    "x = Conv2D(filters, kernel_size=num_conv, padding='same', activation='relu', strides=1)(x)\n",
    "shape_before_flattening = K.int_shape(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(intermediate_dim, activation='relu', name='latent_project')(x)\n",
    "\n",
    "print(\"Shape before flattening:\", shape_before_flattening)\n",
    "\n",
    "# mean and var\n",
    "z_mean = Dense(latent_dim, name='Z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='Z_var')(x)\n",
    "\n",
    "# make an encoder model (not used until after training)\n",
    "encoder = Model(input_img, z_mean)\n",
    "\n",
    "# sampling layer\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., stddev=1.)\n",
    "    return z_mean + K.exp(z_log_var) * epsilon\n",
    "\n",
    "z = layers.Lambda(sampling, name=\"Z_sample\")([z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder\n",
    "decoder_input = layers.Input(K.int_shape(z)[1:])\n",
    "y = Dense(intermediate_dim, activation='relu')(decoder_input)  # (z)\n",
    "y = Dense(np.prod(shape_before_flattening[1:]), activation='relu')(y)\n",
    "y = Reshape(shape_before_flattening[1:])(y)\n",
    "y = Conv2DTranspose(filters, kernel_size=num_conv, padding='same', strides=1, activation='relu',\n",
    "                    name='deconv_1')(y)  # deconv 1\n",
    "y = Conv2DTranspose(filters, kernel_size=num_conv, padding='same', strides=(2, 2), activation='relu',\n",
    "                    name='deconv_2')(y)  # deconv 2\n",
    "y = Conv2DTranspose(filters, kernel_size=(3, 3), strides=(2, 2), padding='valid', activation='relu',\n",
    "                    name='deconv_3')(y)  # deconv 3, upsamp\n",
    "y = Conv2D(img_chns, kernel_size=2, padding='valid', activation='sigmoid', name=\"mean_squash\")(y)  # mean squash\n",
    "decoder = Model(decoder_input, y, name=\"Decoder\")\n",
    "z_decoded = decoder(z)  # y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xent(y_true, y_pred):\n",
    "    return keras.metrics.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "def kl_measure(loc, log_var):\n",
    "    return -0.5 * K.mean(1 + log_var - K.square(loc) - K.exp(log_var), axis=-1)\n",
    "\n",
    "def kl_custom_metric(y_true, y_pred):\n",
    "    # Ignore input and take from z tensors.\n",
    "    return kl_measure(z_mean, z_log_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAELayer(keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(VAELayer, self).__init__(**kwargs)\n",
    "\n",
    "    def vae_loss(self, x, z_decoded):\n",
    "        x = K.flatten(x)\n",
    "        z_decoded = K.flatten(z_decoded)\n",
    "        r_loss = original_dim * xent(x, z_decoded)\n",
    "        kl_loss = kl_measure(z_mean, z_log_var)\n",
    "        print(\"KL Shape:\", K.int_shape(kl_loss))\n",
    "        print(\"Xent shape:\", K.int_shape(r_loss))\n",
    "        return K.mean(r_loss + kl_loss)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        z_decoded = inputs[1]\n",
    "        loss = self.vae_loss(x, z_decoded)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        return x\n",
    "\n",
    "y = VAELayer()([input_img, z_decoded])\n",
    "\n",
    "vae = Model(input_img, y, name=\"VAE\")\n",
    "vae.compile(optimizer='adam', metrics=['mse', 'binary_crossentropy'])\n",
    "\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(data, shuffle=True, epochs=50, batch_size=128)\n",
    "\n",
    "model.save_weights(filepath+\"full_vae_weights.h5\")\n",
    "encoder.save_weights(filepath+\"encoder_only_weights.h5\")\n",
    "decoder.save_weights(filepath+\"decoder_only_weights.h5\")\n",
    "\n",
    "# Generates latent z-values for all pictures in one rollout.\n",
    "def generate_latent_variables(input):\n",
    "    z_mean = encoder.predict(input, batch_size=1)\n",
    "    return z_mean\n",
    "\n",
    "# Regenerates pictures based on latent values.\n",
    "def generate_picture_from_latent(latent_variables):\n",
    "    decoded_images = decoder.predict(latent_variables)\n",
    "    return decoded_images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
