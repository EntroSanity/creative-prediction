{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Star Trek Titles with an RNN\n",
    "\n",
    "- Trains on Star Trek episode titles\n",
    "- Outputs \"fake\" titles.\n",
    "- Uses the \"charRNN\" idea.\n",
    "\n",
    "Much comes from a [Keras example](https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py).\n",
    "\n",
    "## Setup Environment\n",
    "\n",
    "- Import Keras\n",
    "- Open up the Star Trek corpus\n",
    "- We need to translate the textual data into a format that the RNN can accept as input.\n",
    "- Give each letter an index and create dictionaries to translate from index to character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 11010\n",
      "total chars: 49\n",
      "Max: 50\n",
      "Mean: 14.001362397820163\n",
      "Median: 13.0\n",
      "Min: 2\n",
      "\n",
      "Character Dictionary:  {'\\n': 0, ' ': 1, '!': 2, \"'\": 3, '(': 4, ')': 5, ',': 6, '-': 7, '.': 8, '0': 9, '1': 10, '2': 11, '3': 12, '4': 13, '5': 14, '7': 15, '8': 16, '9': 17, ':': 18, '?': 19, 'a': 20, 'b': 21, 'c': 22, 'd': 23, 'e': 24, 'f': 25, 'g': 26, 'h': 27, 'i': 28, 'j': 29, 'k': 30, 'l': 31, 'm': 32, 'n': 33, 'o': 34, 'p': 35, 'q': 36, 'r': 37, 's': 38, 't': 39, 'u': 40, 'v': 41, 'w': 42, 'x': 43, 'y': 44, 'z': 45, 'à': 46, 'é': 47, '’': 48}\n",
      "\n",
      "Inverse Character Dictionary:  {0: '\\n', 1: ' ', 2: '!', 3: \"'\", 4: '(', 5: ')', 6: ',', 7: '-', 8: '.', 9: '0', 10: '1', 11: '2', 12: '3', 13: '4', 14: '5', 15: '7', 16: '8', 17: '9', 18: ':', 19: '?', 20: 'a', 21: 'b', 22: 'c', 23: 'd', 24: 'e', 25: 'f', 26: 'g', 27: 'h', 28: 'i', 29: 'j', 30: 'k', 31: 'l', 32: 'm', 33: 'n', 34: 'o', 35: 'p', 36: 'q', 37: 'r', 38: 's', 39: 't', 40: 'u', 41: 'v', 42: 'w', 43: 'x', 44: 'y', 45: 'z', 46: 'à', 47: 'é', 48: '’'}\n"
     ]
    }
   ],
   "source": [
    "## Much borrowed from https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.callbacks import LambdaCallback\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "\n",
    "text = open(\"../datasets/startrekepisodes.txt\").read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocabulary_size = len(chars)\n",
    "print('total chars:', vocabulary_size)\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "\n",
    "# How long is a title?\n",
    "titles = text.split('\\n')\n",
    "lengths = np.array([len(n) for n in titles])\n",
    "print(\"Max:\", np.max(lengths))\n",
    "print(\"Mean:\", np.mean(lengths))\n",
    "print(\"Median:\", np.median(lengths))\n",
    "print(\"Min:\", np.min(lengths))\n",
    "print()\n",
    "\n",
    "# hence choose 30 as seuence length to train on.\n",
    "print(\"Character Dictionary: \", char_indices)\n",
    "print()\n",
    "print(\"Inverse Character Dictionary: \", indices_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Training Data\n",
    "\n",
    "- Cut up the corpus into semi-redundant sequences of 30 characters.\n",
    "- Change indices into \"one-hot\" vector encodings.\n",
    "\n",
    "<img src=\"figures/slicing_text.png\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences (Xs): 3660\n",
      "Number of next_chars (ys): 3660\n",
      "\n",
      "Here's the first example:\n",
      "X: the man trap\n",
      "charlie x\n",
      "where n\n",
      "y: o\n"
     ]
    }
   ],
   "source": [
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 30\n",
    "step = 3\n",
    "\n",
    "sentences = [] #The training data\n",
    "next_chars = [] #The training labels\n",
    "\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "    \n",
    "print('Number of sequences (Xs):', len(sentences))\n",
    "print('Number of next_chars (ys):', len(next_chars))\n",
    "\n",
    "print(\"\\nHere's the first example:\")\n",
    "print(\"X:\",sentences[0])\n",
    "print(\"y:\",next_chars[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Onehot encoding:\n",
    "\n",
    "* a -> [1, 0, 0, ..., 0]\n",
    "* b -> [0, 1, 0, ..., 0]\n",
    "* ...\n",
    "\n",
    "Each training sample becomes 2D tensor:\n",
    "\n",
    "* \"This is the text\" -> X = [[0, 0, ..., 1, 0, ..., 0], ..., [0, 0, ..., 1, 0, ... 0]]\n",
    "\n",
    "Each target (next letter) becomes 1D onehot tensor:\n",
    "\n",
    "* a -> y = [1, 0, 0, ..., 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done preparing training corpus, shapes of sets are:\n",
      "X shape: (3660, 30, 49)\n",
      "y shape: (3660, 49)\n",
      "Vocabulary of characters: 49\n"
     ]
    }
   ],
   "source": [
    "#X shape: 3D tensor. First dimension is the sentences, second is each letter in each sentence, third is the onehot\n",
    "#vector representing that letter.\n",
    "X = np.zeros((len(sentences), maxlen, vocabulary_size), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), vocabulary_size), dtype=np.bool)\n",
    "    \n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "    \n",
    "print(\"Done preparing training corpus, shapes of sets are:\")\n",
    "print(\"X shape: \" + str(X.shape))\n",
    "print(\"y shape: \" + str(y.shape))\n",
    "print(\"Vocabulary of characters:\", vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False  True False False False False False False False False\n",
      " False]\n"
     ]
    }
   ],
   "source": [
    "# Look at some data:\n",
    "\n",
    "print(X[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "- Model has one hidden layer of 128 LSTM cells.\n",
    "- Output layer uses the \"softmax\" activation function to output a probability distribution over next letters.\n",
    "\n",
    "This is model is designed for \"one-by-one\" prediction, i.e., it predicts the very next letter in a sequence of text. \n",
    "\n",
    "- For the sentence \"My cat is named Simon\"\n",
    "   - x: \"My cat is named Simo\"\n",
    "   - y: \"n\"\n",
    "   \n",
    "The RNN is structured as follows:\n",
    "\n",
    "<img src=\"figures/n-in-1-out.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TODO: Cut out of skeleton.\n",
    "\n",
    "layer_size = 128\n",
    "# build the model: a single LSTM layer.\n",
    "model_train = Sequential()\n",
    "\n",
    "model_train.add(LSTM(layer_size, input_shape=(maxlen, len(chars))))\n",
    "# Project back to vocabulary. One output node for each letter.\n",
    "# Dense indicates a fully connected layer.\n",
    "# Softmax activation ensures the combined values of all outputs form a probability distribution:\n",
    "# They sum to 1, with each individual value between 0 and 1.\n",
    "model_train.add(Dense(len(chars), activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 128)               91136     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 49)                6321      \n",
      "=================================================================\n",
      "Total params: 97,457\n",
      "Trainable params: 97,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Categorical crossentropy  minimizes the distance between the probability distributions \n",
    "# output by the network and the true distribution of the targets.\n",
    "# The optimizer specifies HOW the gradient of the loss will be used to update parameters.\n",
    "# Different optimizers have different tricks to avoid local optima, etc.\n",
    "# RMSProp is adaptive, adjusting the rate of learning to how fast we're currently learning.\n",
    "# Choose one by experimenting, or selecting one documented to work well for this problem by other researchers.\n",
    "model_train.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.01))\n",
    "model_train.summary()\n",
    "\n",
    "# LSTM is more complicated than the basic RNN we introduced. It has more free parameters, therefore more parameters \n",
    "# than one might expect below. We use them since they are better at learning long-term structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from the Model\n",
    "\n",
    "- The model doesn't output _letters_, but a distribution for the probability for each letter. - Could just take letter with max probability\n",
    "- Better to do a random sampling from the distribution.\n",
    "- Also have opportunity to \"reweight\" the distribution, to make more \"creative\" choices.\n",
    "\n",
    "<img src=\"figures/reweighting.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "- Here's the code for the sampling function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Higher diversity -> more randomness in the generation.\n",
    "def sample(probability_distribution, diversity=1.0):\n",
    "    # helper function to sample an index from a probability distribution\n",
    "    probability_distribution = np.asarray(probability_distribution).astype('float64')\n",
    "    # Reweight the distribution\n",
    "    probability_distribution = np.log(probability_distribution) / diversity\n",
    "    # Here's the Softmax operation\n",
    "    exp_preds = np.exp(probability_distribution)\n",
    "    probability_distribution = exp_preds / np.sum(exp_preds)\n",
    "    #Draws 1 element at random according to the new scaled probability-distribution.\n",
    "    probabilities = np.random.multinomial(n=1, pvals = probability_distribution) \n",
    "    return np.argmax(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method for printing some example text after every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_segment(length, diversity, generating_model = model_train, input_sequence_length = maxlen):\n",
    "    start_index = random.randint(0, len(text) - input_sequence_length - 1)\n",
    "\n",
    "    # We need a seed to start the text generation. Since during training the ANN always experiences\n",
    "    # sentences of size 30, we seed it with a sentence of length 30 to get it into a sensible state.\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + input_sequence_length]\n",
    "    generated += sentence\n",
    "    \n",
    "    for i in range(length):\n",
    "        x_pred = np.zeros((1, input_sequence_length, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_indices[char]] = 1.\n",
    "        \n",
    "\n",
    "        predictions_distribution = generating_model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(predictions_distribution, diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        #Stepping one symbol forward in the sentence\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "    return generated\n",
    "\n",
    "def generate_sample_text(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    generated = generate_text_segment(200, 1.0, model_train, input_sequence_length = maxlen)\n",
    "    print(\"Seed:\\n\", generated[:30], \"\\n\")\n",
    "    print(\"Generated text:\\n\", generated[30:], \"\\n\\n\")\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=generate_sample_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "- Training in Keras is done by calling `model_train.fit(X,y)`, where `X`, `y` are the data corpus we prepared earlier.\n",
    "- There's two important paramters for training:\n",
    "    - Batch size: How many examples are used to make one weight update in the model.\n",
    "    - Number of epochs: How many times to iterate through the whole dataset (randomised batches each time).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3660/3660 [==============================] - 2s 543us/step - loss: 3.2142\n",
      "Seed:\n",
      " allegiance\n",
      "captain's holiday\n",
      "t \n",
      "\n",
      "Generated text:\n",
      "  o  ihh sh ce  eehlfnei \n",
      "eg t \n",
      "o hesn h th b v niae\n",
      "\n",
      "ieeia huocchvepdorl\n",
      " snfhhsecrog ogtoan\n",
      "\n",
      "n r gotdfna\n",
      "t\n",
      "ote\n",
      "17egenmli ahdifsrac hav\n",
      " n tsosrl ah ntutospoeea h  ce g'f eadf d\n",
      "ncd1iic vm\n",
      ", ws\n",
      " nerag \n",
      "\n",
      "\n",
      "Epoch 2/50\n",
      "3660/3660 [==============================] - 1s 297us/step - loss: 2.9175\n",
      "Seed:\n",
      " aker\n",
      "parallax\n",
      "time and again\n",
      "l \n",
      "\n",
      "Generated text:\n",
      " .r\n",
      "tandper varyrp natro he\n",
      "tumius\n",
      "ocapfhe fithenanwdnrruri\n",
      "soeroo v tracinbrdhtic hottaa sobnt phaneckob\n",
      "tho ,ian\n",
      " bhp arlaiierdsl(crr hhar\n",
      "crt'\n",
      "fha iarymzf mh tar  1re?aurilhthondh\n",
      "bamkoraghod\n",
      "alaof  \n",
      "\n",
      "\n",
      "Epoch 3/50\n",
      "3660/3660 [==============================] - 1s 329us/step - loss: 2.6395\n",
      "Seed:\n",
      "  her voice\n",
      "tears of the prophe \n",
      "\n",
      "Generated text:\n",
      " muntiluncsfvin shesko\n",
      "s shennemancve)\n",
      "mamcneamewothesce,reaetdusskawd\n",
      "penstonrn\n",
      "ghnq,ixsbac\n",
      "ol fort sopamklrmthegs\n",
      "hinb\n",
      "hhont\n",
      "l meunt f e\n",
      "vhes\n",
      "seamesbrnh c, wnts\n",
      "bon do cathesthonc\n",
      "orskirtinma fmsmalw \n",
      "\n",
      "\n",
      "Epoch 4/50\n",
      "3660/3660 [==============================] - 1s 319us/step - loss: 2.4979\n",
      "Seed:\n",
      "  little war\n",
      "return to tomorrow \n",
      "\n",
      "Generated text:\n",
      " rocallifertion's\n",
      "care\n",
      "thusbalinbeyaslegne gurr\n",
      "thonknsry4morres\n",
      "nars,aty\n",
      "snuy\n",
      "dat\n",
      "ulsycorksicelbokperles\n",
      "ofelinbationacs,rlere\n",
      "rewerallsiiisetha te-agesusyonecorci1ntlf all be silnelws\n",
      "part ty\n",
      "saparet \n",
      "\n",
      "\n",
      "Epoch 5/50\n",
      "3660/3660 [==============================] - 1s 316us/step - loss: 2.3737\n",
      "Seed:\n",
      " assians\n",
      "phantasms\n",
      "melora\n",
      "dark  \n",
      "\n",
      "Generated text:\n",
      " cog\n",
      "aela\n",
      "ian\n",
      "bave lesa\n",
      "d\n",
      "jakt i\n",
      "fi\n",
      "mard,ts part s\n",
      "im an awion\n",
      "the me alesi2\n",
      "aari\n",
      "wui\n",
      "ar\n",
      "t e fosty\n",
      "sentres\n",
      "cowl ag\n",
      "the wehinat\n",
      "sitood\n",
      "invt 1. bl crvinn\n",
      "\n",
      "sortt o\n",
      " potl pe\n",
      "paste\n",
      "w\n",
      "pa thi fircs, part i\n",
      " o \n",
      "\n",
      "\n",
      "Epoch 6/50\n",
      "3660/3660 [==============================] - 1s 303us/step - loss: 2.2730\n",
      "Seed:\n",
      " ood oath\n",
      "journey's end\n",
      "the maq \n",
      "\n",
      "Generated text:\n",
      " tkechont\n",
      "jeu mey\n",
      "the thoimencon: alplivith 1f perkegetcond\n",
      "herkil\n",
      "colancirvonhe wigatthod or the patd mound\n",
      "anterras\n",
      "wbronis\n",
      "the amhont\n",
      "mrybcaclest\n",
      "bapmods oud shacustace\n",
      "blog ore dcima\n",
      "dhoon\n",
      "shat vi  \n",
      "\n",
      "\n",
      "Epoch 7/50\n",
      "3660/3660 [==============================] - 1s 336us/step - loss: 2.1675\n",
      "Seed:\n",
      " o\n",
      "family\n",
      "brothers\n",
      "suddenly hum \n",
      "\n",
      "Generated text:\n",
      " evencofsitho dagpon\n",
      "\n",
      "mqusspatcon\n",
      "aa pait llcban\n",
      "the pardesacarsypasd\n",
      "domend\n",
      "fifithe\n",
      "s\n",
      "palpg\n",
      "the samaladofocancfmifane4thd\n",
      "aycenoncsadath1musevilaker dayavind\n",
      "chaty\n",
      "vhresponceaccdadatca\n",
      "donk midpanaach \n",
      "\n",
      "\n",
      "Epoch 8/50\n",
      "3660/3660 [==============================] - 1s 359us/step - loss: 2.0771\n",
      "Seed:\n",
      " d front\n",
      "silent enemy\n",
      "dear doct \n",
      "\n",
      "Generated text:\n",
      " uaml\n",
      "tyete\n",
      "ye\n",
      "fis ou\n",
      "ficatite\n",
      "aqulatevi\n",
      "twesueyion)\n",
      "deakedy teceon\n",
      "the meten\n",
      "fi\n",
      "oumint\n",
      "of teisete\n",
      "the verenit onceageter\n",
      "dousteuun 1f oese repe\n",
      "mirenend\n",
      "s tho baceyead\n",
      "the caye's uner\n",
      "facti\n",
      "hescesonc\n",
      " \n",
      "\n",
      "\n",
      "Epoch 9/50\n",
      "3660/3660 [==============================] - 1s 363us/step - loss: 1.9657\n",
      "Seed:\n",
      " ents\n",
      "message in a bottle\n",
      "who m \n",
      "\n",
      "Generated text:\n",
      " oncein\n",
      "the piogh\n",
      "sheere\n",
      "the beuslime\n",
      "dobl astave seaeite\n",
      "the iduspor ofehel yocous hetr plodoly\n",
      "bhric dis\n",
      "eay the of the quhime\n",
      "the kictqi\n",
      "al part of methai\n",
      "parrec limere ondin\n",
      "the cousk\n",
      "s\n",
      "teget\n",
      "foure \n",
      "\n",
      "\n",
      "Epoch 10/50\n",
      "3660/3660 [==============================] - 1s 344us/step - loss: 1.8531\n",
      "Seed:\n",
      " gone before\n",
      "lonely among us\n",
      "ju \n",
      "\n",
      "Generated text:\n",
      " ls's oa enon\n",
      "the cays porsine imathes\n",
      "blontuos\n",
      "angenite\n",
      "ione womshini\n",
      "the layay\n",
      "dite\n",
      "ane of chols\n",
      "plois ake herre\n",
      "finat eny\n",
      "dighiscertion\n",
      "act portac\n",
      "the dattye\n",
      "the wagetion\n",
      "yey\n",
      "and ontipispar\n",
      "apelagio \n",
      "\n",
      "\n",
      "Epoch 11/50\n",
      "3660/3660 [==============================] - 1s 333us/step - loss: 1.7445\n",
      "Seed:\n",
      " ne\n",
      "lessons\n",
      "vortex\n",
      "battle lines \n",
      "\n",
      "Generated text:\n",
      " \n",
      "isptirestiveetpkine\n",
      "the envingeviveof, port ofigaty\n",
      "tienintienc enter cesture\n",
      "cosd garce\n",
      "cof wosp te's batce\n",
      "s\n",
      "pattion\n",
      ", partii\n",
      "opfiretive\n",
      "umitivimain:ace, enstuigy\n",
      "theancat methond\n",
      "shegorar\n",
      "the mogs \n",
      "\n",
      "\n",
      "Epoch 12/50\n",
      "3660/3660 [==============================] - 1s 316us/step - loss: 1.6028\n",
      "Seed:\n",
      " logium\n",
      "non sequitur\n",
      "the way of \n",
      "\n",
      "Generated text:\n",
      "  twion\n",
      "tyingerimecros\n",
      "arg of ong\n",
      "spe onsil's qutlrighe qua condus sole ana\n",
      "inne unacificititinn wiocpurkige serreat her pirtt beristinc\n",
      "shong orich legedcono wever plition int anter\n",
      "thin8esin\n",
      "the ney\n",
      " \n",
      "\n",
      "\n",
      "Epoch 13/50\n",
      "3660/3660 [==============================] - 1s 316us/step - loss: 1.4727\n",
      "Seed:\n",
      "  lines\n",
      "favor the bold (part 1) \n",
      "\n",
      "Generated text:\n",
      " \n",
      "strimnnt of antigrees\n",
      "cape mokhan\n",
      "thele ho nd onoume\n",
      "\n",
      "warksini far the porkrvind ave couspity sar of the cinde barke\n",
      "yougreve, part i)\n",
      "helce, part w)\n",
      "the diedss ca?tace, part two\n",
      "the quath on cailonr \n",
      "\n",
      "\n",
      "Epoch 14/50\n",
      "3660/3660 [==============================] - 1s 339us/step - loss: 1.2969\n",
      "Seed:\n",
      " cal probabilities\n",
      "concerning f \n",
      "\n",
      "Generated text:\n",
      " art ine\n",
      "olagosrugron\n",
      "shatt s act one\n",
      "shimes varc wove ofil inqunci, pars ice enit twoon of cofld\n",
      "shincratr\n",
      "the voyegexin\n",
      "the reencis in mura's of kigagor\n",
      "comerate\n",
      "the vearr\n",
      "wimen verang\n",
      "qulsiok\n",
      "pnosua \n",
      "\n",
      "\n",
      "Epoch 15/50\n",
      "3660/3660 [==============================] - 1s 332us/step - loss: 1.2263\n",
      "Seed:\n",
      " ers\n",
      "the savage curtain\n",
      "all our \n",
      "\n",
      "Generated text:\n",
      "  dorkon\n",
      "the pealht: part le\n",
      "blobf athe\n",
      "plostoca\n",
      "dovedorm\n",
      "gravulrod, part i\n",
      "flosh onf\n",
      "the jhion\n",
      "memurrery\n",
      "riten\n",
      "monhe\n",
      "conase\n",
      "indourmiti\n",
      "thancentres\n",
      "str's proxiginis\n",
      "ake\n",
      "ximine th westarce, part w\n",
      "woghi \n",
      "\n",
      "\n",
      "Epoch 16/50\n",
      "3660/3660 [==============================] - 1s 329us/step - loss: 1.1228\n",
      "Seed:\n",
      " teller\n",
      "frame of mind\n",
      "progress\n",
      " \n",
      "\n",
      "Generated text:\n",
      " ion\n",
      "outt of the battli wome\n",
      "the uleredrove\n",
      "attercthe doat i\n",
      "\n",
      "fat oll sittirni\n",
      "equghtion\n",
      "uthill ence un the flospsove\n",
      "wolplight seace\n",
      "sh ana bestivi\n",
      "anc tatlaca\n",
      "loof impursis: part i\n",
      "nemininipe\n",
      "concs,  \n",
      "\n",
      "\n",
      "Epoch 17/50\n",
      "3660/3660 [==============================] - 1s 311us/step - loss: 1.0152\n",
      "Seed:\n",
      " ul\n",
      "nightingale\n",
      "flesh and blood \n",
      "\n",
      "Generated text:\n",
      " , part i\n",
      "furss cas harr\n",
      "the wreemigbad\n",
      "angevis heriluse\n",
      "convey\n",
      "invigar\n",
      "the neyadsian\n",
      "the coushis\n",
      "agigaine\n",
      "honing\n",
      "the coussiey\n",
      "re work\n",
      "mwgatica of enter am part oyount\n",
      "twose vorssparcs, part owa lat on \n",
      "\n",
      "\n",
      "Epoch 18/50\n",
      "3660/3660 [==============================] - 1s 348us/step - loss: 0.9026\n",
      "Seed:\n",
      " l good things...\n",
      "all good thin \n",
      "\n",
      "Generated text:\n",
      " \n",
      "the megety\n",
      "shepcora passiver\n",
      "forrithe iont tathorn\n",
      "the bedony\n",
      "the eeconge uenterco fear\n",
      "the elpinqurre\n",
      "counge jeyory of twold\n",
      "jepkrtion\n",
      "chblds ave seyare of tacoospercer\n",
      "the copont on encencepport i\n",
      " \n",
      "\n",
      "\n",
      "Epoch 19/50\n",
      "3660/3660 [==============================] - 1s 334us/step - loss: 0.7946\n",
      "Seed:\n",
      " h\n",
      "journey's end\n",
      "the maquis, pa \n",
      "\n",
      "Generated text:\n",
      " rt i\n",
      "forstace, part i\n",
      "forks and exinnate\n",
      "shadory mithe retround\n",
      "shast xatciniidind\n",
      "life day\n",
      "ming cars ond shince, part ii\n",
      "prrfight of fition\n",
      "aconaximaninnithe broon\n",
      "inquusori\n",
      "digyoman\n",
      "coddenturide\n",
      "cop \n",
      "\n",
      "\n",
      "Epoch 20/50\n",
      "3660/3660 [==============================] - 1s 348us/step - loss: 0.7294\n",
      "Seed:\n",
      " , more troubles\n",
      "the survivor\n",
      "t \n",
      "\n",
      "Generated text:\n",
      " heand ove shid and ont oe ficry\n",
      "the parkn\n",
      "thunt\n",
      "the wrothe bantion\n",
      "\n",
      "theicaalion\n",
      "cottorminit e\n",
      "ne loss\n",
      "the eryin mall soumond shad\n",
      "shunt of the aldeg mate ofis the payase\n",
      "sharce badke\n",
      "the starris\n",
      "the g \n",
      "\n",
      "\n",
      "Epoch 21/50\n",
      "3660/3660 [==============================] - 1s 342us/step - loss: 0.6136\n",
      "Seed:\n",
      " ir, i presume?\n",
      "rise\n",
      "favorite s \n",
      "\n",
      "Generated text:\n",
      " traat\n",
      "slay\n",
      "on the matcer the way\n",
      "scarksissaice\n",
      "imatevey of fitien\n",
      "the stlan\n",
      "iheas\n",
      "camagerce, part i\n",
      "fnace, part i\n",
      "lans, part i\n",
      "fors in the ald men\n",
      "cound\n",
      "s ur warcs\n",
      "the blign\n",
      "tense the voill\n",
      "sance manc \n",
      "\n",
      "\n",
      "Epoch 22/50\n",
      "3660/3660 [==============================] - 1s 337us/step - loss: 0.5585\n",
      "Seed:\n",
      " ce of evil\n",
      "someone to watch ov \n",
      "\n",
      "Generated text:\n",
      " e\n",
      "the foit of ent\n",
      "the leyality of ant hal wixpmancs\n",
      "shopsiar\n",
      "\n",
      "the teall licnis\n",
      "the umif ficition\n",
      "altion\n",
      "ant taitivace, port i\n",
      "whosss\n",
      "the wayuna of hitcor\n",
      "baltaly\n",
      "idseviniss\n",
      "arigimanger\n",
      "furity\n",
      "vilsinvi \n",
      "\n",
      "\n",
      "Epoch 23/50\n",
      "3660/3660 [==============================] - 1s 331us/step - loss: 0.4878\n",
      "Seed:\n",
      " armageddon game\n",
      "sub rosa\n",
      "whisp \n",
      "\n",
      "Generated text:\n",
      " irmay\n",
      "the chily\n",
      "angey in theollive\n",
      "govel and herr prepucligy\n",
      "jougeatu\n",
      "y\n",
      "déanter\n",
      "the ese\n",
      "ancer digy\n",
      "grond\n",
      "shepvighos\n",
      "actinesu\n",
      "sw\n",
      "coulsis\n",
      "triduse\n",
      "the ond\n",
      "necoumol of the daytnc, part i\n",
      "phobs strrel shin \n",
      "\n",
      "\n",
      "Epoch 24/50\n",
      "3660/3660 [==============================] - 1s 320us/step - loss: 0.4150\n",
      "Seed:\n",
      " gerie, part i\n",
      "the menagerie, p \n",
      "\n",
      "Generated text:\n",
      " art one\n",
      "inat esion\n",
      "coustercor\n",
      "sivingeriviritian\n",
      "terespicter\n",
      "the bepter\n",
      "the quead\n",
      "home\n",
      "the kegall sont the wreshing\n",
      "brofst flitht res\n",
      "plasitinn\n",
      "the thelichont\n",
      "hishiscance\n",
      "the payas\n",
      "thesqurriter\n",
      "the pli \n",
      "\n",
      "\n",
      "Epoch 25/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3660/3660 [==============================] - 1s 333us/step - loss: 0.3644\n",
      "Seed:\n",
      "  measure of a man\n",
      "the dauphin\n",
      " \n",
      "\n",
      "Generated text:\n",
      " comonasour the pieat tears ffokepe\n",
      "shasors dest of bloon\n",
      "the eatct of fercont\n",
      "gatent fattion\n",
      "eyighadd\n",
      "jed\n",
      "chal coun the e port in\n",
      "the behory\n",
      "jetcent\n",
      "the aedin timeld\n",
      "s ancer\n",
      "menano\n",
      "s att orive possily \n",
      "\n",
      "\n",
      "Epoch 26/50\n",
      "3660/3660 [==============================] - 1s 334us/step - loss: 0.3369\n",
      "Seed:\n",
      " ers\n",
      "far beyond the stars\n",
      "prey\n",
      " \n",
      "\n",
      "Generated text:\n",
      " day\n",
      "day\n",
      "pontes, deat of entey\n",
      "the deantterdod\n",
      "the chaak\n",
      "slof ont umat\n",
      "faion\n",
      "the the peath\n",
      "in the deall sy part 1)\n",
      "he ombed min\n",
      "the eattos art twion\n",
      "altenneyues\n",
      "the the dearct of ange\n",
      "the nigy al apoun \n",
      "\n",
      "\n",
      "Epoch 27/50\n",
      "3660/3660 [==============================] - 1s 325us/step - loss: 0.2880\n",
      "Seed:\n",
      " he circle (part 2)\n",
      "interface\n",
      "t \n",
      "\n",
      "Generated text:\n",
      " he bepmegiend\n",
      "foratution\n",
      "art onit oller\n",
      "the conter\n",
      "the alington\n",
      "dhengs\n",
      "af in the coundis\n",
      "the kiglond on attinathen\n",
      "chothellly of angegre paithe, part i\n",
      "plobod hors\n",
      "gerres end\n",
      "entencortinniline\n",
      "howf of \n",
      "\n",
      "\n",
      "Epoch 28/50\n",
      "3660/3660 [==============================] - 1s 338us/step - loss: 0.2799\n",
      "Seed:\n",
      "  part ii\n",
      "resurrection\n",
      "random t \n",
      "\n",
      "Generated text:\n",
      " heuxly of nomogh ploin ttrement\n",
      "the couse\n",
      "the porss\n",
      "bluind fart i\n",
      "flowode of the blead\n",
      "shrate\n",
      "the nownger port fatuen\n",
      "thes leall seups if the bliglis\n",
      "strathe sourra iof matuer\n",
      "the jepory\n",
      "viling\n",
      "duarr  \n",
      "\n",
      "\n",
      "Epoch 29/50\n",
      "3660/3660 [==============================] - 1s 349us/step - loss: 0.2365\n",
      "Seed:\n",
      " oath\n",
      "journey's end\n",
      "the maquis, \n",
      "\n",
      "Generated text:\n",
      "  deak\n",
      "herrnd, ok babll (pprre factor\n",
      "the battlor\n",
      "domen\n",
      "woull ne ond seact, part i\n",
      "fors of de\n",
      "connte corade\n",
      "barken in the daeme\n",
      "the moga of the deatct of and ness\n",
      "blugit of ante\n",
      "ithe minater\n",
      "formikimes \n",
      "\n",
      "\n",
      "Epoch 30/50\n",
      "3660/3660 [==============================] - 1s 329us/step - loss: 0.2237\n",
      "Seed:\n",
      " e world is hollow and i have t \n",
      "\n",
      "Generated text:\n",
      " eatsiny\n",
      "viratoon\n",
      "jetegstercormal coss ar datriviove and erre befory\n",
      "dodking of anges ss af wirror, dorntyi\n",
      "plificratuellice\n",
      "senhinger for wiyesion\n",
      "rathentigy\n",
      "art tenestif ont hest (part 2)\n",
      "deare of en \n",
      "\n",
      "\n",
      "Epoch 31/50\n",
      "3660/3660 [==============================] - 1s 348us/step - loss: 0.2173\n",
      "Seed:\n",
      " \n",
      "rocks and shoals\n",
      "the raven\n",
      "so \n",
      "\n",
      "Generated text:\n",
      " rr and virite mogd sililion\n",
      "ahterathe voure reak\n",
      "ghe clature of the pars ter\n",
      "tecse\n",
      "murrory mingerus ictarcoid ant two park troulld\n",
      "worrssistall of honos\n",
      "stliviover\n",
      "arblongsy\n",
      "al carkprise\n",
      "cart fact oll \n",
      "\n",
      "\n",
      "Epoch 32/50\n",
      "3660/3660 [==============================] - 1s 362us/step - loss: 0.1845\n",
      "Seed:\n",
      " m\n",
      "rapture\n",
      "the darkness and the \n",
      "\n",
      "Generated text:\n",
      "  wey of himat\n",
      "ey\n",
      "erithe corblat turspieving\n",
      "fecpastive\n",
      "stition\n",
      "attereminn\n",
      "pertigno\n",
      "daathisces\n",
      "the nemegion childisn\n",
      "the antexim the pampe\n",
      "minge\n",
      "countarl leat fertion\n",
      "ervingeevi\n",
      "viramonn of entty\n",
      "ey\n",
      "en \n",
      "\n",
      "\n",
      "Epoch 33/50\n",
      "3660/3660 [==============================] - 1s 354us/step - loss: 0.2080\n",
      "Seed:\n",
      " ga glory\n",
      "the ultimate computer \n",
      "\n",
      "Generated text:\n",
      " y\n",
      "yyurre of the matcorcs\n",
      "eccapligro never\n",
      "the veat tims\n",
      "ariverarkignmenterove ardsive arren if art onion\n",
      "duiter\n",
      "the eremega cove romblsing\n",
      "the erementrod\n",
      "art tattore thint le once umas art (part 1)\n",
      "ur \n",
      "\n",
      "\n",
      "Epoch 34/50\n",
      "3660/3660 [==============================] - 1s 346us/step - loss: 0.1789\n",
      "Seed:\n",
      " \n",
      "first contact\n",
      "the ascent\n",
      "the  \n",
      "\n",
      "Generated text:\n",
      " nogh in the dueac\n",
      "plepond the find\n",
      "shencorite vente\n",
      "coussivio\n",
      "the hongs\n",
      "the forters of ange\n",
      "the jodgature of fithor\n",
      "seraplition\n",
      "ochincand s art of gincalb\n",
      "y\n",
      "plajodend\n",
      "yeuntrry\n",
      "blied\n",
      "shinter of enter-\n",
      " \n",
      "\n",
      "\n",
      "Epoch 35/50\n",
      "3660/3660 [==============================] - 1s 323us/step - loss: 0.1569\n",
      "Seed:\n",
      " alileo seven\n",
      "the squire of got \n",
      "\n",
      "Generated text:\n",
      " h wross\n",
      "warctince, part i\n",
      "hersast\n",
      "malion\n",
      "choun couster\n",
      "the searcs\n",
      "the ald nn chend\n",
      "the shatct\n",
      "the the blatle\n",
      "the paranishostoreving hirtar\n",
      "the wogal 2f ant seler\n",
      "the ver\n",
      "the the parshine\n",
      "the wayidar\n",
      "t \n",
      "\n",
      "\n",
      "Epoch 36/50\n",
      "3660/3660 [==============================] - 1s 315us/step - loss: 0.1692\n",
      "Seed:\n",
      " arch, part ii\n",
      "the house of qua \n",
      "\n",
      "Generated text:\n",
      " ly\n",
      "shopn wllond\n",
      "stiveosparm, pors iw\n",
      "the behon\n",
      "ploon welloshid\n",
      "man of hatro\n",
      "shonge\n",
      "the ond lifersteve man\n",
      "engegicaradorn\n",
      "it art oat oni\n",
      "hove sold werror\n",
      "the baht ofe\n",
      "innt umithe sond shad\n",
      "lerkes in th \n",
      "\n",
      "\n",
      "Epoch 37/50\n",
      "3660/3660 [==============================] - 1s 317us/step - loss: 0.1516\n",
      "Seed:\n",
      " \n",
      "future imperfect\n",
      "final missio \n",
      "\n",
      "Generated text:\n",
      " n\n",
      "comege of the ghold\n",
      "homessard\n",
      "yemunar\n",
      "merselscs\n",
      "the fullichor\n",
      "the phime\n",
      "thescs do monos\n",
      "remond shad\n",
      "shie of the daying of ance\n",
      "thes and starco\n",
      "shen wif the rugst freyion\n",
      "chthigh\n",
      "scorider\n",
      "ergegded fe \n",
      "\n",
      "\n",
      "Epoch 38/50\n",
      "3660/3660 [==============================] - 1s 353us/step - loss: 1.0541\n",
      "Seed:\n",
      "  one\n",
      "invasive procedures\n",
      "gambi \n",
      "\n",
      "Generated text:\n",
      " s ten?\n",
      "éov?\n",
      "io man\n",
      "honoey and?\n",
      "the éigd?él?\n",
      "rong\n",
      "the coublockin c??e'?\n",
      "scadeat\n",
      "caresine of-?ath?\n",
      "sq?arumeghes of f?jlg, darke, part i\n",
      "llosond shad\n",
      "the astater\n",
      "comzety\n",
      "the eyemonce usif caqll shilir's  \n",
      "\n",
      "\n",
      "Epoch 39/50\n",
      "3660/3660 [==============================] - 1s 338us/step - loss: 1.1202\n",
      "Seed:\n",
      " t\n",
      "the slaver weapon\n",
      "the eye of \n",
      "\n",
      "Generated text:\n",
      "  forct, part i\n",
      "lofs ana bafkithe cone\n",
      "the anterre palas\n",
      "thes\n",
      "the enemy\n",
      "diresive sondr magher the beuter-\n",
      "frestive\n",
      "hhoundes the queat fal panse\n",
      "busaior\n",
      "yout irtors\n",
      "day\n",
      "an enamer\n",
      "ey\n",
      "the vis ontcy\n",
      "the ve \n",
      "\n",
      "\n",
      "Epoch 40/50\n",
      "3660/3660 [==============================] - 1s 353us/step - loss: 0.1258\n",
      "Seed:\n",
      " he bonding\n",
      "booby trap\n",
      "the enem \n",
      "\n",
      "Generated text:\n",
      " one\n",
      "sond sy prith\n",
      "w\n",
      "yearcs, part i\n",
      "flosond shid\n",
      "the daechile wome\n",
      "sonds ul tike\n",
      "the battl\n",
      "the daurnt of andos\n",
      "the and niness\n",
      "arore\n",
      "the magntion\n",
      "\n",
      "the bahtlor\n",
      "pant ome\n",
      "armerner ofrw\n",
      "the balll ouplork ta \n",
      "\n",
      "\n",
      "Epoch 41/50\n",
      "3660/3660 [==============================] - 1s 329us/step - loss: 0.1334\n",
      "Seed:\n",
      " offspring\n",
      "sins of the father\n",
      "a \n",
      "\n",
      "Generated text:\n",
      " lequeatro\n",
      "heast of gind se\n",
      "porttine\n",
      "shidond\n",
      "sond lear\n",
      "monk the peear\n",
      "the oukrs\n",
      "the shild\n",
      "ant encence, part ii\n",
      "the of the bahor\n",
      "shok ance\n",
      "the and learkit the endend\n",
      "the ximd\n",
      "the virss\n",
      "cove sourn of the \n",
      "\n",
      "\n",
      "Epoch 42/50\n",
      "3660/3660 [==============================] - 1s 333us/step - loss: 0.1325\n",
      "Seed:\n",
      " sing\n",
      "the lorelei signal\n",
      "more t \n",
      "\n",
      "Generated text:\n",
      " he flithe contac-\n",
      "formind\n",
      "mence, part i\n",
      "ploss dlore\n",
      "umppoch barkstince bent alleatery\n",
      "hosiey\n",
      "antencr,seatcove\n",
      "athe manterus\n",
      "the matkines\n",
      "kedbent of latro\n",
      "the nearcs\n",
      "for al of the parthi\n",
      "the magkst ffe \n",
      "\n",
      "\n",
      "Epoch 43/50\n",
      "3660/3660 [==============================] - 1s 333us/step - loss: 0.1280\n",
      "Seed:\n",
      " ce\n",
      "riddles\n",
      "dragon's teeth\n",
      "one  \n",
      "\n",
      "Generated text:\n",
      " sold ant twe worss\n",
      "fachire of the matcers\n",
      "probes\n",
      "luge om homon\n",
      "woke wold wine\n",
      "the eatcs of man\n",
      "the warths feching\n",
      "the of the fatterse\n",
      "coldea of kigho\n",
      "the bahils\n",
      "dof and searcs\n",
      "peracors\n",
      "pustence, part  \n",
      "\n",
      "\n",
      "Epoch 44/50\n",
      "3660/3660 [==============================] - 1s 325us/step - loss: 0.1300\n",
      "Seed:\n",
      " \n",
      "a matter of perspective\n",
      "yeste \n",
      "\n",
      "Generated text:\n",
      " 'sserficeta\n",
      "counde corkstifecion\n",
      "\n",
      "heantion\n",
      "art aronishide\n",
      "vindes\n",
      "the finth s shrace\n",
      "tementer\n",
      "freye\n",
      "indave parthes\n",
      "shoss sulal te sond squrres\n",
      "perallise\n",
      "coundature\n",
      "the of the parthis\n",
      "the kpald\n",
      "shre of  \n",
      "\n",
      "\n",
      "Epoch 45/50\n",
      "3660/3660 [==============================] - 1s 342us/step - loss: 0.1290\n",
      "Seed:\n",
      " ive\n",
      "eye of the needle\n",
      "visionar \n",
      "\n",
      "Generated text:\n",
      " \n",
      "the eate, part i\n",
      "phojs, part i\n",
      "fhoss in eall of anceg (part 1)\n",
      "herouminn\n",
      "honestard\n",
      "ind umings\n",
      "ewoithe maty\n",
      "al of fetcentey\n",
      "ax part vio\n",
      "froving\n",
      "all gime sontay\n",
      "ange caithincs, part i\n",
      "whoss ant two\n",
      "the \n",
      "\n",
      "\n",
      "Epoch 46/50\n",
      "3660/3660 [==============================] - 1s 317us/step - loss: 0.1280\n",
      "Seed:\n",
      " magicks of megas-tu\n",
      "once upon  \n",
      "\n",
      "Generated text:\n",
      " the rey\n",
      "enternticyourrd\n",
      "the meyald\n",
      "hove arquuen\n",
      "the way\n",
      "doghes cosspine\n",
      "tayesinite fertion\n",
      "cait if kinityes\n",
      "er\n",
      "the ghall shod\n",
      "the chity\n",
      "al parvis\n",
      "the of th- fil tife\n",
      "shattentiendes\n",
      "the pignt of and\n",
      "th \n",
      "\n",
      "\n",
      "Epoch 47/50\n",
      "3660/3660 [==============================] - 1s 322us/step - loss: 0.1178\n",
      "Seed:\n",
      " ant\n",
      "fascination\n",
      "past tense, pa \n",
      "\n",
      "Generated text:\n",
      " rtti\n",
      "thes\n",
      "the veals\n",
      "blookwall fincause\n",
      "tue mond squde, part i\n",
      "whossos derre dear\n",
      "the daurhty of fichige\n",
      "chilusal of hetrove\n",
      "ittarc the equgh tarel gime and the bahtll\n",
      "the mall on endeg frivinges\n",
      "the v \n",
      "\n",
      "\n",
      "Epoch 48/50\n",
      "3660/3660 [==============================] - 1s 331us/step - loss: 0.1169\n",
      "Seed:\n",
      "  dauphin\n",
      "contagion\n",
      "the royale\n",
      " \n",
      "\n",
      "Generated text:\n",
      " the stadd\n",
      "shid\n",
      "arm the verre of the arcter\n",
      "the of latlore\n",
      "emppork herrst fecit\n",
      "tres, add te seat of encent\n",
      "the thead\n",
      "homeoty distrry\n",
      "defica\n",
      "dous of the arc inter\n",
      "chen\n",
      "may\n",
      "ag prryitinn\n",
      "ic the gaty\n",
      "the  \n",
      "\n",
      "\n",
      "Epoch 49/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3660/3660 [==============================] - 1s 315us/step - loss: 0.1229\n",
      "Seed:\n",
      "  business\n",
      "learning curve\n",
      "shaka \n",
      "\n",
      "Generated text:\n",
      " al\n",
      "coun sorrspiemin\n",
      "the veatt oe omissaress\n",
      "foishise\n",
      "baugess\n",
      "fuer\n",
      "the scedcris\n",
      "the fait s ast of ghod\n",
      "arters ive mant one\n",
      "the veospora\n",
      "buty and gataion\n",
      "erignmal conescord\n",
      "ince barkey shiscor\n",
      "the appar \n",
      "\n",
      "\n",
      "Epoch 50/50\n",
      "3660/3660 [==============================] - 1s 329us/step - loss: 0.1166\n",
      "Seed:\n",
      " error\n",
      "q2\n",
      "author, author\n",
      "friend \n",
      "\n",
      "Generated text:\n",
      "  s anges os ac markidey\n",
      "ey\n",
      "albrrim chils\n",
      "johonar\n",
      "the pleagy\n",
      "secentere of the plish\n",
      "bloon ince bossian\n",
      "the wayinnes\n",
      "\n",
      "herrature of herospicetred\n",
      "an corrstif of the parse\n",
      "blefers\n",
      "crops (part 1)\n",
      "herf for  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = model_train.fit(X, y, batch_size=128, epochs=50, callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model if necessary\n",
    "model_train.save(\"keras-startrek-LSTM-model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting training and validation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1212fcf98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'b-', label='Training loss')\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a Decoder model\n",
    "\n",
    "During training, we presented sequences of 30 characters, along with the correct next character.\n",
    "When_using the trained model, it may be more useful to feed in 1 character at a time, and seeing the next\n",
    "predicted one. That will also convince us that the network is actually _using_ its internal state.\n",
    "\n",
    "- Needs input length of 1.\n",
    "- Needs batch size of 1\n",
    "- Needs LSTM to be stateful\n",
    "- check that params is the same as model_train\n",
    "\n",
    "<img src=\"figures/1-in-1-out.png\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model if necessary.\n",
    "# model_train = load_model(\"keras-startrek-LSTM-model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (1, 128)                  91136     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (1, 49)                   6321      \n",
      "=================================================================\n",
      "Total params: 97,457\n",
      "Trainable params: 97,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build a decoding model (input length 1, batch size 1, stateful)\n",
    "layer_size = 128\n",
    "\n",
    "model_dec = Sequential()\n",
    "# 1 letter in, 1 letter out.\n",
    "# Stateful=True keeps the state from the end of one batch to the start of the next\n",
    "# In other words, the network \"remembers\" its state from one input to the next. This is essential when\n",
    "# the network looks at 1 input at a time.\n",
    "model_dec.add(LSTM(layer_size, stateful=True, batch_input_shape=(1,1,len(chars))))\n",
    "\n",
    "# project back to vocabulary\n",
    "model_dec.add(Dense(vocabulary_size, activation='softmax'))\n",
    "model_dec.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.01))\n",
    "model_dec.summary()\n",
    "\n",
    "# set weights from training model\n",
    "# Note that we can reuse these weights, since the sizes of the trained and decoder network are the same.\n",
    "# The trained network took in 30 characters, but remember that all these 30 used the same input weights.\n",
    "# That is one of the advantages of RNNs: They are independent of sequence lengths.\n",
    "model_dec.set_weights(model_train.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Model\n",
    "\n",
    "- Take a quote then add 400 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " bleatter  hontcyes\n",
      "ihanchonich\n",
      "su\n",
      "ecpait (pario\n",
      "wemenseove pars of ancitiom\n",
      "ermind starce\n",
      "formtigm\n",
      "menseon\n",
      "chothe flinke\n",
      "iquses frofe\n",
      "thehelime\n",
      "partyisticadian umengorss\n",
      "blomhiswo\n",
      "the worpl gemal ry of the kines\n",
      ".fuer\n",
      "the jlheposhes work plish\n",
      "blobls\n",
      "callors\n",
      "endaghepceves\n",
      "il fort sandss ligey\n",
      "mhen\n",
      "en\n",
      "pleyalbll shinm fere\n",
      "gusols ofsive whors\n",
      "damegs. if omo\n",
      "sourress\n",
      "esu\n",
      "iminateoustore of gighs of wiming\n",
      "mpar of gemesis\n",
      "comelrsinm\n",
      "caresing of eng\n",
      "gors if ofing\n",
      "the artanceyes\n",
      "cove reforss\n",
      "dearcexigatay\n",
      "enfever\n",
      "the cloddatuges of shigren\n",
      "cogherus chil tenpey\n",
      "ex part ww\n",
      "(part one\n",
      "shodf ritlochord\n",
      "pactior\n",
      "desneceloke\n",
      "vol\n",
      "countify\n",
      "ive\n",
      "icue aronf, part i\n",
      "etres of bathok\n",
      "meionispicmad\n",
      "the ligh ont herrrstorm\n",
      "ighantigame's mins\n",
      "des\n",
      "thee shatlave\n",
      "in t umen ence bafity\n",
      "sedegaad\n",
      "she qfhaw\n",
      "w:ople rerkect f ce\n",
      "shiderferqe\n",
      "uwif caxl anfen pirstion\n",
      "\n",
      "fuerino of and thers\n",
      "seugry\n",
      "fiort (partios\n",
      "drrendirece, part ii\n",
      "the recngs oft\n",
      "the daecty\n",
      "shime affrin\n",
      "the parse\n",
      "bamogs\n",
      "dife\n",
      "falipntyy of fachtlow\n",
      "plith art\n"
     ]
    }
   ],
   "source": [
    "# Sample 1000 characters from the decoding model using a random seed from the vocabulary.\n",
    "generated = generate_text_segment(1000, diversity=2.0, generating_model = model_dec, input_sequence_length = 1)\n",
    "sys.stdout.write(generated)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {
    "68a0903c8cb44d7f844576a9e191a074": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "73196c60cb1f4fa1a36a9200fb5ec526": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "75ed5e35093c4569af70d449c8d599dd": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "aa7bb86c093a47a48ddd69380fe4d1b1": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
