{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixture Density Network\n",
    "\n",
    "Reproducing the classic Bishop MDN network tasks in Keras. The idea in this task is to predict a the value of an inverse sine function. This function has multiple real-valued solutions at each point, so the ANN model needs to have the capacity to handle this in it's loss function. An MDN is a good way to handle the predictions of these multiple output values.\n",
    "\n",
    "There's a couple of other versions of this task, and this implementation owes much to the following:\n",
    "\n",
    "- [David Ha - Mixture Density Networks with TensorFlow](http://blog.otoro.net/2015/11/24/mixture-density-networks-with-tensorflow/)\n",
    "- [Mixture Density Networks in Edward](http://edwardlib.org/tutorials/mixture-density-network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal imports for everybody\n",
    "import keras\n",
    "from context import * # imports the MDN layer \n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Data\n",
    "\n",
    "- Let's generate some data that is a bit difficult to predict!\n",
    "- The task will be to predict the y-axis data based on the x-axis as input.\n",
    "- We'll generate some data so that there are _multiple_ *y* options for some values of *x*!\n",
    "- This is a classic MDN toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generating some data:\n",
    "NSAMPLE = 3000\n",
    "\n",
    "y_data = np.float32(np.random.uniform(-10.5, 10.5, NSAMPLE))\n",
    "r_data = np.random.normal(size=NSAMPLE)\n",
    "x_data = np.sin(0.75 * y_data) * 7.0 + y_data * 0.5 + r_data * 1.0\n",
    "x_data = x_data.reshape((NSAMPLE, 1))\n",
    "\n",
    "#plt.figure(figsize=(8, 8))\n",
    "plt.plot(x_data,y_data,'ro', alpha=0.3)\n",
    "#plt.savefig(\"arcsine-function.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to fit with MSE:\n",
    "\n",
    "First let's try to fit with a typical feedforward neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_HIDDEN = 15\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(N_HIDDEN, batch_input_shape=(None, 1), activation='tanh'))\n",
    "model.add(keras.layers.Dense(N_HIDDEN, activation='tanh'))\n",
    "model.add(keras.layers.Dense(1, activation='linear'))\n",
    "model.compile(loss='mse', optimizer='rmsprop')\n",
    "model.summary()\n",
    "history = model.fit(x=x_data, y=y_data, batch_size=128, epochs=200, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(10, 5))\n",
    "#plt.ylim([0,9])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.savefig(\"feedforward-mse-prediction-loss-plot.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample on some test data:\n",
    "x_test = np.float32(np.arange(-15,15,0.01))\n",
    "NTEST = x_test.size\n",
    "print(\"Testing:\", NTEST, \"samples.\")\n",
    "\n",
    "# x_test is a big array of possible x_values to predict!\n",
    "x_test = x_test.reshape(NTEST,1) # needs to be a matrix, not a vector\n",
    "\n",
    "# Make predictions from the model - we can predict everything at once.\n",
    "#y_test = model.predict(x_data)\n",
    "y_test = model.predict(x_test)\n",
    "\n",
    "# Plot the samples\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(x_data,y_data,'ro', x_test, y_test, 'bo',alpha=0.3)\n",
    "#plt.plot(x_data,y_data,'ro', x_data, y_test, 'bo',alpha=0.3)\n",
    "plt.savefig(\"arcsine-feedforward-mse-prediction.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the MDN Model\n",
    "\n",
    "Now we will construct the MDN model in Keras. This uses the `Sequential` model interface in Keras.\n",
    "\n",
    "The `MDN` layer comes after one or more `Dense` layers. You need to define the output dimension and number of mixtures for the MDN like so: `MDN(output_dimension, number_mixtures)`.\n",
    "\n",
    "For this problem, we only need an output dimension of 1 as we are predicting one value (y). Adding more mixtures adds a more parameters (model is more complex, takes longer to train), but might help make the solutions better. You can see from the training data that there are at maximum 5 different layers to predict in the curve, so setting `N_MIXES = 5` is a good place to start.\n",
    "\n",
    "For MDNs, we have to use a special loss function that can handle the mixture parameters: the function has to take into account the number of output dimensions and mixtures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_HIDDEN = 15\n",
    "N_MIXES = 5\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(N_HIDDEN, batch_input_shape=(None, 1), activation='relu'))\n",
    "model.add(keras.layers.Dense(N_HIDDEN, activation='relu'))\n",
    "model.add(mdn.MDN(1, N_MIXES))\n",
    "model.compile(loss=mdn.get_mixture_loss_func(1,N_MIXES), optimizer='rmsprop') #, metrics=[mdn.get_mixture_mse_accuracy(1,N_MIXES)])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "Now we train the model using Keras' normal `fit` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=x_data, y=y_data, batch_size=128, epochs=400, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model if you want to.\n",
    "# model.save(\"MDN-1D-sine-prediction-model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model if you want to.\n",
    "# To load models from file, you need to supply the layer and loss function as custom_objects:\n",
    "# model = keras.models.load_model('MDN-1D-sine-prediction-model.h5', custom_objects={'MDN': mdn.MDN, 'loss_func': mdn.get_mixture_loss_func(1, N_MIXES)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation Loss\n",
    "\n",
    "It's interesting to see how the model trained. We can see that after a certain point training is rather slow.\n",
    "\n",
    "For this problem a loss value around 1.5 produces quite good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(10, 5))\n",
    "#plt.ylim([0,9])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.savefig(\"arcsine-feedforward-mdn-loss.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Functions\n",
    "\n",
    "The MDN model outputs parameters of a mixture model---a list of means (mu), variances (sigma), and weights (pi).\n",
    "\n",
    "The `mdn` module provides a function to sample from these parameters as follows. First the parameters are split up into `mu`s, `sigma`s and `pi`s, then the categorical distribution formed by the `pi`s is sampled to choose which mixture component should be sampled, then that component's `mu`s and `sigma`s is used to sample from a multivariate normal model, here's the code:\n",
    "\n",
    "    def sample_from_output(params, output_dim, num_mixes, temp=1.0):\n",
    "        \"\"\"Sample from an MDN output with temperature adjustment.\"\"\"\n",
    "        mus = params[:num_mixes*output_dim]\n",
    "        sigs = params[num_mixes*output_dim:2*num_mixes*output_dim]\n",
    "        pis = softmax(params[-num_mixes:], t=temp)\n",
    "        m = sample_from_categorical(pis)\n",
    "        # Alternative way to sample from categorical:\n",
    "        # m = np.random.choice(range(len(pis)), p=pis)\n",
    "        mus_vector = mus[m*output_dim:(m+1)*output_dim]\n",
    "        sig_vector = sigs[m*output_dim:(m+1)*output_dim] * temp  # adjust for temperature\n",
    "        cov_matrix = np.identity(output_dim) * sig_vector\n",
    "        sample = np.random.multivariate_normal(mus_vector, cov_matrix, 1)\n",
    "        return sample\n",
    "        \n",
    "If you only have one prediction to sample from, you can use the function as is; but if you need to sample from a lot of predictions at once (as in the following sections), you can use `np.apply_along_axis` to apply it to a whole numpy array of predicted parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out the MDN Model\n",
    "\n",
    "Now we try out the model by making predictions at 3000 evenly spaced points on the x-axis. \n",
    "\n",
    "Mixture models output lists of parameters, so we're going to sample from these parameters for each point on the x-axis, and also try plotting the parameters themselves so we can have some insight into what the model is learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample on some test data:\n",
    "x_test = np.float32(np.arange(-15,15,0.01))\n",
    "NTEST = x_test.size\n",
    "print(\"Testing:\", NTEST, \"samples.\")\n",
    "\n",
    "# x_test is a big array of possible x_values to predict!\n",
    "x_test = x_test.reshape(NTEST,1) # needs to be a matrix, not a vector\n",
    "\n",
    "# Make predictions from the model - we can predict everything at once.\n",
    "y_test = model.predict(x_test)\n",
    "# y_test contains parameters for distributions, not actual points on the graph.\n",
    "# To find points on the graph, we need to sample from each distribution.\n",
    "\n",
    "# Sample from the predicted distributions\n",
    "y_samples = np.apply_along_axis(mdn.sample_from_output, 1, y_test, 1, N_MIXES,temp=1.0)\n",
    "\n",
    "# Split up the mixture parameters (for future fun)\n",
    "mus = np.apply_along_axis((lambda a: a[:N_MIXES]),1, y_test)\n",
    "sigs = np.apply_along_axis((lambda a: a[N_MIXES:2*N_MIXES]),1, y_test)\n",
    "pis = np.apply_along_axis((lambda a: mdn.softmax(a[2*N_MIXES:])),1, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the predictions\n",
    "\n",
    "- Let's plot the predictions\n",
    "- This looks pretty good! The model shows that it can predict reasonably\n",
    "- There's a few small mistakes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the samples\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(x_data,y_data,'ro', x_test, y_samples[:,:,0], 'bo',alpha=0.3)\n",
    "plt.savefig(\"arcsine-feedforward-mdn-predictions.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "# These look pretty good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking inside the model\n",
    "\n",
    "- Let's look inside the model a bit\n",
    "- what are the mean values that are predicted for each value of x?\n",
    "- Some of these means follow the function, and some are a bit useless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the means - this gives us some insight into how the model learns to produce the mixtures.\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(x_data,y_data,'ro', x_test, mus,'bo',alpha=0.3)\n",
    "plt.savefig(\"arcsine-feedforward-mdn-means.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "# Cool!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Looking a bit more closely at means and the mixture weights \n",
    "- we multiply the mean lines by their weight for each x.\n",
    "- This shows how the model uses the mixture elements to construct the function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the variances and weightings of the means as well.\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax1 = fig.add_subplot(111)\n",
    "# ax1.scatter(data[0], data[1], marker='o', c='b', s=data[2], label='the data')\n",
    "ax1.scatter(x_data,y_data,marker='o', c='r', alpha=0.3)\n",
    "for i in range(N_MIXES):\n",
    "    ax1.scatter(x_test, mus[:,i], marker='o', s=200*sigs[:,i]*pis[:,i],alpha=0.3)\n",
    "plt.savefig(\"arcsine-feedforward-mdn-weighted-means.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting inside the MDN layer\n",
    "\n",
    "Let's get inside the MDN layer and see how it REALLY works.\n",
    "\n",
    "Here's a brief implementation with the Keras functional API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Dense, Input, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.engine.topology import Layer\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elu_plus_one_plus_epsilon(x):\n",
    "    \"\"\"ELU activation with a very small addition to help prevent NaN in loss.\"\"\"\n",
    "    return (K.elu(x) + 1 + 1e-8)\n",
    "\n",
    "N_HIDDEN = 15\n",
    "N_MIXES = 5\n",
    "\n",
    "inputs = Input(shape=(1,), name='inputs')\n",
    "hidden1 = Dense(N_HIDDEN, activation='relu', name='hidden1')(inputs)\n",
    "hidden2 = Dense(N_HIDDEN, activation='relu', name='hidden2')(hidden1)\n",
    "\n",
    "mdn_mus = Dense(N_MIXES, name='mdn_mus')(hidden2)\n",
    "mdn_sigmas = Dense(N_MIXES, activation=elu_plus_one_plus_epsilon, name='mdn_sigmas')(hidden2)\n",
    "mdn_pi = Dense(N_MIXES, name='mdn_pi')(hidden2)\n",
    "\n",
    "mdn_out = Concatenate(name='mdn_outputs')([mdn_mus, mdn_sigmas, mdn_pi])\n",
    "\n",
    "model = Model(inputs=inputs, outputs=mdn_out)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdn_loss(y_true, y_pred):\n",
    "    # Split the inputs into paramaters\n",
    "    out_mu, out_sigma, out_pi = tf.split(y_pred, num_or_size_splits=[N_MIXES, N_MIXES, N_MIXES],\n",
    "                                         axis=-1, name='mdn_coef_split')\n",
    "    mus = tf.split(out_mu, num_or_size_splits=N_MIXES, axis=1)\n",
    "    sigs = tf.split(out_sigma, num_or_size_splits=N_MIXES, axis=1)\n",
    "    # Construct the mixture models\n",
    "    cat = tfd.Categorical(logits=out_pi)\n",
    "    coll = [tfd.MultivariateNormalDiag(loc=loc, scale_diag=scale) for loc, scale\n",
    "            in zip(mus, sigs)]\n",
    "    mixture = tfd.Mixture(cat=cat, components=coll)\n",
    "    # Calculate the loss function\n",
    "    loss = mixture.log_prob(y_true)\n",
    "    loss = tf.negative(loss)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss\n",
    "\n",
    "model.compile(loss=mdn_loss, optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=x_data, y=y_data, batch_size=128, epochs=400, validation_split=0.15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
