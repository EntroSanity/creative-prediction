---
layout: reveal-larger
title: Creative Prediction Projects
permalink: /presentations/creprepro/
---

{% include slides/title.html %}

<section data-markdown>
<textarea data-template>
## Supervisors

![]({{site.baseurl}}/assets/people/charlesmartin.jpg) <!-- .element: width="30%"-->
![]({{site.baseurl}}/assets/people/benediktewallace.jpg) <!-- .element: width="30%"-->

**Charles Martin**. Lecturer in Computer Science, Australian
National University. charles.martin@anu.edu.au

**Benedikte Wallace**. PhD Researcher, University of Oslo.
benediwa@ifi.uio.no
</textarea>
</section>

<section data-markdown>
<textarea data-template>
## Creative Predictions

![]({{site.baseurl}}/assets/creative-prediction-image.png) <!-- .element: width="100%"-->

</textarea>
</section>

<section data-markdown>
<textarea data-template>
## Learning to Predict Sequences

![]({{site.baseurl}}/assets/sequence-learning.png) <!-- .element: width="100%"-->

</textarea>
</section>

<section data-markdown>
<textarea data-template>

## Melody to Harmony in MicroJam

![]({{site.baseurl}}/assets/robojam-interaction.png) <!-- .element: width="50%" -->

1. Gain an overview of DL for music generation.
2. Develop a melody to harmony sequence to sequence model
3. Train the model on matched melody/harmony sequences 
4. Use MicroJam-sourced data as input and see if the generated harmonies make sense!

</textarea>
</section>

<!-- style="float:right;" -->

<section data-markdown>
<textarea data-template>

## Seq-to-Seq Music Generation

![](https://magenta.tensorflow.org/assets/music_transformer/motifs_shaded_boxes.png) <!-- .element: width="75%" -->

1. Understand the Transformer architecture.
2. Implement your own Transformer (e.g., in Keras).
3. Find a musical dataset that could be trained.
4. Train your model, listen to the results and find a way to evaluate them.

</textarea>
</section>

<section data-markdown>
<textarea data-template>

## Generating colour palettes from audio data

![]({{site.baseurl}}/assets/nainoa-shizuru-NcdG9mK3PBY-unsplash.jpg) <!-- .element: width="50%" -->

1. Gain an overview of DL for audio processing.
2. Obtain a dataset of audio and video (or colour) data.
3. Try different neural network designs and evaluate the results. (Even a simple fully-connected ANN might work well!)

</textarea>
</section>


<section data-markdown>
<textarea data-template>

## Motion-to-Motion Generators

![]({{site.baseurl}}/assets/motion-to-motion.png) <!-- .element: width="80%" -->

1. Gain an overview of the main DL methods used for motion generation including RNNs, MDRNNs, and world models.
2. Find a dataset of motion capture or other movement data (or capture one yourself!)
3. Train the ANN and evaluate its generative abilities.

</textarea>
</section>
