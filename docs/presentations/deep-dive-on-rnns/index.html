---
layout: reveal
title:  Deep Dive on RNNs
permalink: /presentations/deep-dive-on-rnns/
---

<section id="title-slide">
  <h1 class="title">Deep Dive on RNNs</h1>
  <p class="author">Charles Martin</p>
</section>

<section class="slide level2">
<h3 id="what-is-an-artificial-neurone">What is an Artificial Neurone?</h3>
<p><img data-src="./1-artificial-neurone.png" style="width:100.0%" /></p>
<p><a href="https://en.wikipedia.org/wiki/File:Blausen_0657_MultipolarNeuron.png">Source - Wikimedia Commons</a></p>
</section>

<section id="feed-forward-network" class="slide level2">
<h2>Feed-Forward Network</h2>
<p><img data-src="./2-feed-forward.png" style="width:75.0%" /></p>
<p>For each unit: <span class="math inline">\(y = \text{tanh}\big(Wx + b \big)\)</span></p>
</section>

<section id="recurrent-network" class="slide level2">
<h2>Recurrent Network</h2>
<p><img data-src="./3-recurrent.png" style="width:75.0%" /></p>
<p>For each unit: <span class="math inline">\(y_t = \text{tanh}\big(Ux_t + Vh_{t-1} + b \big)\)</span></p>
</section>

<section id="sequence-learning-tasks" class="slide level2">
<h2>Sequence Learning Tasks</h2>
<div class="columns">
<div class="column">
<p><img data-src="sequence-learning.png" /> <img data-src="sequence-learning2.png" /></p>
</div><div class="column">
<p><img data-src="sequence-learning3.jpg" /></p>
</div>
</div>
</section>

<section id="recurrent-network-1" class="slide level2">
<h2>Recurrent Network</h2>
<p><img data-src="4-unfolding.png" style="width:75.0%" /></p>
<p>simplifying…</p>
</section>
<section id="recurrent-network-2" class="slide level2">
<h2>Recurrent Network</h2>
<p><img data-src="5-recurrent-vert.png" style="width:25.0%" /></p>
<p>simplifying and rotating…</p>
</section>

<section id="state-in-recurrent-networks" class="slide level2">
<h2>“State” in Recurrent Networks</h2>
<div class="columns">
<div class="column">
<p><img data-src="5-recurrent-vert.png" style="width:50.0%" /></p>
</div><div class="column">
<ul>
<li class="fragment">Recurrent Networks are all about storing a “state” in between computations.</li>
<li class="fragment">A “lossy summary of… past sequences”</li>
<li class="fragment"><em>h</em> is the “hidden state” of our RNN</li>
<li class="fragment">What influences <em>h</em>?</li>
</ul>
</div>
</div>
</section>

<section class="slide level2">

<h3 id="defining-the-rnn-state">Defining the RNN State</h3>
<div class="columns">
<div class="column">
<p><img data-src="5-recurrent-vert.png" style="width:50.0%" /></p>
</div><div class="column">
<p>We can define a simplified RNN represented by this diagram as follows:</p>
<p><span class="math display">\[h_t = \text{tanh}\big(Ux_t + Vh_{t-1} + b \big)\]</span></p>
<p><span class="math display">\[\hat{y}_t = \text{softmax}(c + Wh_t)\]</span></p>
</div>
</div>
</section>

<section id="unfolding-an-rnn-in-time" class="slide level2">
<h2>Unfolding an RNN in Time</h2>
<figure>
<img data-src="6-recurrent-unroll.png" alt="Unfolding an RNN in Time" /><figcaption>Unfolding an RNN in Time</figcaption>
</figure>
<ul>
<li class="fragment">By unfolding the RNN we can compute <span class="math inline">\(\hat{y}\)</span> for a given length of sequence.</li>
<li class="fragment">Note that the weight matrices <span class="math inline">\(U\)</span>, <span class="math inline">\(V\)</span>, <span class="math inline">\(W\)</span> are the same for each timestep; this is the big advantage of RNNs!</li>
</ul>
</section>
<section id="forward-propagation" class="slide level2">
<h2>Forward Propagation</h2>
<div class="columns">
<div class="column">
<p><img data-src="forward-prop.png" /></p>
</div><div class="column">
<p>We can now use the following equations to compute <span class="math inline">\(\hat{y}_3\)</span>, by computing <span class="math inline">\(h\)</span> for the previous steps:</p>
<p><span class="math display">\[h_t = \text{tanh}\big(Ux_t + Vh_{t-1} + b \big)\]</span></p>
<p><span class="math display">\[\hat{y}_t = \text{softmax}(c + Wh_t)\]</span></p>
</div>
</div>
</section>

<section id="y-hat-is-softmaxd" class="slide level2">
<h2>Y-hat is Softmax’d</h2>
<p><img data-src="softmax-is-probability.png" style="width:75.0%" /></p>
<p><span class="math inline">\(\hat{y}\)</span> is a probability distribution! A finite number of weights that add to 1:</p>
<p><span class="math display">\[\sigma(\mathbf{z})_j = \frac{e^{z_j}}{\sum_{k=1}^K e^{z_k}} \text{ for } j = 1,\ldots, K\]</span></p>
</section>

<section id="calculating-loss-categorical-cross-entropy" class="slide level2">
<h2>Calculating Loss: Categorical Cross Entropy</h2>
<div class="columns">
<div class="column">
<p><img data-src="calculating-loss.png" /></p>
</div><div class="column">
<p>We use the categorical cross-entropy function for loss:</p>
<p><span class="math display">\[\begin{align*}
h_t &amp;= \text{tanh}\big( {b} + {Vh}_{t-1} + {Ux}_t \big) \\
\hat{y}_t &amp;= \text{softmax}(c + Wh_t) \\
L_t &amp;= -y_t \cdot \text{log}(\hat{y}_t) \\
\text{Loss} &amp;= \sum_t L_t \\
\end{align*}\]</span></p>
</div>
</div>
</section>

<section id="backpropagation-through-time-bptt" class="slide level2">
<h2>Backpropagation Through Time (BPTT)</h2>
<div class="columns">
<div class="column">
<p><img data-src="7-recurrent-loss.png" /></p>
</div><div class="column">
<p>Propagates error correction backwards through the network graph, adjusting all parameters (<em>U</em>, <em>V</em>, <em>W</em>) to minimise loss.</p>
</div>
</div>
</section>

<section id="example-character-level-text-model" class="slide level2">
<h2>Example: Character-level text model</h2>
<ul>
<li class="fragment"><strong>Training data:</strong> a collection of text.</li>
<li class="fragment"><strong>Input (<em>X</em>):</strong> snippets of 30 characters from the collection.</li>
<li class="fragment"><strong>Target output (<em>y</em>)</strong>: 1 character, the next one after the 30 in each <em>X</em>.</li>
</ul>
</section>

<section id="training-the-character-level-model" class="slide level2">
<h2>Training the Character-level Model</h2>
<div class="columns">
<div class="column">
<p><img data-src="charRNN-training.png" /></p>
</div><div class="column">
<ul>
<li class="fragment">Target: A probability distribution with <span class="math inline">\(P(n) = 1\)</span></li>
<li class="fragment">Output: A probability distribution over all next letters.</li>
<li class="fragment">E.g.: “My cat is named Simon” would lead to <strong>X</strong>: “My cat is named Simo” and <strong>y</strong>: “n”</li>
</ul>
</div>
</div>
</section>

<section id="using-the-trained-model-to-generate-text" class="slide level2">
<h2>Using the trained model to generate text</h2>
<div class="columns">
<div class="column">
<p><img data-src="charRNN-sampling.png" /></p>
</div><div class="column">
<ul>
<li class="fragment"><strong>S</strong>: Sampling function, sample a letter using the output probability distribution.</li>
<li class="fragment">The generated letter is reinserted at as the next input.</li>
<li class="fragment">We don’t want to always draw the most likely character. The would give frequent repetition and “copying” from the training text. Need a sampling strategy.</li>
</ul>
</div>
</div>
</section>

<section id="char-rnn" class="slide level2">
<h2>Char-RNN</h2>
<div class="columns">
<div class="column">
<ul>
<li class="fragment">RNN as a sequence generator</li>
<li class="fragment">Input is current symbol, output is next predicted symbol.</li>
<li class="fragment">Connect output to input and continue!</li>
<li class="fragment">CharRNN simply applies this to a (subset) of ASCII characters.</li>
<li class="fragment">Train and generate on any text corpus: Fun!</li>
</ul>
</div><div class="column">
<p><img data-src="charRNN-example.png" /></p>
<p>See: <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">Karpathy, A. (2015). The unreasonable effectiveness of recurrent neural networks.</a></p>
</div>
</div>
</section>

<section id="char-rnn-examples" class="slide level2">
<h2>Char-RNN Examples</h2>
<div class="columns">
<div class="column">
<p>Shakespeare (Karpathy, 2015):</p>
<blockquote>
<p>Second Senator: They are away this miseries, produced upon my soul, Breaking and strongly should be buried, when I perish The earth and thoughts of many states.</p>
</blockquote>
<blockquote>
<p>DUKE VINCENTIO: Well, your wit is in the care of side and that.</p>
</blockquote>
</div><div class="column">
<p>Latex Algebraic Geometry:</p>
<p><img data-src="charRNN-latex-karpathy.jpeg" /></p>
<p>N.B. “<em>Proof.</em> Omitted.” Lol.</p>
</div>
</div>
</section>

<section id="rnn-architectures-and-lstm" class="slide level2">
<h2>RNN Architectures and LSTM</h2>
</section>

<section id="bidirectional-rnns" class="slide level2">
<h2>Bidirectional RNNs</h2>
<div class="columns">
<div class="column">
<p><img data-src="blstm.png" /></p>
</div><div class="column">
<ul>
<li class="fragment">Useful for tasks where the whole sequence is available.</li>
<li class="fragment">Each output unit (<span class="math inline">\(\hat{y}\)</span>) depends on both past and future - but most sensitive to closer times.</li>
<li class="fragment">Popular in speech recognition, translation etc.</li>
</ul>
</div>
</div>
</section>

<section id="encoder-decoder-seq-to-seq" class="slide level2">
<h2>Encoder-Decoder (seq-to-seq)</h2>
<div class="columns">
<div class="column">
<p><img data-src="seq2seq.png" /></p>
</div><div class="column">
<ul>
<li class="fragment">Learns to generate output sequence (<strong>y</strong>) from an input sequence (<strong>x</strong>).</li>
<li class="fragment">Final hidden state of encoder is used to compute a context variable <em>C</em>.</li>
<li class="fragment">For example, translation.</li>
</ul>
</div>
</div>
</section>

<section id="deep-rnns" class="slide level2">
<h2>Deep RNNs</h2>
<div class="columns">
<div class="column">
<p><img data-src="rnn-deep.png" style="width:30.0%" /></p>
</div><div class="column">
<ul>
<li class="fragment">Does adding deeper layers to an RNN make it work better?</li>
<li class="fragment">Several options for architecture.</li>
<li class="fragment">Simply stacking RNN layers is very popular; shown to work better by Graves et al. (2013)</li>
<li class="fragment">Intuitively: layers might learn some hierarchical knowledge automatically.</li>
<li class="fragment">Typical setup: up to three recurrent layers.</li>
</ul>
</div>
</div>
</section>

<section id="long-term-dependencies" class="slide level2">
<h2>Long-Term Dependencies</h2>
<div class="columns">
<div class="column">
<ul>
<li class="fragment">Learning long dependencies is a mathematical challenge.</li>
<li class="fragment">Basically: gradients propagated through the same weights tend to vanish (mostly) or explode (rarely)</li>
<li class="fragment">E.g., consider a simplified RNN with no nonlinear activation function or input.</li>
<li class="fragment">Each time step multiplies <em>h(0)</em> by <em>W</em>.</li>
<li class="fragment">This corresponds to raising power of eigenvalues in <span class="math inline">\(\Lambda\)</span>.</li>
<li class="fragment">Eventually, components of <em>h(0)</em> not aligned with the largest eigenvector will be discarded.</li>
</ul>
</div><div class="column">
<p><span class="math display">\[\begin{align*}
h_t &amp;= Wh_{t-1}\\
h_t &amp;= (W^t)h_0
\end{align*}\]</span></p>
<p>(supposing <strong>W</strong> admits eigendecomposition with orthogonal matrix <strong>Q</strong>)</p>
<p><span class="math display">\[\begin{align*}
W &amp;= Q\Lambda Q^{\top}\\
h_t &amp;= Q\Lambda ^t Qh_0
\end{align*}\]</span></p>
</div>
</div>
</section>

<section id="vanishing-and-exploding-gradients" class="slide level2">
<h2>Vanishing and Exploding Gradients</h2>
<div class="columns">
<div class="column">
<ul>
<li class="fragment">“in order to store memories in a way that is robust to small perturbations, the RNN must enter a region of parameter space where gradients vanish”</li>
<li class="fragment">“whenever the model is able to represent long term dependencies, the gradient of a long term interaction has exponentially smaller magnitude than the gradient of a short term interaction.”</li>
</ul>
</div><div class="column">
<ul>
<li class="fragment">Note that this problem is only relevant for recurrent networks since the weights <strong>W</strong> affecting the hidden state are the same at each time step.</li>
<li class="fragment">Goodfellow and Benigo (2016): “the problem of learning long-term dependencies remains one of the main challenges in deep learning”</li>
<li class="fragment"><a href="http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/">WildML (2015). Backpropagation Through Time and Vanishing Gradients</a></li>
<li class="fragment"><a href="https://ml4a.github.io/ml4a/RNNs/">ML for artists</a></li>
</ul>
</div>
</div>
</section>

<section id="gated-rnns" class="slide level2">
<h2>Gated RNNs</h2>
<div class="columns">
<div class="column">
<p><img data-src="gated-rnn.png" /></p>
</div><div class="column">
<ul>
<li class="fragment">Possible solution!</li>
<li class="fragment">Provide a gate that can change the hidden state a little bit at each step.</li>
<li class="fragment">The gates are controlled by <strong>learnable weights</strong> as well!</li>
<li class="fragment">Hidden state weights that may <strong>change</strong> at each time step.</li>
<li class="fragment">Create <strong>paths through time</strong> with derivatives that do not vanish/explode.</li>
<li class="fragment">Gates choose information to <strong>accumulate</strong> or <strong>forget</strong> at each time step.</li>
<li class="fragment"><strong>Most effective sequence models used in practice!</strong></li>
</ul>
</div>
</div>
</section>

<section id="long-short-term-memory" class="slide level2">
<h2>Long Short-Term Memory</h2>
<div class="columns">
<div class="column">
<ul>
<li class="fragment">Self-loop containing an internal state (c).</li>
<li class="fragment">Three extra gating units:
<ul>
<li class="fragment"><strong>Forget gate</strong>: controls how much memory is preserved.</li>
<li class="fragment"><strong>Input gate</strong>: control how much of current input is stored.</li>
<li class="fragment"><strong>Output gate</strong>: control how much of state is shown to output.</li>
</ul></li>
<li class="fragment">Each gate has own <strong>weights</strong> and <strong>biases</strong>, so this uses <em>lots</em> more parameters.</li>
<li class="fragment">Some variants on this design, e.g., use c as additional input to three gate units.</li>
</ul>
</div><div class="column">
<p><img data-src="lstm.png" style="width:80.0%" /></p>
</div>
</div>
</section>

<section id="long-short-term-memory-1" class="slide level2">
<h2>Long Short-Term Memory</h2>
<div class="columns">
<div class="column">
<ul>
<li class="fragment">Forget gate: <em>f</em></li>
<li class="fragment">Internal state: <em>s</em></li>
<li class="fragment">Input gate: <em>g</em></li>
<li class="fragment">Output gate: <em>q</em></li>
<li class="fragment">Output: <em>h</em></li>
</ul>
</div><div class="column">
<p><img data-src="lstm.png" style="width:80.0%" /></p>
</div>
</div>
</section>

<section id="other-gating-units" class="slide level2">
<h2>Other Gating Units</h2>
<div class="columns">
<div class="column">
<p><img data-src="gru.png" style="width:100.0%" /> Source: <a href="https://youtu.be/FpQCAd0zKiU">(Olah, C. 2015.)</a></p>
</div><div class="column">
<ul>
<li class="fragment">Are three gates necessary?</li>
<li class="fragment">Other gating units are simpler, e.g., Gated Recurrent Unit (GRU)</li>
<li class="fragment">For the moment, LSTMs are winning in practical use.</li>
<li class="fragment">Maybe someone wants to explore alternatives in a project?</li>
</ul>
</div>
</div>
</section>

<section id="visualising-lstm-activations" class="slide level2">
<h2>Visualising LSTM activations</h2>
<p>Sometimes, the LSTM cell state corresponds with features of the sequential data:</p>
<p><img data-src="lstm-state-visualisation.png" style="width:80.0%" /></p>
<p>Source: <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">(Karpathy, 2015)</a></p>
</section>

<section id="charrnn-applications-folkrnn" class="slide level2">
<h2>CharRNN Applications: FolkRNN</h2>
<p>Some kinds of music can be represented in a text-like manner.</p>
<p><img data-src="folk-rnn.png" style="width:80.0%" /></p>
<p>Source: <a href="https://core.ac.uk/download/pdf/55873381.pdf">Sturm et al. 2015. Folk Music Style Modelling by Recurrent Neural Networks with Long Short Term Memory Units</a></p>
</section>

<section id="other-charrnn-applications" class="slide level2">
<h2>Other CharRNN Applications</h2>
<div class="columns">
<div class="column">
<p><img data-src="rnn-monet.png" /> <a href="http://blog.manugarri.com/teaching-recurrent-neural-networks-about-monet/">Teaching Recurrent Neural Networks about Monet</a></p>
</div><div class="column">
<p><img data-src="rnn-paint-colours.png" /> <a href="http://aiweirdness.com/post/160776374467/new-paint-colors-invented-by-neural-network">New Paint Colours Invented by Neural Network</a></p>
</div>
</div>
</section>

<section id="google-magenta-performance-rnn" class="slide level2">
<h2>Google Magenta Performance RNN</h2>
<div class="columns">
<div class="column">
<p><img data-src="performance-rnn.jpg" /></p>
</div><div class="column">
<p><img data-src="performance-rnn2.png" /></p>
</div>
</div>
<ul>
<li class="fragment">State-of-the-art in music generating RNNs.</li>
<li class="fragment">Encode MIDI musical sequences as categorical data.</li>
<li class="fragment">Now supports polyphony (multiple notes), dynamics (volume), expressive timing (rubato).</li>
<li class="fragment">E.g.: <a href="https://youtu.be/JVf6esaXeLE">YouTube demo</a></li>
</ul>
</section>

<section id="neural-ipad-band-another-charrnn" class="slide level2">
<h2>Neural iPad Band, another CharRNN</h2>
<div class="columns">
<div class="column">
<p><img data-src="neural-ipad-screen.jpg" /> <img data-src="neural-ipad-diagram.png" /></p>
</div><div class="column">
<ul>
<li class="fragment">iPad music transcribed as sequence of numbers for each performer.</li>
<li class="fragment">Trick: encode multiple ints as one (preserving ordering).</li>
<li class="fragment"><a href="https://youtu.be/FpQCAd0zKiU">Video</a></li>
</ul>
</div>
</div>
</section>

<section id="books-and-learning-references" class="slide level2">
<h2>Books and Learning References</h2>
<ul>
<li class="fragment"><a href="http://www.deeplearningbook.org">Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press.</a></li>
<li class="fragment"><a href="https://www.manning.com/books/deep-learning-with-python">François Chollet. 2018. Manning.</a></li>
<li class="fragment"><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Chris Olah. 2015. Understanding LSTMs</a></li>
<li class="fragment"><a href="https://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html">RNNs in Tensorflow</a></li>
<li class="fragment"><a href="https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0">Maybe RNN/LSTM is dead? CNNs can work similarly to BLSTMs</a></li>
<li class="fragment"><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">Karpathy. 2015. The Unreasonable Effectiveness of RNNs</a></li>
</ul>
</section>

<section id="summary" class="slide level2">
<h2>Summary</h2>
<ul>
<li class="fragment">Recurrent Neural Networks let us capture and model the structure of sequential data.</li>
<li class="fragment">Sampling from trained RNNs allow us to generate new, creative sequences.</li>
<li class="fragment">The internal state of RNNs make them interesting for interactive applications, since it lets them capture and continue from the current context or “style”.</li>
<li class="fragment">LSTM units are able to overcome the vanishing gradient problem to some extent.</li>
</ul>
</section>
