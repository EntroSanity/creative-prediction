{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Level RNN using LSTM cells.\n",
    "\n",
    "- Trains on Star Trek episode titles\n",
    "- Outputs \"fake\" titles.\n",
    "\n",
    "Much comes from a [Keras example](https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py).\n",
    "\n",
    "## Setup Environment\n",
    "\n",
    "- Import Keras\n",
    "- Open up the Star Trek corpus\n",
    "- Give each leter an index and create dictionaries to translate from index to character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 11010\n",
      "total chars: 49\n",
      "Max: 50\n",
      "Mean: 14.0013623978\n",
      "Median: 13.0\n",
      "Min: 2\n"
     ]
    }
   ],
   "source": [
    "## Much borrowed from https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "\n",
    "text = open(\"startrekepisodes.txt\").read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocabulary_size = len(chars)\n",
    "print('total chars:', vocabulary_size)\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "# How long is a title?\n",
    "titles = text.split('\\n')\n",
    "lengths = np.array([len(n) for n in titles])\n",
    "print(\"Max:\", np.max(lengths))\n",
    "print(\"Mean:\", np.mean(lengths))\n",
    "print(\"Median:\", np.median(lengths))\n",
    "print(\"Min:\", np.min(lengths))\n",
    "\n",
    "# hence choose 30 as seuence length to train on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Training Data\n",
    "\n",
    "- Cut up the corpus into sequences of 40 characters.\n",
    "- Change indexes into \"one-hot\" vector encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 3660\n",
      "Done preparing training corpus, shapes of sets are:\n",
      "X shape: (3660, 30)\n",
      "y shape: (3660, 49)\n",
      "Vocabulary of characters: 49\n"
     ]
    }
   ],
   "source": [
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 30\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "X = np.zeros((len(sentences), maxlen), dtype=int)\n",
    "y = np.zeros((len(sentences), vocabulary_size), dtype=np.bool)\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    X[i] = np.array([char_indices[x] for x in sentences[i]])\n",
    "    y[i, char_indices[next_chars[i]]] = True\n",
    "print(\"Done preparing training corpus, shapes of sets are:\")\n",
    "print(\"X shape: \" + str(X.shape))\n",
    "print(\"y shape: \" + str(y.shape))\n",
    "print(\"Vocabulary of characters:\", vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "- Model has one hidden layer of 128 LSTM cells.\n",
    "- Input layer is an Embedding to convert from indices to a vector encoding automatically (common trick - but does it work?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 30, 256)           12544     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 30, 256)           525312    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 49)                12593     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 49)                0         \n",
      "=================================================================\n",
      "Total params: 1,075,761\n",
      "Trainable params: 1,075,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "layer_size = 256\n",
    "dropout_rate = 0.5\n",
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model_train = Sequential()\n",
    "model_train.add(Embedding(vocabulary_size, layer_size, input_length=maxlen))\n",
    "\n",
    "# LSTM part\n",
    "model_train.add(Dropout(dropout_rate))\n",
    "model_train.add(LSTM(layer_size, return_sequences=True))\n",
    "model_train.add(Dropout(dropout_rate))\n",
    "model_train.add(LSTM(layer_size))\n",
    "\n",
    "# Project back to vocabulary\n",
    "model_train.add(Dense(vocabulary_size))\n",
    "model_train.add(Activation('softmax'))\n",
    "model_train.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.01))\n",
    "model_train.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "- Train on batches of 128 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3660/3660 [==============================] - 26s 7ms/step - loss: 3.5391\n",
      "Epoch 2/10\n",
      "3660/3660 [==============================] - 24s 7ms/step - loss: 3.3889\n",
      "Epoch 3/10\n",
      "3660/3660 [==============================] - 24s 7ms/step - loss: 3.3651\n",
      "Epoch 4/10\n",
      "3660/3660 [==============================] - 25s 7ms/step - loss: 3.3820\n",
      "Epoch 5/10\n",
      "3660/3660 [==============================] - 25s 7ms/step - loss: 3.3744\n",
      "Epoch 6/10\n",
      "3660/3660 [==============================] - 24s 7ms/step - loss: 3.3674\n",
      "Epoch 7/10\n",
      "3660/3660 [==============================] - 24s 7ms/step - loss: 3.3707\n",
      "Epoch 8/10\n",
      "3660/3660 [==============================] - 24s 7ms/step - loss: 3.3601\n",
      "Epoch 9/10\n",
      "3660/3660 [==============================] - 25s 7ms/step - loss: 3.3611\n",
      "Epoch 10/10\n",
      "3660/3660 [==============================] - 25s 7ms/step - loss: 3.3675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x122bc1550>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the Model.\n",
    "model_train.fit(X, y, batch_size=64, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_train.fit(X, y, batch_size=64, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model if necessary\n",
    "model_train.save(\"keras-startrek-LSTM-model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Model\n",
    "\n",
    "- Take a quote then add 400 characters.\n",
    "\n",
    "### Make a Decoder model\n",
    "\n",
    "- Needs input length of 1.\n",
    "- Needs batch size of 1\n",
    "- Needs LSTM to be stateful\n",
    "- check that params is the same as model_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model if necessary.\n",
    "model_train = load_model(\"keras-startrek-LSTM-model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (1, 1, 256)               12544     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (1, 1, 256)               0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (1, 1, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (1, 1, 256)               0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (1, 256)                  525312    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (1, 49)                   12593     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (1, 49)                   0         \n",
      "=================================================================\n",
      "Total params: 1,075,761\n",
      "Trainable params: 1,075,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build a decoding model (input length 1, batch size 1, stateful)\n",
    "layer_size = 256\n",
    "dropout_rate = 0.5\n",
    "\n",
    "model_dec = Sequential()\n",
    "model_dec.add(Embedding(vocabulary_size, layer_size, input_length=1, batch_input_shape=(1,1)))\n",
    "\n",
    "# LSTM part\n",
    "model_dec.add(Dropout(dropout_rate))\n",
    "model_dec.add(LSTM(layer_size, stateful=True, return_sequences=True))\n",
    "model_dec.add(Dropout(dropout_rate))\n",
    "model_dec.add(LSTM(layer_size, stateful=True))\n",
    "\n",
    "# project back to vocabulary\n",
    "model_dec.add(Dense(vocabulary_size))\n",
    "model_dec.add(Activation('softmax'))\n",
    "model_dec.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.01))\n",
    "model_dec.summary()\n",
    "\n",
    "# set weights from training model\n",
    "model_dec.set_weights(model_train.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sampling function\n",
    "\n",
    "def sample_model(seed, model_name, length=400):\n",
    "    '''Samples a charRNN given a seed sequence.'''\n",
    "    generated = ''\n",
    "    sentence = seed.lower()[:]\n",
    "    generated += sentence\n",
    "    print(\"Seed: \", generated)\n",
    "    \n",
    "    for i in range(length):\n",
    "        x = np.array([char_indices[n] for n in sentence])\n",
    "        x = np.reshape(x,(1,1))\n",
    "        preds = model_name.predict(x, verbose=0)[0]\n",
    "        next_index = sample(preds, 0.5)\n",
    "        next_char = indices_char[next_index]\n",
    "        \n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "    print(\"Generated: \", generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:   \n",
      "Generated:   kr\n",
      "sss  sss s sssss  ssss ssssss s     sss  s ssss  s  s ssss  s s ssss  s  ss sss         s  ssssssss s   s ss s  ss sss  ssss ssss  sss sss  sss s   ss ssss s  ss ss s    ss ss     s sss ss s  ss s s s s sssss ss s sss   s s   ss s  s s      ss   ss    sss sss  sss s sss ss ssss    s ss  sss  ss   ss  s  sssss  ss ss s s  s     \n",
      " sssss s ssssss  ssrs  sss  s ss   s    ss s   ssts  ss ss  s sss     s s s s   s  s   ss sss    s   s s  s s  s s  s ss   s    ssss   s   s s   ssrsssss ss       sss s ss s s s s   s  sss ssss  s ss   s sss ss  ss  ss rsssss  sss   ssss  r s  ssss s      ss sss ss   sss   sssss    sssrs ss s s  s  s  s s    ssss s ssss   s  ss ssss ss    s s ssssssssssss sss   s s   rss s ss ss    s   ss  ssss s   ss sss  ss s s ss    r     s ss   ss  s    s s       s s    sss   s s ss   ss ss s   s   s ss   sssrs ssss ssss     ss s       ss sssss ss s srss  s s    ss    s s         s  sss   s ss  ss ts s  s sss   sss   s\n",
      "    sss  s  s s   s ssss   s  s s  s s  ss   ssssssss\n"
     ]
    }
   ],
   "source": [
    "# Sample 1000 characters from the model using a random seed from the vocabulary.\n",
    "sample_model(indices_char[random.randint(0,vocabulary_size-1)], model_dec, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments - Model Maker\n",
    "\n",
    "A single function to make both training and running models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_maker(model, layer_size=64, dropout_rate=0.5, num_layers=1, vocab_size=20, input_length=1, lr=0.01, train_mode=True):\n",
    "    \"\"\"Builds a charRNN model with variable layer size, number of layers, droupout, learning rate, and a training mode.\"\"\"\n",
    "    if train_mode:\n",
    "        stateful = False\n",
    "        input_shape = (None, input_length)\n",
    "    else:\n",
    "        stateful = True\n",
    "        input_shape = (1, input_length)\n",
    "    \n",
    "    # Input embedding\n",
    "    model.add(Embedding(vocab_size, layer_size, input_length=input_length, batch_input_shape=input_shape))\n",
    "              \n",
    "    # LSTM layers + 1\n",
    "    for i in range(num_layers - 1):\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        model.add(LSTM(layer_size, return_sequences=True, stateful=stateful))\n",
    "    \n",
    "    # Final LSTM layer\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(layer_size, stateful=stateful))\n",
    "\n",
    "    # Project back to vocabulary\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=lr))\n",
    "    model.summary()\n",
    "\n",
    "# m = Sequential()\n",
    "# model_maker(m, layer_size=128, vocab_size=vocabulary_size, input_length=30, train_mode=True)\n",
    "# m.fit(X, y, batch_size=64, epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
