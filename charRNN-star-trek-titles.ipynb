{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Level RNN using LSTM cells.\n",
    "\n",
    "- Trains on Star Trek episode titles\n",
    "- Outputs \"fake\" titles.\n",
    "\n",
    "Much comes from a [Keras example](https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py).\n",
    "\n",
    "## Setup Environment\n",
    "\n",
    "- Import Keras\n",
    "- Open up the Star Trek corpus\n",
    "- Give each leter an index and create dictionaries to translate from index to character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 11017\n",
      "total chars: 52\n"
     ]
    }
   ],
   "source": [
    "## Much borrowed from https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "\n",
    "text = open(\"startrekepisodes.txt\").read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocabulary_size = len(chars)\n",
    "print('total chars:', vocabulary_size)\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max: 50\n",
      "Mean: 14.0108991826\n",
      "Median: 13.0\n",
      "Min: 2\n"
     ]
    }
   ],
   "source": [
    "# How long is a title?\n",
    "titles = text.split('\\n')\n",
    "lengths = np.array([len(n) for n in titles])\n",
    "print(\"Max:\", np.max(lengths))\n",
    "print(\"Mean:\", np.mean(lengths))\n",
    "print(\"Median:\", np.median(lengths))\n",
    "print(\"Min:\", np.min(lengths))\n",
    "\n",
    "# hence choose 30 as seuence length to train on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Training Data\n",
    "\n",
    "- Cut up the corpus into sequences of 40 characters.\n",
    "- Change indexes into \"one-hot\" vector encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 3663\n",
      "Done preparing training corpus, shapes of sets are:\n",
      "X shape: (3663, 30)\n",
      "y shape: (3663, 52)\n",
      "Vocabulary of characters: 52\n"
     ]
    }
   ],
   "source": [
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 30\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "X = np.zeros((len(sentences), maxlen), dtype=int)\n",
    "y = np.zeros((len(sentences), vocabulary_size), dtype=np.bool)\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    X[i] = np.array([char_indices[x] for x in sentences[i]])\n",
    "    y[i, char_indices[next_chars[i]]] = True\n",
    "print(\"Done preparing training corpus, shapes of sets are:\")\n",
    "print(\"X shape: \" + str(X.shape))\n",
    "print(\"y shape: \" + str(y.shape))\n",
    "print(\"Vocabulary of characters:\", vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_maker(model, layer_size=64, dropout_rate=0.5, num_layers=1, vocab_size=20, input_length=1, lr=0.01, train_mode=True):\n",
    "    \"\"\"Builds a charRNN model with variable layer size, number of layers, droupout, learning rate, and a training mode.\"\"\"\n",
    "    if train_mode:\n",
    "        stateful = False\n",
    "        input_shape = (None, input_length)\n",
    "    else:\n",
    "        stateful = True\n",
    "        input_shape = (1, input_length)\n",
    "    \n",
    "    # Input embedding\n",
    "    model.add(Embedding(vocab_size, layer_size, input_length=input_length, batch_input_shape=input_shape))\n",
    "              \n",
    "    # LSTM layers + 1\n",
    "    for i in range(num_layers - 1):\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        model.add(LSTM(layer_size, return_sequences=True, stateful=stateful))\n",
    "    \n",
    "    # Final LSTM layer\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(layer_size, stateful=stateful))\n",
    "\n",
    "    # Project back to vocabulary\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=lr))\n",
    "    model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building a model!\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 30, 128)           6272      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 49)                6321      \n",
      "=================================================================\n",
      "Total params: 144,177\n",
      "Trainable params: 144,177\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m = Sequential()\n",
    "model_maker(m, layer_size=128, vocab_size=vocabulary_size, input_length=30, train_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3660/3660 [==============================] - 5s 1ms/step - loss: 2.9570\n",
      "Epoch 2/5\n",
      "3660/3660 [==============================] - 4s 1ms/step - loss: 2.4900\n",
      "Epoch 3/5\n",
      "3660/3660 [==============================] - 4s 1ms/step - loss: 2.3087\n",
      "Epoch 4/5\n",
      "3660/3660 [==============================] - 4s 1ms/step - loss: 2.1372\n",
      "Epoch 5/5\n",
      "3660/3660 [==============================] - 5s 1ms/step - loss: 1.9828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11d56a208>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.fit(X, y, batch_size=64, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building a model!\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1, 64)             1280      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                1300      \n",
      "=================================================================\n",
      "Total params: 35,604\n",
      "Trainable params: 35,604\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "- Model has one hidden layer of 128 LSTM cells.\n",
    "- Input layer is an Embedding to convert from indices to a vector encoding automatically (common trick - but does it work?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 30, 256)           13312     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 30, 256)           525312    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 52)                13364     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 52)                0         \n",
      "=================================================================\n",
      "Total params: 1,077,300.0\n",
      "Trainable params: 1,077,300.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "layer_size = 256\n",
    "dropout_rate = 0.5\n",
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model_train = Sequential()\n",
    "model_train.add(Embedding(vocabulary_size, layer_size, input_length=maxlen))\n",
    "\n",
    "# LSTM part\n",
    "model_train.add(Dropout(dropout_rate))\n",
    "model_train.add(LSTM(layer_size, return_sequences=True))\n",
    "model_train.add(Dropout(dropout_rate))\n",
    "model_train.add(LSTM(layer_size))\n",
    "\n",
    "# Project back to vocabulary\n",
    "model_train.add(Dense(vocabulary_size))\n",
    "model_train.add(Activation('softmax'))\n",
    "model_train.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.01))\n",
    "model_train.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "- Train on batches of 128 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3663/3663 [==============================] - 22s - loss: 3.6762    \n",
      "Epoch 2/10\n",
      "3663/3663 [==============================] - 20s - loss: 3.4224    \n",
      "Epoch 3/10\n",
      "3663/3663 [==============================] - 20s - loss: 3.3314    \n",
      "Epoch 4/10\n",
      "3663/3663 [==============================] - 20s - loss: 3.3159    \n",
      "Epoch 5/10\n",
      "3663/3663 [==============================] - 20s - loss: 3.2872    \n",
      "Epoch 6/10\n",
      "3663/3663 [==============================] - 20s - loss: 3.1465    \n",
      "Epoch 7/10\n",
      "3663/3663 [==============================] - 20s - loss: 3.0250    \n",
      "Epoch 8/10\n",
      "3663/3663 [==============================] - 20s - loss: 2.9185    \n",
      "Epoch 9/10\n",
      "3663/3663 [==============================] - 20s - loss: 2.8390    \n",
      "Epoch 10/10\n",
      "3663/3663 [==============================] - 20s - loss: 2.7557    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11ed61c10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the Model.\n",
    "model_train.fit(X, y, batch_size=64, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4026/4026 [==============================] - 23s - loss: 1.3949    \n",
      "Epoch 2/20\n",
      "4026/4026 [==============================] - 23s - loss: 1.4134    \n",
      "Epoch 3/20\n",
      "4026/4026 [==============================] - 23s - loss: 1.3781    \n",
      "Epoch 4/20\n",
      "4026/4026 [==============================] - 23s - loss: 1.3509    \n",
      "Epoch 5/20\n",
      "4026/4026 [==============================] - 23s - loss: 1.3902    \n",
      "Epoch 6/20\n",
      "4026/4026 [==============================] - 23s - loss: 1.3710    \n",
      "Epoch 7/20\n",
      "4026/4026 [==============================] - 22s - loss: 1.3382    \n",
      "Epoch 8/20\n",
      "4026/4026 [==============================] - 23s - loss: 1.3373    \n",
      "Epoch 9/20\n",
      "4026/4026 [==============================] - 25s - loss: 1.3159    \n",
      "Epoch 10/20\n",
      "4026/4026 [==============================] - 23s - loss: 1.2890    \n",
      "Epoch 11/20\n",
      "4026/4026 [==============================] - 23s - loss: 1.3015    \n",
      "Epoch 12/20\n",
      "4026/4026 [==============================] - 22s - loss: 1.2889    \n",
      "Epoch 13/20\n",
      "4026/4026 [==============================] - 22s - loss: 1.2860    \n",
      "Epoch 14/20\n",
      "4026/4026 [==============================] - 23s - loss: 1.2963    \n",
      "Epoch 15/20\n",
      "4026/4026 [==============================] - 22s - loss: 1.2825    \n",
      "Epoch 16/20\n",
      "4026/4026 [==============================] - 23s - loss: 1.2772    \n",
      "Epoch 17/20\n",
      "4026/4026 [==============================] - 22s - loss: 1.2556    \n",
      "Epoch 18/20\n",
      "4026/4026 [==============================] - 23s - loss: 1.2470    \n",
      "Epoch 19/20\n",
      "4026/4026 [==============================] - 22s - loss: 1.2165    \n",
      "Epoch 20/20\n",
      "4026/4026 [==============================] - 23s - loss: 1.2419    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x124bdca10>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_train.fit(X, y, batch_size=64, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model if necessary\n",
    "model_train.save(\"keras-startrek-LSTM-model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Model\n",
    "\n",
    "- Take a quote then add 400 characters.\n",
    "\n",
    "### Make a Decoder model\n",
    "\n",
    "- Needs input length of 1.\n",
    "- Needs batch size of 1\n",
    "- Needs LSTM to be stateful\n",
    "- check that params is the same as model_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model if necessary.\n",
    "model_train = load_model(\"keras-startrek-LSTM-model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (1, 1, 256)               13312     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (1, 1, 256)               0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (1, 1, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (1, 1, 256)               0         \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (1, 256)                  525312    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (1, 52)                   13364     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (1, 52)                   0         \n",
      "=================================================================\n",
      "Total params: 1,077,300.0\n",
      "Trainable params: 1,077,300.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build a decoding model (input length 1, batch size 1, stateful)\n",
    "layer_size = 256\n",
    "dropout_rate = 0.5\n",
    "\n",
    "model_dec = Sequential()\n",
    "model_dec.add(Embedding(vocabulary_size, layer_size, input_length=1, batch_input_shape=(1,1)))\n",
    "\n",
    "# LSTM part\n",
    "model_dec.add(Dropout(dropout_rate))\n",
    "model_dec.add(LSTM(layer_size, stateful=True, return_sequences=True))\n",
    "model_dec.add(Dropout(dropout_rate))\n",
    "model_dec.add(LSTM(layer_size, stateful=True))\n",
    "\n",
    "# project back to vocabulary\n",
    "model_dec.add(Dense(vocabulary_size))\n",
    "model_dec.add(Activation('softmax'))\n",
    "model_dec.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.01))\n",
    "model_dec.summary()\n",
    "\n",
    "# set weights from training model\n",
    "model_dec.set_weights(model_train.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sampling function\n",
    "\n",
    "def sample_model(seed, model_name, length=400):\n",
    "    '''Samples a charRNN given a seed sequence.'''\n",
    "    generated = ''\n",
    "    sentence = seed.lower()[:]\n",
    "    generated += sentence\n",
    "    print(\"Seed: \", generated)\n",
    "    \n",
    "    for i in range(length):\n",
    "        x = np.array(map((lambda x: char_indices[x]), sentence))\n",
    "        x = np.reshape(x,(1,1))\n",
    "        preds = model_name.predict(x, verbose=0)[0]\n",
    "        next_index = sample(preds, 0.5)\n",
    "        next_char = indices_char[next_index]\n",
    "        \n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "    print(\"Generated: \", generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:  �\n",
      "Generated:  �ort the barenthe tont\n",
      "bars\n",
      "the the the bare\n",
      "the the the ponthe ont\n",
      "garthe\n",
      "ses\n",
      "the she the the the part\n",
      "she partrenine the parenentunt\n",
      "nenthe santhe brertinerer\n",
      "bartrine\n",
      "the sonthe the bare\n",
      "the pars\n",
      "dine\n",
      "ban\n",
      "the sre the bonthe the the the partintins\n",
      "the ponthe the anesenine\n",
      "partert parts\n",
      "serthe panthe the ertres\n",
      "the cre inens\n",
      "the anensonthe the banthe\n",
      "the sons\n",
      "sre the the the sint the ant\n",
      "srenthe part\n",
      "fame\n",
      "porert\n",
      "the partont the sre\n",
      "the the the senthe the oncere\n",
      "the pane\n",
      "the sayenre\n",
      "the the parene\n",
      "the se\n",
      "part\n",
      "contre the sinthe the inenthe the the entunthe sintre sint the bronsranthe the the the the surthe part the sintins\n",
      "cort sint banthe the re\n",
      "the ante\n",
      "fonthe inthe the barthe the the prert the the the dale\n",
      "the s onenens ors\n",
      "nonthe the pare\n",
      "the the anerenthestertre\n",
      "the barthe the parte\n",
      "the parthe ent santhe part\n",
      "the barturthe payensoy\n",
      "pre partrere\n",
      "sunse the pare\n",
      "the the partrens\n",
      "inthe enthe parthe she\n",
      "pares\n",
      "frens\n",
      "the sint the porer\n",
      "the pre the whe the bartrentonins\n",
      "parerthe the pre the\n"
     ]
    }
   ],
   "source": [
    "# Sample 1000 characters from the model using a random seed from the vocabulary.\n",
    "sample_model(indices_char[random.randint(0,vocabulary_size-1)], model_dec, 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
