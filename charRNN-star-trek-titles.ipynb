{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Level RNN using LSTM cells.\n",
    "\n",
    "- Trains on Star Trek episode titles\n",
    "- Outputs \"fake\" titles.\n",
    "\n",
    "Much comes from a [Keras example](https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py).\n",
    "\n",
    "## Setup Environment\n",
    "\n",
    "- Import Keras\n",
    "- Open up the Star Trek corpus\n",
    "- Give each leter an index and create dictionaries to translate from index to character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 12106\n",
      "total chars: 55\n"
     ]
    }
   ],
   "source": [
    "## Much borrowed from https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "\n",
    "text = open(\"startrekepisodes.txt\").read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocabulary_size = len(chars)\n",
    "print('total chars:', vocabulary_size)\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "15.4497282609\n",
      "15.0\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# How long is a title?\n",
    "titles = text.split('\\n')\n",
    "lengths = np.array([len(n) for n in titles])\n",
    "print(np.max(lengths))\n",
    "print(np.mean(lengths))\n",
    "print(np.median(lengths))\n",
    "print(np.min(lengths))\n",
    "\n",
    "# hence choose 30 as seuence length to train on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Training Data\n",
    "\n",
    "- Cut up the corpus into sequences of 40 characters.\n",
    "- Change indexes into \"one-hot\" vector encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 4026\n",
      "Done converting y to one-hot.\n",
      "Done preparing training corpus, shapes of sets are:\n",
      "X shape: (4026, 30)\n",
      "y shape: (4026, 55)\n",
      "Vocabulary of characters: 55\n"
     ]
    }
   ],
   "source": [
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 30\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "X = np.zeros((len(sentences), maxlen), dtype=int)\n",
    "y = np.zeros((len(sentences), vocabulary_size), dtype=np.bool)\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    X[i] = np.array(map((lambda x: char_indices[x]), sentences[i]))\n",
    "    y[i, char_indices[next_chars[i]]] = True\n",
    "print(\"Done converting y to one-hot.\")\n",
    "print(\"Done preparing training corpus, shapes of sets are:\")\n",
    "print(\"X shape: \" + str(X.shape))\n",
    "print(\"y shape: \" + str(y.shape))\n",
    "print(\"Vocabulary of characters:\", vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "- Model has one hidden layer of 128 LSTM cells.\n",
    "- Input layer is an Embedding to convert from indices to a vector encoding automatically (common trick - but does it work?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 30, 128)           7040      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 30, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 30, 128)           131584    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 30, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 55)                7095      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 55)                0         \n",
      "=================================================================\n",
      "Total params: 277,303.0\n",
      "Trainable params: 277,303.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model_train = Sequential()\n",
    "model_train.add(Embedding(vocabulary_size, 128, input_length=maxlen))\n",
    "\n",
    "# LSTM part\n",
    "# model_train.add(LSTM(128))\n",
    "# Optional components\n",
    "model_train.add(Dropout(0.5))\n",
    "model_train.add(LSTM(128, return_sequences=True))\n",
    "model_train.add(Dropout(0.5))\n",
    "# model_train.add(LSTM(128, return_sequences=True))\n",
    "#model_train.add(Dropout(0.5))\n",
    "model_train.add(LSTM(128))\n",
    "\n",
    "# Project back to vocabulary\n",
    "model_train.add(Dense(vocabulary_size))\n",
    "model_train.add(Activation('softmax'))\n",
    "model_train.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.01))\n",
    "model_train.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "- Train on batches of 128 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "4026/4026 [==============================] - 10s - loss: 3.1674    \n",
      "Epoch 2/30\n",
      "4026/4026 [==============================] - 9s - loss: 2.7495     \n",
      "Epoch 3/30\n",
      "4026/4026 [==============================] - 9s - loss: 2.4932     \n",
      "Epoch 4/30\n",
      "4026/4026 [==============================] - 9s - loss: 2.3648     \n",
      "Epoch 5/30\n",
      "4026/4026 [==============================] - 10s - loss: 2.2880    \n",
      "Epoch 6/30\n",
      "4026/4026 [==============================] - 9s - loss: 2.2147     \n",
      "Epoch 7/30\n",
      "4026/4026 [==============================] - 9s - loss: 2.1373     \n",
      "Epoch 8/30\n",
      "4026/4026 [==============================] - 9s - loss: 2.0690     \n",
      "Epoch 9/30\n",
      "4026/4026 [==============================] - 9s - loss: 1.9940     \n",
      "Epoch 10/30\n",
      "4026/4026 [==============================] - 9s - loss: 1.9370     \n",
      "Epoch 11/30\n",
      "4026/4026 [==============================] - 9s - loss: 1.8612     \n",
      "Epoch 12/30\n",
      "4026/4026 [==============================] - 9s - loss: 1.8096     \n",
      "Epoch 13/30\n",
      "4026/4026 [==============================] - 10s - loss: 1.7427    \n",
      "Epoch 14/30\n",
      "4026/4026 [==============================] - 10s - loss: 1.7075    \n",
      "Epoch 15/30\n",
      "4026/4026 [==============================] - 11s - loss: 1.6506    \n",
      "Epoch 16/30\n",
      "4026/4026 [==============================] - 10s - loss: 1.5945    \n",
      "Epoch 17/30\n",
      "4026/4026 [==============================] - 9s - loss: 1.5528     \n",
      "Epoch 18/30\n",
      "4026/4026 [==============================] - 10s - loss: 1.5200    \n",
      "Epoch 19/30\n",
      "4026/4026 [==============================] - 9s - loss: 1.4812     \n",
      "Epoch 20/30\n",
      "4026/4026 [==============================] - 10s - loss: 1.4441    \n",
      "Epoch 21/30\n",
      "4026/4026 [==============================] - 9s - loss: 1.4144     \n",
      "Epoch 22/30\n",
      "4026/4026 [==============================] - 9s - loss: 1.3603     \n",
      "Epoch 23/30\n",
      "4026/4026 [==============================] - 9s - loss: 1.3455     \n",
      "Epoch 24/30\n",
      "4026/4026 [==============================] - 10s - loss: 1.3046    \n",
      "Epoch 25/30\n",
      "4026/4026 [==============================] - 9s - loss: 1.2708     \n",
      "Epoch 26/30\n",
      "4026/4026 [==============================] - 9s - loss: 1.2517     \n",
      "Epoch 27/30\n",
      "4026/4026 [==============================] - 9s - loss: 1.2222     \n",
      "Epoch 28/30\n",
      "4026/4026 [==============================] - 11s - loss: 1.2205    \n",
      "Epoch 29/30\n",
      "4026/4026 [==============================] - 11s - loss: 1.1953    \n",
      "Epoch 30/30\n",
      "4026/4026 [==============================] - 11s - loss: 1.1608    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11bdd0fd0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the Model.\n",
    "model_train.fit(X, y, batch_size=128, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model if necessary\n",
    "model_train.save(\"keras-startrek-LSTM-model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Model\n",
    "\n",
    "- Take a quote then add 400 characters.\n",
    "\n",
    "### Make a Decoder model\n",
    "\n",
    "- Needs input length of 1.\n",
    "- Needs batch size of 1\n",
    "- Needs LSTM to be stateful\n",
    "- check that params is the same as model_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model if necessary.\n",
    "model_train = load_model(\"keras-startrek-LSTM-model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (1, 1, 128)               7040      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (1, 1, 128)               0         \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (1, 1, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (1, 1, 128)               0         \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (1, 128)                  131584    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (1, 55)                   7095      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (1, 55)                   0         \n",
      "=================================================================\n",
      "Total params: 277,303.0\n",
      "Trainable params: 277,303.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build a decoding model (input length 1, batch size 1, stateful)\n",
    "model_dec = Sequential()\n",
    "model_dec.add(Embedding(vocabulary_size, 128, input_length=1, batch_input_shape=(1,1)))\n",
    "\n",
    "# LSTM part\n",
    "model_dec.add(Dropout(0.5))\n",
    "model_dec.add(LSTM(128, stateful=True, return_sequences=True))\n",
    "model_dec.add(Dropout(0.5))\n",
    "model_dec.add(LSTM(128, stateful=True))\n",
    "\n",
    "# project back to vocabulary\n",
    "model_dec.add(Dense(vocabulary_size))\n",
    "model_dec.add(Activation('softmax'))\n",
    "model_dec.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.01))\n",
    "model_dec.summary()\n",
    "\n",
    "# set weights from training model\n",
    "model_dec.set_weights(model_train.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sampling function\n",
    "\n",
    "def sample_model(seed, model_name, length=400):\n",
    "    '''Samples a charRNN given a seed sequence.'''\n",
    "    generated = ''\n",
    "    sentence = seed.lower()[:]\n",
    "    generated += sentence\n",
    "    print(\"Seed: \", generated)\n",
    "    \n",
    "    for i in range(length):\n",
    "        x = np.array(map((lambda x: char_indices[x]), sentence))\n",
    "        x = np.reshape(x,(1,1))\n",
    "        preds = model_name.predict(x, verbose=0)[0]\n",
    "        next_index = sample(preds, 0.5)\n",
    "        next_char = indices_char[next_index]\n",
    "        \n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "    print(\"Generated: \", generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:  e\n",
      "Generated:  e  \n",
      "the savan comess  \n",
      "satche shion and if the rarph   \n",
      "inifagis \n",
      "dacan  \n",
      "the ommage  \n",
      "the savan fect \n",
      "mansare\n",
      "stattere   \n",
      "the dave fart 1)\n",
      "twe ascead fricect  \n",
      "same of comming gomme  \n",
      "the first dice   \n",
      "comd gattere   \n",
      "the falanger   \n",
      "the ascers of the plach   \n",
      "cloid and twe lath dow\n",
      "the save   \n",
      "cond twe lorg  \n",
      "the mald fice\n",
      "the farand? part ii\n",
      "twe faland mime  \n",
      "the search, part 1)\n",
      "davan traid delect \n",
      "mantant  \n",
      "the cirdas gire  \n",
      "the savan  \n",
      "the surrier   \n",
      "davan for the cesand \n",
      "card fart ii   \n",
      "the rayading gurd  \n",
      "the save   \n",
      "comd gome boult  \n",
      "the savan ficect  \n",
      "the maqt and twe bloud  \n",
      "the forgd  \n",
      "rejengt and twe layk \n",
      "the ause sespers  \n",
      "the acve some\n",
      "bobmlind  \n",
      "card of genggt   \n",
      "the savan cattere, part 1)\n",
      "twe anter  \n",
      "sport and twe lawse (part ii   \n",
      "the anseave  \n",
      "in the frech   \n",
      "cord the ane   \n",
      "the edse   \n",
      "cand of groud fart ii   \n",
      "the savan fire\n",
      "factorss\n",
      "bath dant teno   \n",
      "annalif courall in the cehand, part ii   \n",
      "the suving gomme   \n",
      "obpalalle \n",
      "clazd gortar\n",
      "statint tenod  \n",
      "the cesand  \n",
      "ca\n"
     ]
    }
   ],
   "source": [
    "# Sample 1000 characters from the model using a random seed from the vocabulary.\n",
    "sample_model(indices_char[random.randint(0,vocabulary_size-1)], model_dec, 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
