







<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">

    
	<title>Deep Dive on RNNs</title>
    

    

    

	<meta name="apple-mobile-web-app-capable" content="yes">
	<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

	<meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link rel="stylesheet" type="text/css" href="/reveal.js/reset.css">
    <link rel="stylesheet" type="text/css" href="/reveal.js/reveal.css">
    <link rel="stylesheet" type="text/css" href="/assets/revealthemes/crepre-dark.css">
  </head>
  <body>
    <div class="reveal"><div class="slides">






































































































































































































<section class="title-slide center" data-background-color="0x000000" data-background-image="/assets/charles-setup.jpg" data-background-size="cover" data-background-opacity="0.7">


<header>
    <h1>Creative Prediction with Neural Networks</h1>
    <p>A course in ML/AI for creative expression</p>
</header>

<h2>Deep Dive on RNNs</h2>

<p>Charles Martin - The Australian National University</p>

<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
	<img src="/assets/resources/by-nc-sa.svg" style="height:1.5rem; margin-left:0;">
</a>

</section><section data-background-image="/assets/canberra1.jpg" data-background-size="cover">
  
  <h2>Ngunnawal &amp; Ngambri &amp; Ngarigu Country</h2>
  
</section><section id="artificial-neuron"><h2>What is an Artificial Neuron?</h2>
<p class='style="width:80%;'><img src="/assets/deep-dive-on-rnns/1-artificial-neurone.png" alt=""></p>
<p><a href="https://en.wikipedia.org/wiki/File:Blausen_0657_MultipolarNeuron.png">Source - Wikimedia Commons</a></p></section><section id="feed-forward"><h2>Feed-Forward Network</h2>
<p><img data-src="/assets/deep-dive-on-rnns/2-feed-forward.png" style="width:60%;  display: block; margin-left: auto; margin-right: auto;"></p>
<p>For each unit: <span class="math inline">\(y = \text{tanh}\big(Wx + b \big)\)</span></p></section><section id="recurrent-network"><h2>Recurrent Network</h2>
<p><img data-src="/assets/deep-dive-on-rnns/3-recurrent.png" style="width:60%;  display: block; margin-left: auto; margin-right: auto;"></p>
<p>For each unit: <span class="math inline">\(y_t = \text{tanh}\big(Ux_t + Vh_{t-1} + b \big)\)</span></p></section><section id="sequence-learning"><h2>Sequence Learning Tasks</h2>
<p style="text-align:center;"><img data-src="/assets/deep-dive-on-rnns/sequence-learning.png" style="width:45%">
<img data-src="/assets/deep-dive-on-rnns/sequence-learning3.jpg" style="width:50%"></p>
<p><img data-src="/assets/deep-dive-on-rnns/sequence-learning2.png" style="width:80%; display: block; margin-left: auto; margin-right: auto;"></p></section><section id="recurrent-network-1"><h2>Recurrent Network</h2>
<p><img data-src="/assets/deep-dive-on-rnns/4-unfolding.png" style="width:75.0%; display: block; margin-left: auto; margin-right: auto;"></p>
<p>simplifying…</p></section><section id="recurrent-network-2"><h2>Recurrent Network</h2>
<p><img data-src="/assets/deep-dive-on-rnns/5-recurrent-vert.png" style="width:25.0%; display: block; margin-left: auto; margin-right: auto;"></p>
<p>simplifying and rotating…</p></section><section id="state-in-recurrent-networks"><h2>“State” in Recurrent Networks</h2>
<p><img data-src="/assets/deep-dive-on-rnns/5-recurrent-vert.png" style="width:25%; float:right;"></p>
<ul>
  <li>Recurrent Networks are all about storing a “state” in between computations…</li>
  <li>A “lossy summary of… past sequences”</li>
  <li>
<em>h</em> is the “hidden state” of our RNN</li>
  <li>What influences <em>h</em>?</li>
</ul></section><section id="defining-the-rnn-state"><h2>Defining the RNN State</h2>
<p><img data-src="/assets/deep-dive-on-rnns/5-recurrent-vert.png" style="width:25%; float:right;"></p>
<p>We can define a simplified RNN represented by this diagram as follows:</p>
<p>\[h_t = \text{tanh}\big(Ux_t + Vh_{t-1} + b \big)\]</p>
<p>\[\hat{y}_t = \text{softmax}(c + Wh_t)\]</p></section><section id="unfolding-an-rnn-in-time"><h2>Unfolding an RNN in Time</h2>
<p><img data-src="/assets/deep-dive-on-rnns/6-recurrent-unroll.png" alt="Unfolding an RNN in Time" style="width:80.0%; display: block;     margin-left: auto; margin-right: auto;"></p>
<ul>
  <li>By unfolding the RNN we can compute \(\hat{y}\) for a given length of sequence.</li>
  <li>Note that the weight matrices \(U\), \(V\), \(W\) are the same for each timestep; this is the big advantage of RNNs!</li>
</ul></section><section id="forward-propagation"><h2>Forward Propagation</h2>
<div class="columns">
<div class="column">
<p><img data-src="/assets/deep-dive-on-rnns/forward-prop.png"></p>
</div>
<div class="column">
<p>We can now use the following equations to compute <span class="math inline">\(\hat{y}_t\)</span>, by computing <span class="math inline">\(h\)</span> for the previous steps:</p>
<p><span class="math display">\[h_t = \text{tanh}\big(Ux_t + Vh_{t-1} + b \big)\]</span></p>
<p><span class="math display">\[\hat{y}_t = \text{softmax}(c + Wh_t)\]</span></p>
</div>
</div></section><section id="y-hat-is-softmaxd"><h2>Y-hat is Softmax’d</h2>
<p><img data-src="/assets/deep-dive-on-rnns/softmax-is-probability.png" style="width:60%; display: block; margin-left: auto; margin-right: auto;"></p>
<p style="text-align:center;"><span class="math inline">\(\hat{y}\)</span> is a probability distribution!</p>
<p><span class="math display">\[\sigma(\mathbf{z})_j = \frac{e^{z_j}}{\sum_{k=1}^K e^{z_k}} \text{ for } j = 1,\ldots, K\]</span></p></section><section id="calculating-loss-categorical-cross-entropy"><h2>Calculating Loss: Categorical Cross Entropy</h2>
<div class="columns">
<div class="column">
<p><img data-src="/assets/deep-dive-on-rnns/calculating-loss.png"></p>
</div>
<div class="column">
<p>We use the categorical cross-entropy function for loss:</p>
<p><span class="math display">\[\begin{align*}
h_t &amp;= \text{tanh}\big( {b} + {Vh}_{t-1} + {Ux}_t \big) \\
\hat{y}_t &amp;= \text{softmax}(c + Wh_t) \\
L_t &amp;= -y_t \cdot \text{log}(\hat{y}_t) \\
\text{Loss} &amp;= \sum_t L_t \\
\end{align*}\]</span></p>
</div>
</div></section><section id="backpropagation-through-time-bptt"><h2>Backpropagation Through Time (BPTT)</h2>
<div class="columns">
<div class="column">
<p><img data-src="/assets/deep-dive-on-rnns/7-recurrent-loss.png" style="width:85%; display: block; margin-left: auto; margin-right: auto;"></p>
</div>
<div class="column">
<p>Propagates error correction backwards through the network graph, adjusting all parameters (<em>U</em>, <em>V</em>, <em>W</em>) to minimise loss.</p>
</div>
</div></section><section id="example-character-level-text-model"><h2>Example: Character-level text model</h2>
<ul>
<li class="fragment">
<strong>Training data:</strong> a collection of text.</li>
<li class="fragment">
<strong>Input (<em>X</em>):</strong> snippets of 30 characters from the collection.</li>
<li class="fragment">
<strong>Target output (<em>y</em>)</strong>: 1 character, the next one after the 30 in each <em>X</em>.</li>
</ul></section><section id="training-the-character-level-model"><h2>Training the Character-level Model</h2>
<div class="columns">
<div class="column">
<p><img data-src="/assets/deep-dive-on-rnns/charRNN-training.png"></p>
</div>
<div class="column">
<ul>
<li>
<strong>Target:</strong> A probability distribution with <span class="math inline">\(P(n) = 1\)</span>
</li>
<li>
<strong>Output:</strong> A probability distribution over all next letters.</li>
<li>
<strong>E.g.:</strong> “My cat is named Simon” would lead to <strong>X</strong>: “My cat is named Simo” and <strong>y</strong>: “n”</li>
</ul>
</div>
</div></section><section id="using-the-trained-model-to-generate-text"><h2>Using the trained model to generate text</h2>
<div class="columns">
<div class="column">
<p><img data-src="/assets/deep-dive-on-rnns/charRNN-sampling.png"></p>
</div>
<div class="column">
<ul>
<li class="fragment">
<strong>S</strong>: Sampling function, sample a letter using the output probability distribution.</li>
<li class="fragment">The generated letter is reinserted at as the next input.</li>
<li class="fragment">We don’t want to always draw the most likely character. The would give frequent repetition and “copying” from the training text. Need a sampling strategy.</li>
</ul>
</div>
</div></section><section id="char-rnn"><h2>Char-RNN</h2>
<div class="columns">
<div class="column">
<ul>
<li class="fragment">RNN as a sequence generator</li>
<li class="fragment">Input is current symbol, output is next predicted symbol.</li>
<li class="fragment">Connect output to input and continue!</li>
<li class="fragment">CharRNN simply applies this to a (subset) of ASCII characters.</li>
<li class="fragment">Train and generate on any text corpus: Fun!</li>
</ul>
</div>
<div class="column">
<p><img data-src="/assets/deep-dive-on-rnns/charRNN-example.png"></p>
<p>See: <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">Karpathy, A. (2015). The unreasonable effectiveness of recurrent neural networks.</a></p>
</div>
</div></section><section id="char-rnn-examples"><h2>Char-RNN Examples</h2>
<div class="columns">
<div class="column">
<p>Shakespeare (Karpathy, 2015):</p>
<blockquote>
<p>Second Senator: They are away this miseries, produced upon my soul, Breaking and strongly should be buried, when I perish The earth and thoughts of many states.</p>
<p>DUKE VINCENTIO: Well, your wit is in the care of side and that.</p>
</blockquote>
</div>
<div class="column">
<p>Latex Algebraic Geometry:</p>
<p><img data-src="/assets/deep-dive-on-rnns/charRNN-latex-karpathy.jpeg" style="width:70%;"></p>
<p>N.B. “<em>Proof.</em> Omitted.” Lol.</p>
</div>
</div></section><section id="startrekrnn" data-background-image="/assets/lectures/all-demo.jpg" data-background-size="cover" data-background-opacity="0.8"><h2>Time to Hack</h2>
<p>Making an RNN that generates Star Trek titles</p>
<p><a href="https://colab.research.google.com/github/cpmpercussion/creative-prediction/blob/master/notebooks/1-star-trek-titles-RNN-basic.ipynb">open in Colab</a></p></section><section class="impact center" data-background-color="#262626" id="rnn-architectures-and-lstm"><p>RNN Architectures and <strong>LSTM</strong></p></section><section id="bidirectional-rnns"><h2>Bidirectional RNNs</h2>
<div class="columns">
<div class="column">
<p><img data-src="/assets/deep-dive-on-rnns/blstm.png" style="width:75%; display: block; margin-left: auto; margin-right: auto;"></p>
</div>
<div class="column">
<ul>
<li class="fragment">Useful for tasks where the whole sequence is available.</li>
<li class="fragment">Each output unit (<span class="math inline">\(\hat{y}\)</span>) depends on both past and future - but most sensitive to closer times.</li>
<li class="fragment">Popular in speech recognition, translation etc.</li>
</ul>
</div>
</div></section><section id="encoder-decoder-seq-to-seq"><h2>Encoder-Decoder (seq-to-seq)</h2>
<p><img data-src="/assets/deep-dive-on-rnns/seq2seq.png" style="width:70%;float:right;"></p>
<p style="text-align:left;" class="fragment">Learns to generate output sequence (<strong>y</strong>) from an input sequence (<strong>x</strong>).</p>
<p style="text-align:left;" class="fragment">Final hidden state of encoder is used to compute a context variable <em>C</em>.</p>
<p style="text-align:left;" class="fragment">For example, translation.</p></section><section id="deep-rnns"><h2>Deep RNNs</h2>
<div class="columns">
<div class="column">
<p><img data-src="/assets/deep-dive-on-rnns/rnn-deep.png" style="width:75%; display: block; margin-left: auto; margin-right: auto;"></p>
</div>
<div class="column">
<ul>
<li class="fragment">Does adding deeper layers to an RNN make it work better?</li>
<li class="fragment">Several options for architecture.</li>
<li class="fragment">Simply stacking RNN layers is very popular; shown to work better by Graves et al. (2013)</li>
<li class="fragment">Intuitively: layers might learn some hierarchical knowledge automatically.</li>
<li class="fragment">Typical setup: up to three recurrent layers.</li>
</ul>
</div>
</div></section><section id="long-term-dependencies"><h2>Long-Term Dependencies</h2>
<div class="columns">
<div class="column">
<ul>
<li class="fragment">Learning long dependencies is a mathematical challenge.</li>
<li class="fragment">Basically: gradients propagated through the same weights tend to vanish (mostly) or explode (rarely)</li>
<li class="fragment">E.g., consider a simplified RNN with no nonlinear activation function or input.</li>
<li class="fragment">Each time step multiplies <em>h(0)</em> by <em>W</em>.</li>
<li class="fragment">This corresponds to raising power of eigenvalues in <span class="math inline">\(\Lambda\)</span>.</li>
<li class="fragment">Eventually, components of <em>h(0)</em> not aligned with the largest eigenvector will be discarded.</li>
</ul>
</div>
<div class="column">
<p><span class="math display">\[\begin{align*}
h_t &amp;= Wh_{t-1}\\
h_t &amp;= (W^t)h_0
\end{align*}\]</span></p>
<p>(supposing <strong>W</strong> admits eigendecomposition with orthogonal matrix <strong>Q</strong>)</p>
<p><span class="math display">\[\begin{align*}
W &amp;= Q\Lambda Q^{\top}\\
h_t &amp;= Q\Lambda ^t Qh_0
\end{align*}\]</span></p>
</div>
</div></section><section id="vanishing-and-exploding-gradients"><h2>Vanishing and Exploding Gradients</h2>
<div class="columns">
<div class="column">
<ul>
<li class="fragment">“in order to store memories in a way that is robust to small perturbations, the RNN must enter a region of parameter space where gradients vanish”</li>
<li class="fragment">“whenever the model is able to represent long term dependencies, the gradient of a long term interaction has exponentially smaller magnitude than the gradient of a short term interaction.”</li>
</ul>
</div>
<div class="column">
<ul>
<li class="fragment">Note that this problem is only relevant for recurrent networks since the weights <strong>W</strong> affecting the hidden state are the same at each time step.</li>
<li class="fragment">Goodfellow and Benigo (2016): “the problem of learning long-term dependencies remains one of the main challenges in deep learning”</li>
<li class="fragment"><a href="http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/">WildML (2015). Backpropagation Through Time and Vanishing Gradients</a></li>
<li class="fragment"><a href="https://ml4a.github.io/ml4a/RNNs/">ML for artists</a></li>
</ul>
</div>
</div></section><section id="gated-rnns"><h2>Gated RNNs</h2>
<div class="columns">
<div class="column">
<p><img data-src="/assets/deep-dive-on-rnns/gated-rnn.png" style="width:80%; display: block; margin-left: auto; margin-right: auto;"></p>
</div>
<div class="column">
<ul>
  <li>Provide <strong>gates</strong> that can change the hidden state a little bit at each step.</li>
  <li class="fragment">The gates are controlled by <strong>learnable weights</strong> as well!</li>
  <li class="fragment">Hidden state weights that may <strong>change</strong> at each time step.</li>
  <li class="fragment">Create <strong>paths through time</strong> with derivatives that do not vanish/explode.</li>
  <li class="fragment">Gates choose information to <strong>accumulate</strong> or <strong>forget</strong> at each step.</li>
</ul>
</div>
</div></section><section id="long-short-term-memory"><h2>Long Short-Term Memory</h2>
<div class="columns">
<div class="column">
<ul>
<li class="fragment">Self-loop containing internal state (c).</li>
<li class="fragment">Three extra gating units:
<ul>
<li class="fragment">
<strong>Forget gate</strong>: controls how much memory is preserved.</li>
<li class="fragment">
<strong>Input gate</strong>: control how much of current input is stored.</li>
<li class="fragment">
<strong>Output gate</strong>: control how much of state is shown to output.</li>
</ul>
</li>
<li class="fragment">Each gate has own <strong>weights</strong> and <strong>biases</strong>, so this uses <em>lots</em> more parameters.</li>
<!-- <li class="fragment">Some variants on this design, e.g., use c as additional input to three gate units.</li> -->
</ul>
</div>
<div class="column">
<p><img data-src="/assets/deep-dive-on-rnns/lstm.png" style="width:60%; display: block; margin-left: auto; margin-right: auto;"></p>
</div>
</div></section><section id="other-gating-units"><h2>Other Gating Units</h2>
<div class="columns">
<div class="column">
  <p><img data-src="/assets/deep-dive-on-rnns/gru.png" style="width:100.0%">
    Source: <a href="https://youtu.be/FpQCAd0zKiU">(Olah, C. 2015.)</a></p>
</div>
<div class="column">
<ul>
<li>Are three gates necessary?</li>
<li>Other gating units are simpler, e.g., Gated Recurrent Unit (GRU)</li>
<li>For the moment, LSTMs are winning in practical use.</li>
<li>Alternative unit design: project idea?</li>
</ul>
</div>
</div></section><section id="visualising-lstm-activations"><h2>Visualising LSTM activations</h2>
<p>Sometimes, the LSTM cell state corresponds with features of the sequential data:</p>
<p><img data-src="/assets/deep-dive-on-rnns/lstm-state-visualisation.png" style="width:70.0%"></p>
<p>Source: <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">(Karpathy, 2015)</a></p></section><section id="charrnn-applications-folkrnn"><h2>CharRNN Applications: FolkRNN</h2>
<p>Some kinds of music can be represented in a text-like manner.</p>
<p><img data-src="/assets/deep-dive-on-rnns/folk-rnn.png" style="width:70.0%"></p>
<p>Source: <a href="https://core.ac.uk/download/pdf/55873381.pdf">Sturm et al. 2015. Folk Music Style Modelling by Recurrent Neural Networks with Long Short Term Memory Units</a></p></section><section id="other-charrnn-applications"><h2>Other CharRNN Applications</h2>
<div class="columns">
  <div class="column">
    <img data-src="/assets/deep-dive-on-rnns/rnn-monet.png" style="width:85.0%; display: block; margin-left: auto; margin-right: auto;">
<p> <a href="http://blog.manugarri.com/teaching-recurrent-neural-networks-about-monet/">Teaching Recurrent Neural Networks about Monet</a></p>
  </div>
<div class="column">
    <img data-src="/assets/deep-dive-on-rnns/rnn-paint-colours.png" style="width:50%; display: block; margin-left: auto; margin-right: auto;">
<p> <a href="http://aiweirdness.com/post/160776374467/new-paint-colors-invented-by-neural-network">New Paint Colours Invented by Neural Network</a></p>
</div>
</div></section><section id="google-magenta-performance-rnn"><h2>Google Magenta Performance RNN</h2>
<div class="columns">
<div class="column">
<p><img data-src="/assets/deep-dive-on-rnns/performance-rnn.jpg" style="width:50%; display: block; margin-left: auto; margin-right: auto;"></p>
</div>
<div class="column">
<p><img data-src="/assets/deep-dive-on-rnns/performance-rnn2.png" style="width:50%; display: block; margin-left: auto; margin-right: auto;"></p>
</div>
</div>
<ul>
<li>State-of-the-art in music generating RNNs.</li>
<li>Encode MIDI musical sequences as categorical data.</li>
<li>Now supports polyphony (multiple notes), dynamics (volume), expressive timing (rubato).</li>
<li>E.g.: <a href="https://youtu.be/JVf6esaXeLE">YouTube demo</a>
</li>
</ul></section><section id="neural-ipad-band-another-charrnn"><h2>Neural iPad Band, another CharRNN</h2>
<div class="columns">
<div class="column">
  <p>
    <img data-src="/assets/deep-dive-on-rnns/neural-ipad-screen.jpg" style="display: block; margin-left: auto; margin-right: auto;">
    <img data-src="/assets/deep-dive-on-rnns/neural-ipad-diagram.png" style="display: block; margin-left: auto; margin-right: auto;">
  </p>
</div>
<div class="column">
<ul>
<li>iPad music transcribed as sequence of numbers for each performer.</li>
<li>Trick: encode multiple ints as one (preserving ordering).</li>
<li><a href="https://youtu.be/FpQCAd0zKiU">Video</a></li>
</ul>
</div>
</div></section><section id="reading" data-background-image="/assets/lectures/all-reading.jpg" data-background-size="cover" data-background-opacity="0.8"><h2>Books and Learning References</h2>
<p><a href="http://www.deeplearningbook.org">Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press.</a></p>
<p><a href="https://www.manning.com/books/deep-learning-with-python">François Chollet. 2018. Manning.</a></p>
<p><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Chris Olah. 2015. Understanding LSTMs</a></p>
<p><a href="https://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html">RNNs in Tensorflow</a></p>
<p><a href="https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0">Maybe RNN/LSTM is dead? CNNs can work similarly to BLSTMs</a></p>
<p><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">Karpathy. 2015. The Unreasonable Effectiveness of RNNs</a></p>
<p><a href="http://shop.oreilly.com/product/0636920189817.do">Foster. 2019. Generative Deep Learning: Teaching Machines to Paint, Write, Compose, and Play</a></p></section><section id="examples" data-background-image="/assets/lectures/all-demo.jpg" data-background-size="cover" data-background-opacity="0.8"><h2>Time to Hack</h2>
<p>These examples run in Google Colaboratory, just click the link to start them up.</p>
<p>Star Trek RNN (<a href="https://colab.research.google.com/github/cpmpercussion/creative-prediction/blob/master/notebooks/1-star-trek-titles-RNN-basic.ipynb">open in Colab</a>)</p>
<p>Advanced CharRNN (<a href="https://colab.research.google.com/github/cpmpercussion/creative-prediction/blob/master/notebooks/2-star-trek-titles-advanced.ipynb">open in Colab</a>)</p>
<p>Melody Generation (<a href="https://colab.research.google.com/github/cpmpercussion/creative-prediction/blob/master/notebooks/3-zeldic-musical-RNN.ipynb">open in Colab</a>)</p></section><section id="summary"><h2>Summary</h2>
<ul>
  <li>Recurrent Neural Networks let us capture and model the structure of sequential data.</li>
  <li>Sampling from trained RNNs allow us to generate new, creative sequences.</li>
  <li>The internal state of RNNs make them interesting for interactive applications, since it lets them capture and continue from the current context or “style”.</li>
  <li>LSTM units are able to overcome the vanishing gradient problem to some extent.</li>
</ul></section>
</div></div>

    
    <script src="/reveal.js/plugin/math/math.js"></script>
    
    <script src="/chalkboard-redux/plugin.js"></script>
    

    <script src="/reveal.js/reveal.js"></script>
    <script>
// this is the reveal config
Reveal.initialize(
  {
  width: 1920,
  height: 1080,
  margin: 0.1,
  center: false,
  controls: false,
  transition: 'none',
  history: true,
  slideNumber: true,
  plugins: [
    ChalkboardRedux,
    RevealMath
  ],
  math: {
    mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js',
    config: 'TeX-AMS_HTML-full',
  }
}

);
    </script>

    <!-- we don't use reveal.js's highlight.js plugin because it doesn't play -->
    <!-- nice with the "revealified" markdown-fenced-code-blocks -->

    <!-- basically, this is a consequence of this whole thing being hacks-upon-hacks -->

    <script src="/reveal.js/hljs/highlight.pack.js"></script>
    <script src="/reveal.js/hljs/extempore.min.js"></script>
    <link rel="stylesheet" href="/reveal.js/hljs/atom-one-dark.css">

    <script>
     hljs.highlightAll();
    </script>

  </body>
</html>
