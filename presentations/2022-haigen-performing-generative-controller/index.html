







<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">

    
	<title>Performing with a Generative Electronic Music Controller</title>
    

    

    

	<meta name="apple-mobile-web-app-capable" content="yes">
	<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

	<meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link rel="stylesheet" type="text/css" href="/reveal.js/reset.css">
    <link rel="stylesheet" type="text/css" href="/reveal.js/reveal.css">
    <link rel="stylesheet" type="text/css" href="/assets/revealthemes/reveal/moon.css">
  </head>
  <body>
    <div class="reveal"><div class="slides">
<!-- theme: assets/revealthemes/crepre-dark.scss -->















































<section class="title-slide center" data-background-color="#000000" data-background-image="/assets/charles-setup.jpg" data-background-size="cover" data-background-opacity="0.5" id="title">

<header>
  <h1>Performing with a Generative Electronic Music Controller</h1>
  <p></p>
</header>

<h2>Dr Charles Martin - The Australian National University</h2>

<p>web: <a href="https://charlesmartin.com.au">charlesmartin.com.au</a>     twitter/github: <a href="https://github.com/cpmpercussion" class="user-mention">@cpmpercussion</a></p>

</section><section data-background-color="#000000" id="country" data-background-image="/assets/canberra1.jpg" data-background-size="cover" data-background-opacity="0.8">
  
  <h2>Ngunnawal &amp; Ngambri Country</h2>
  
</section><section id="impsxtouch" data-background-video="/assets/predictive-models-in-interactive-music/imps-xtouch-loop.mp4" data-background-size="cover" data-background-video-muted="true" data-background-video-loop="true" data-background="#000000" data-background-opacity="1.0">
  
</section><section data-background-color="#000000" data-background-image="/assets/lectures/all-demo.jpg" data-background-size="cover" data-background-opacity="0.4">
  
  <h2>Embodied Music Generation</h2>
  
<ul>
  <li>
    <p><strong>note generation</strong>: generate “symbolic” music—notes (A, B, C, half-note, quaver, etc.). Abstract version of sounds created by <em>some</em> musical instruments.</p>
  </li>
  <li>
    <p><strong>embodied gesture generation</strong>: generate the <em>movements</em> a performer makes to operate a particular musical instrument.</p>
  </li>
</ul>
<p>this project explores <strong>embodied gesture generation</strong> in an improvised electronic music context!</p></section><section data-background-color="#000000" id="xtouch" data-background-image="/assets/performing-generative-controller/xtouch-imps-4.jpg" data-background-size="cover" data-background-opacity="0.4">
  
  <h2>Why do this?</h2>
  
<ul>
  <li>
    <p>Lots of musical instruments <strong>don’t use “notes”</strong></p>
  </li>
  <li>
    <p>e.g., turntable, mixer, modular synthesiser, effects pedal, etc</p>
  </li>
  <li>
    <p>what does “intelligence” and “co-creation” look like in these instruments?</p>
  </li>
  <li>
    <p>can we incorporate generative AI into a longer-term performance practice?</p>
  </li>
</ul></section><section data-background-color="#000000" id="xtouch" data-background-image="/assets/performing-generative-controller/xtouch-annotated.jpg" data-background-size="contain">
  
</section><section id="generative-ai-system"><h2>Generative AI System</h2>
<ul>
  <li>
    <p>gestural predictions are made by a Mixture Density Recurrent Neural Network (implemented using <a href="https://creativeprediction.xyz/imps/">“Interactive Music Prediction System”—IMPS</a>)</p>
  </li>
  <li>
    <p>MDRNN: an extension of common LSTM/RNN designs to allow expressive predictions of multiple continuous variables.</p>
  </li>
  <li>
    <p>MDRNN specs: 2 32-unit LSTM layers, 9-dimensional mixture density layer (8 knobs + time)</p>
  </li>
  <li>
    <p>IMPS: A CLI Python program that provides MDRNN, data collection, training and interaction features.</p>
  </li>
  <li>
    <p>communicates with music software over OSC (Open Sound Control)</p>
  </li>
  <li>
    <p>in this case, MDRNN is configured for <strong>“call-and-response”</strong> interaction (or <strong>“continuation”</strong>)</p>
  </li>
</ul></section><section data-background-color="#FFFFFF" id="xtouch" data-background-image="/assets/imps/mdn-motivation.png" data-background-size="contain">
  
</section><section data-background-color="#000000" id="xtouch" data-background-image="/assets/performing-generative-controller/xtouch-imps-2.jpg" data-background-size="cover" data-background-opacity="0.6">
  
  <h2>Performances and Experiences</h2>
  
<ul>
  <li>
    <p>deployed in performance since 2019</p>
  </li>
  <li>
    <p>so it works! and it’s practical!</p>
  </li>
  <li>
    <p>but is it better than a <strong>random walk generator?</strong></p>
  </li>
</ul></section><section id="impsxtouch" data-background-video="/assets/performing-generative-controller/xtouch-1-sine-tone-solo.mp4" data-background-size="cover" data-background-video-loop="true" data-background="#000000" data-background-opacity="1.0">
  
</section><section id="impsxtouch" data-background-video="/assets/performing-generative-controller/xtouch-2-noise-switchover.mp4" data-background-size="cover" data-background-video-loop="true" data-background="#000000" data-background-opacity="1.0">
  
</section><section id="impsxtouch" data-background-video="/assets/performing-generative-controller/xtouch-3-samples-independent.mp4" data-background-size="cover" data-background-video-loop="true" data-background="#000000" data-background-opacity="1.0">
  
</section><section data-background-color="#000000" id="xtouch" data-background-image="/assets/performing-generative-controller/xtouch-imps-3.jpg" data-background-size="cover" data-background-opacity="0.4">
  
  <h2>Influence and Co-Creation</h2>
  
<ul>
  <li>
    <p>can be steered (a little bit) by performer’s gestures</p>
  </li>
  <li>
    <p>tends to continue adjusting knobs the performer last used</p>
  </li>
  <li>
    <p>learns interesting behaviours from data (moving one vs multiple knobs, pauses, continuous changes)</p>
  </li>
  <li>
    <p>good to for performer to have a different task to work on.</p>
  </li>
  <li>
    <p>also important to allow performer to “just listen”</p>
  </li>
</ul></section><section data-background-color="#000000" data-background-image="/assets/lectures/all-demo.jpg" data-background-size="cover" data-background-opacity="0.4">
  
  <h2>Small Data and Co-Adaptation</h2>
  
<ul>
  <li>
    <p>interactions from each performance are saved</p>
  </li>
  <li>
    <p>some of these have been incorporated into training datasets</p>
  </li>
  <li>
    <p>co-adaptive: system grows and changes along with the performer (yet to be studied rigorously)</p>
  </li>
</ul></section><section id="impsxtouch" data-background-video="/assets/predictive-models-in-interactive-music/imps-xtouch-loop.mp4" data-background-size="cover" data-background-video-muted="true" data-background-video-loop="true" data-background="#000000" data-background-opacity="0.4">
  
  <h1>Thanks!</h1>
  
<ul>
  <li>IMPS on <a href="https://github.com/cpmpercussion/imps">GitHub</a>
</li>
  <li>creative ML: <a href="https://creativeprediction.xyz/">creativeprediction.xyz</a>
</li>
  <li>Twitter/Github: <a href="https://www.twitter.com/cpmpercussion">@cpmpercussion</a>
</li>
  <li>Homepage: <a href="https://charlesmartin.com.au">charlesmartin.com.au</a>
</li>
</ul></section>
</div></div>

    
    <script src="/reveal.js/plugin/math/math.js"></script>
    
    <script src="/chalkboard-redux/plugin.js"></script>
    

    <script src="/reveal.js/reveal.js"></script>
    <script>
// this is the reveal config
Reveal.initialize(
  {
  width: 1920,
  height: 1080,
  margin: 0.1,
  center: true,
  controls: true,
  transition: 'fade',
  history: false,
  hash: true,
  slideNumber: true,
  plugins: [
    ChalkboardRedux,
    RevealMath
  ],
  math: {
    mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js',
    config: 'TeX-AMS_HTML-full',
  }
}

);
    </script>

    <!-- we don't use reveal.js's highlight.js plugin because it doesn't play -->
    <!-- nice with the "revealified" markdown-fenced-code-blocks -->

    <!-- basically, this is a consequence of this whole thing being hacks-upon-hacks -->

    <script src="/reveal.js/hljs/highlight.pack.js"></script>
    <script src="/reveal.js/hljs/extempore.min.js"></script>
    <link rel="stylesheet" href="/reveal.js/hljs/atom-one-dark.css">

    <script>
     hljs.highlightAll();
    </script>

  </body>
</html>
