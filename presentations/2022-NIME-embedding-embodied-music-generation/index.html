







<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">

    
	<title>Embedding Embodied Music Generation</title>
    

    

    

	<meta name="apple-mobile-web-app-capable" content="yes">
	<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

	<meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link rel="stylesheet" type="text/css" href="/reveal.js/reset.css">
    <link rel="stylesheet" type="text/css" href="/reveal.js/reveal.css">
    <link rel="stylesheet" type="text/css" href="/assets/revealthemes/reveal/moon.css">
  </head>
  <body>
    <div class="reveal"><div class="slides">
<!-- theme: assets/revealthemes/crepre-dark.scss -->

















<!-- EMPI -->

























































<!-- what's next -->































<section class="title-slide center" data-background-color="#000000" data-background-image="/assets/charles-setup.jpg" data-background-size="cover" data-background-opacity="0.5" id="title">

<header>
  <h1>Embedding Embodied Music Generation</h1>
  <p></p>
</header>

<h2>Dr Charles Martin - The Australian National University</h2>

<p>web: <a href="https://charlesmartin.com.au">charlesmartin.com.au</a>     twitter/github: <a href="https://github.com/cpmpercussion" class="user-mention">@cpmpercussion</a></p>

</section><section data-background-color="#000000" id="country" data-background-image="/assets/canberra1.jpg" data-background-size="cover" data-background-opacity="0.8">
  
  <h2>Ngunnawal &amp; Ngambri Country</h2>
  
</section><section id="impsxtouch" data-background-video="/assets/predictive-models-in-interactive-music/imps-xtouch-loop.mp4" data-background-size="cover" data-background-video-muted="true" data-background-video-loop="true" data-background="#000000" data-background-opacity="1.0">
  
</section><section data-background-color="#000000" data-background-image="/assets/lectures/all-demo.jpg" data-background-size="cover" data-background-opacity="0.4">
  
  <h2>Embodied Music Generation</h2>
  
<ul>
  <li>
    <p><strong>note generation</strong>: generate “symbolic” music—notes (A, B, C, half-note, quaver, etc.). Abstract version of sounds created by <em>some</em> musical instruments.</p>
  </li>
  <li>
    <p><strong>embodied gesture generation</strong>: generate the <em>movements</em> a performer makes to operate a particular musical instrument.</p>
  </li>
</ul>
<p>this project explores <strong>embodied gesture generation</strong> in an improvised electronic music context!</p></section><section data-background-color="#000000" id="xtouch" data-background-image="/assets/performing-generative-controller/xtouch-imps-4.jpg" data-background-size="cover" data-background-opacity="0.4">
  
  <h2>Why do this?</h2>
  
<ul>
  <li>
    <p>Lots of musical instruments <strong>don’t use “notes”</strong></p>
  </li>
  <li>
    <p>e.g., turntable, mixer, modular synthesiser, effects pedal, etc</p>
  </li>
  <li>
    <p>what does “intelligence” and “co-creation” look like in these instruments?</p>
  </li>
  <li>
    <p>can we incorporate generative AI into a longer-term performance practice?</p>
  </li>
</ul></section><section id="empititle" data-background-video="/assets/empi/empi-micro-demo.mp4" data-background-size="cover" data-background-video-muted="true" data-background-video-loop="true" data-background="#000000" data-background-opacity="0.7">
  
  <h1>Embodied Predictive Musical Instrument (EMPI)</h1>
  
</section><section id="empidetails"><h2>Embodied Predictive Musical Instrument (EMPI)</h2>
<p><img src="/assets/empi/EMPI-system-diagram.jpg" alt="" style="width:50%;"></p>
<ul>
  <li>Predicts next movement and time, represents physically.</li>
  <li>Experiments with interaction mappings; mainly focussed on call-response</li>
  <li>Weird and confusing/fun?</li>
</ul></section><section id="training-data"><h2>Training Data</h2>
<p><img src="/assets/empi/training_human.png" alt="Human Data">
<img src="/assets/empi/training_sine.png" alt="Sine Data" style="width:24%">
<img src="/assets/empi/training_square.png" alt="Square Data" style="width:24%">
<img src="/assets/empi/training_saw.png" alt="Saw Data" style="width:24%">
<img src="/assets/empi/training_noise.png" alt="Noise Data" style="width:24%"></p></section><section id="generated-data"><h2>Generated Data</h2>
<p><img src="/assets/empi/generation_human_4500points.png" alt="Human Generation" style="width:50%">
<img src="/assets/empi/generation_synth.png" alt="Synth Generation" style="width:50%">
<img src="/assets/empi/generation_noise.png" alt="Noise Generation" style="width:50%"></p></section><section id="empihuman" data-background-video="/assets/empi/empi-short-demo.mp4" data-background-size="cover" data-background-video-loop="true" data-background="#000000" data-background-opacity="1.0">
  
</section><section id="empinoise" data-background-video="/assets/empi/empi_synth.mp4" data-background-size="cover" data-background-video-loop="true" data-background="#000000" data-background-opacity="1.0">
  
</section><section id="empinoise" data-background-video="/assets/empi/empi_noise.mp4" data-background-size="cover" data-background-video-loop="true" data-background="#000000" data-background-opacity="1.0">
  
</section><section data-background-color="#000000" id="empi-experiment" data-background-image="/assets/empi/study-image.jpg" data-background-size="cover" data-background-opacity="0.4">
  
  <h2>Improvisations with EMPI</h2>
  
<ul>
  <li>
    <p>12 participants</p>
  </li>
  <li>
    <p>two independent factors: <em>model</em> and <em>feedback</em></p>
  </li>
  <li>
    <p>model: human, synthetic, noise</p>
  </li>
  <li>
    <p>feedback: motor on, motor off</p>
  </li>
</ul></section><section id="empi-survey"><h2>Results: Survey</h2>
<p><img src="/assets/empi/survey_boxplot_results.jpg" alt="" style="width:80%;"></p>
<p>Change of ML model had significant effect: Q2, Q4, Q5, Q6, Q7</p></section><section id="empi-survey-takeaway"><h2>Results: Survey</h2>
<ul>
  <li>
    <p>human model most “related”, noise was least</p>
  </li>
  <li>
    <p>human model most “musically creative”</p>
  </li>
  <li>
    <p>human model easiest to “influence”</p>
  </li>
  <li>
    <p>noise model not rated badly!</p>
  </li>
</ul>
<p>Participants generally preferred human or synth, but not always!</p></section><section id="empi-perf-length"><h2>Results: Performance Length</h2>
<p><img src="/assets/empi/performance_length.jpg" alt="" style="width:80%;"></p>
<p>Human and synth: <strong>more range</strong> of performance lengths with motor on.</p>
<p>Noise: <strong>more range</strong> with motor off.</p></section><section id="takeaways"><h2>Takeaways</h2>
<p>Studied self-contained intelligent instrument in <strong>genuine performance</strong>.</p>
<p>Physical representation could be <strong>polarising</strong>.</p>
<p>Performers work hard to <strong>understand</strong> and <strong>influence</strong> ML model.</p>
<p>Constrained, intelligent instrument can produce a <strong>compelling experience</strong>.</p></section><section data-background-color="#000000" id="xtouch" data-background-image="/assets/performing-generative-controller/xtouch-annotated.jpg" data-background-size="contain">
  
</section><section id="generative-ai-system"><h2>Generative AI System</h2>
<ul>
  <li>
    <p>gestural predictions are made by a Mixture Density Recurrent Neural Network (implemented using <a href="https://creativeprediction.xyz/imps/">“Interactive Music Prediction System”—IMPS</a>)</p>
  </li>
  <li>
    <p>MDRNN: an extension of common LSTM/RNN designs to allow expressive predictions of multiple continuous variables.</p>
  </li>
  <li>
    <p>MDRNN specs: 2 32-unit LSTM layers, 9-dimensional mixture density layer (8 knobs + time)</p>
  </li>
  <li>
    <p>IMPS: A CLI Python program that provides MDRNN, data collection, training and interaction features.</p>
  </li>
  <li>
    <p>communicates with music software over OSC (Open Sound Control)</p>
  </li>
  <li>
    <p>in this case, MDRNN is configured for <strong>“call-and-response”</strong> interaction (or <strong>“continuation”</strong>)</p>
  </li>
</ul></section><section data-background-color="#FFFFFF" id="xtouch" data-background-image="/assets/imps/mdn-motivation.png" data-background-size="contain">
  
</section><section data-background-color="#000000" id="xtouch" data-background-image="/assets/performing-generative-controller/xtouch-imps-2.jpg" data-background-size="cover" data-background-opacity="0.6">
  
  <h2>Performances and Experiences</h2>
  
<ul>
  <li>
    <p>deployed in performance since 2019</p>
  </li>
  <li>
    <p>so it works! and it’s practical!</p>
  </li>
  <li>
    <p>but is it better than a <strong>random walk generator?</strong></p>
  </li>
</ul></section><section id="impsxtouch" data-background-video="/assets/performing-generative-controller/xtouch-1-sine-tone-solo.mp4" data-background-size="cover" data-background-video-loop="true" data-background="#000000" data-background-opacity="1.0">
  
</section><section id="impsxtouch" data-background-video="/assets/performing-generative-controller/xtouch-2-noise-switchover.mp4" data-background-size="cover" data-background-video-loop="true" data-background="#000000" data-background-opacity="1.0">
  
</section><section id="impsxtouch" data-background-video="/assets/performing-generative-controller/xtouch-3-samples-independent.mp4" data-background-size="cover" data-background-video-loop="true" data-background="#000000" data-background-opacity="1.0">
  
</section><section data-background-color="#000000" id="xtouch" data-background-image="/assets/performing-generative-controller/xtouch-imps-3.jpg" data-background-size="cover" data-background-opacity="0.4">
  
  <h2>Influence and Co-Creation</h2>
  
<ul>
  <li>
    <p>can be steered (a little bit) by performer’s gestures</p>
  </li>
  <li>
    <p>tends to continue adjusting knobs the performer last used</p>
  </li>
  <li>
    <p>learns interesting behaviours from data (moving one vs multiple knobs, pauses, continuous changes)</p>
  </li>
  <li>
    <p>good to for performer to have a different task to work on.</p>
  </li>
  <li>
    <p>also important to allow performer to “just listen”</p>
  </li>
</ul></section><section data-background-color="#000000" data-background-image="/assets/lectures/all-demo.jpg" data-background-size="cover" data-background-opacity="0.4">
  
  <h2>Small Data and Co-Adaptation</h2>
  
<ul>
  <li>
    <p>interactions from each performance are saved</p>
  </li>
  <li>
    <p>some of these have been incorporated into training datasets</p>
  </li>
  <li>
    <p>co-adaptive: system grows and changes along with the performer (yet to be studied rigorously)</p>
  </li>
</ul></section><section id="impsxtouch" data-background-video="/assets/predictive-models-in-interactive-music/imps-xtouch-loop.mp4" data-background-size="cover" data-background-video-muted="true" data-background-video-loop="true" data-background="#000000" data-background-opacity="0.4">
  
  <h1>Thanks!</h1>
  
<ul>
  <li>IMPS on <a href="https://github.com/cpmpercussion/imps">GitHub</a>
</li>
  <li>creative ML: <a href="https://creativeprediction.xyz/">creativeprediction.xyz</a>
</li>
  <li>Twitter/Github: <a href="https://www.twitter.com/cpmpercussion">@cpmpercussion</a>
</li>
  <li>Homepage: <a href="https://charlesmartin.com.au">charlesmartin.com.au</a>
</li>
</ul></section>
</div></div>

    
    <script src="/reveal.js/plugin/math/math.js"></script>
    
    <script src="/chalkboard-redux/plugin.js"></script>
    

    <script src="/reveal.js/reveal.js"></script>
    <script>
// this is the reveal config
Reveal.initialize(
  {
  width: 1920,
  height: 1080,
  margin: 0.1,
  center: true,
  controls: true,
  transition: 'fade',
  history: false,
  hash: true,
  slideNumber: true,
  plugins: [
    ChalkboardRedux,
    RevealMath
  ],
  math: {
    mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js',
    config: 'TeX-AMS_HTML-full',
  }
}

);
    </script>

    <!-- we don't use reveal.js's highlight.js plugin because it doesn't play -->
    <!-- nice with the "revealified" markdown-fenced-code-blocks -->

    <!-- basically, this is a consequence of this whole thing being hacks-upon-hacks -->

    <script src="/reveal.js/hljs/highlight.pack.js"></script>
    <script src="/reveal.js/hljs/extempore.min.js"></script>
    <link rel="stylesheet" href="/reveal.js/hljs/atom-one-dark.css">

    <script>
     hljs.highlightAll();
    </script>

  </body>
</html>
