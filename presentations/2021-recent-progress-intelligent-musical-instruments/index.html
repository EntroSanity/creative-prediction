







<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">

    
	<title>Recent Progress in Intelligent Musical Instruments</title>
    

    

    

	<meta name="apple-mobile-web-app-capable" content="yes">
	<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

	<meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link rel="stylesheet" type="text/css" href="/reveal.js/reset.css">
    <link rel="stylesheet" type="text/css" href="/reveal.js/reveal.css">
    <link rel="stylesheet" type="text/css" href="/assets/revealthemes/reveal/white.css">
  </head>
  <body>
    <div class="reveal"><div class="slides">
<!-- crepre-dark.scss -->



































<!-- here's where I come in -->



















<!-- Interactive RNN Instrument -->









<!-- Gesture RNN -->











<!-- Robojam and Microjam -->













<!-- IMPS -->





























































<!-- EMPI -->

























































<!-- what's next -->























<!-- Neurofeedback 2020 video  -->


<!-- https://youtu.be/WumtMGHAuV8 -->







<section class="title-slide center" data-background-color="#000000" data-background-image="/assets/charles-setup.jpg" data-background-size="cover" data-background-opacity="0.5" id="title">

<header>
  <h1>Recent Progress in Intelligent Musical Instruments</h1>
  <p></p>
</header>

<h2>Dr Charles Martin - The Australian National University</h2>

<p>web: <a href="https://charlesmartin.com.au">charlesmartin.com.au</a>     twitter/github: <a href="https://github.com/cpmpercussion" class="user-mention">@cpmpercussion</a></p>

</section><section data-background-color="#000000" id="country" data-background-image="/assets/canberra1.jpg" data-background-size="cover" data-background-opacity="0.8">
  
  <h2>Ngunnawal &amp; Ngambri Country</h2>
  
</section><section data-background-color="#000000" id="vision" data-background-image="/assets/predictive-models-in-interactive-music/rohan-performance-2-performance.JPG" data-background-size="cover" data-background-opacity="0.5">
  
  <h2>vision</h2>
  
<p><em>Intelligent Musical Instruments</em> become a normal part of musical
performance and production.</p></section><section data-background-color="#000000" id="why" data-background-image="/assets/predictive-models-in-interactive-music/2015-11-28-Electrofringe-Workshop-2.jpg" data-background-size="cover" data-background-opacity="0.5">
  
  <h2>why?</h2>
  
<p>Assist professional musicians &amp; composers</p>
<p>Engage novice musicians &amp; students</p>
<p>Reveal <em>creative interaction</em> with intelligent systems</p>
<p>Create <em>new kinds of music!</em></p></section><section data-background-color="#000000" id="musical-predictions" data-background-image="/assets/predictive-models-in-interactive-music/musical-performance-predictions.jpg" data-background-size="cover" data-background-opacity="0.5">
  
  <h2>making intelligent musical predictions</h2>
  
</section><section id="history"><h1>History</h1></section><section data-background-color="#000000" id="digitalmusicalinstruments" data-background-image="/assets/intmusinstruments/don-banks-ANU.jpg" data-background-size="cover" data-background-opacity="0.9">
  
  <h2>Digital Musical Instruments (1979ish-)</h2>
  
</section><section data-background-color="#000000" id="voyager" data-background-image="/assets/intmusinstruments/GeorgeLewis-1999-credit-IanCummings.jpg" data-background-size="cover" data-background-opacity="0.9">
  
  <h2>Voyager - George E Lewis (1986-)</h2>
  
</section><section data-background-color="#000000" id="continuator" data-background-image="/assets/intmusinstruments/continuator-francoispachet.jpg" data-background-size="cover" data-background-opacity="0.9">
  
  <h2>Continuator - François Pachet (2001)</h2>
  
</section><section data-background-color="#000000" id="wekinator" data-background-image="/assets/intmusinstruments/rebecca-fiebrink-wekinator.jpg" data-background-size="cover" data-background-opacity="0.9">
  
  <h2>Wekinator - Rebecca Fiebrink (2009-)</h2>
  
</section><section data-background-color="#FFFFFF" id="magenta" data-background-image="/assets/intmusinstruments/magenta-demos.jpg" data-background-size="contain" data-background-opacity="0.9">
  
  <h2>Magenta Project - Google (2016-)</h2>
  
</section><section id="where-are-all-the-intelligent-musical-instruments"><h1>where are all the intelligent musical instruments?</h1></section><section data-background-color="#FFFFFF" id="venn-diagram" data-background-image="/assets/charlesmartin-background.jpg" data-background-size="contain">
  
</section><section id="perfdata"><h2>Performance data is diverse</h2>
<p><img src="/assets/predictive-models-in-interactive-music/imps-nimes-examples.jpg" alt=""></p>
<table>
  <thead>
    <tr>
      <th><strong>Music Systems</strong></th>
      <th><strong>Data</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Score / Notation</td>
      <td>Symbolic Music, Image</td>
    </tr>
    <tr>
      <td>Digital Instruments</td>
      <td>MIDI</td>
    </tr>
    <tr>
      <td>Recording &amp; Production</td>
      <td>Digital  Audio</td>
    </tr>
    <tr>
      <td>New Musical Interfaces</td>
      <td>Gestural and Sensor Data</td>
    </tr>
    <tr>
      <td>Show Control</td>
      <td>Video, Audio, Lighting, Control Signals</td>
    </tr>
  </tbody>
</table></section><section id="predictingsequences"><h2>Predicting sequences</h2>
<p><img src="/assets/predictive-models-in-interactive-music/sequence-learning.png" alt=""></p></section><section id="interacting-with-predictions"><h2>Interacting with predictions</h2>
<p><img src="/assets/predictive-models-in-interactive-music/predictive-interaction-motivation.png" alt=""></p></section><section id="orchestra"><h1>creating an orchestra of intelligent instruments…</h1></section><section id="interactive-rnn-instrument"><h2>Interactive RNN Instrument</h2>
<p><img src="/assets/predictive-models-in-interactive-music/physical-intelligent-instrument2.png" alt="Physical Intelligent Instrument" style="width:35%"></p>
<ul>
  <li>Generates endless music with a melody RNN.</li>
  <li>Switchable Dataset.</li>
  <li>Controls for sampling “temperature”.</li>
</ul></section><section id="rpiinsttitle" data-background-video="/assets/predictive-models-in-interactive-music/physical-intelligent-instrument.mp4" data-background-size="cover" data-background-video-loop="true" data-background="#000000" data-background-opacity="0.7">
  
  <h1>Physical Intelligent Instrument</h1>
  
</section><section id="gesture-rnn-title" data-background-video="/assets/predictive-models-in-interactive-music/neural-ensemble-interaction.mp4" data-background-size="cover" data-background-video-loop="true" data-background="#000000" data-background-opacity="0.7">
  
  <h1>GestureRNN</h1>
  
</section><section id="gesture-rnn-details"><h2>GestureRNN</h2>
<p><img src="/assets/predictive-models-in-interactive-music/gesture-rnn.png" alt="Gesture RNN" style="width:80%"></p>
<ul>
  <li>Predicts 1 of 9 “gestures” for three AI performers.</li>
  <li>Trained on labelled data from 5 hours of quartet performances.</li>
  <li>Actual “sounds” are chunks of each gesture played back.</li>
</ul></section><section id="robojamtitle" data-background-video="/assets/predictive-models-in-interactive-music/robojam-drum-response.mp4" data-background-size="contain" data-background-video-loop="true" data-background="#000000" data-background-opacity="0.7">
  
  <h1>RoboJam</h1>
  
</section><section id="robojamdetails"><h2>Robojam and Microjam</h2>
<p><img src="/assets/predictive-models-in-interactive-music/robojam-interaction.png" alt="Robojam Interaction" style="width:45%"></p>
<ul>
  <li>Predicts next touch location in screen (x, y, dt).</li>
  <li>Trained on ~1500 5s performances.</li>
  <li>Produces duet “responses” to the user.</li>
</ul></section><section id="mdn-model"><h2>Mixture Density Network</h2>
<p><img src="/assets/predictive-models-in-interactive-music/mdn-motivation.png" alt=""></p></section><section id="robojamperf" data-background-video="/assets/predictive-models-in-interactive-music/robojam-one-loop.mp4" data-background-size="cover" data-background-video-loop="true" data-background="#000000" data-background-opacity="1.0">
  
</section><section id="imps-title" data-background-video="/assets/predictive-models-in-interactive-music/imps-lightpad-loop.mp4" data-background-size="cover" data-background-video-muted="true" data-background-video-loop="true" data-background="#000000" data-background-opacity="0.7">
  
  <h1>IMPS System</h1>
  
</section><section id="imps-details"><h2>IMPS System</h2>
<p><img src="/assets/predictive-models-in-interactive-music/IMPS_connection_example.png" alt="" style="width:80%;"></p>
<ul>
  <li>Opinionated Neural Network for interacting with NIMES.</li>
  <li>Automatically collects data and trains.</li>
  <li>“Wekinator” for deep learning?</li>
</ul></section><section id="three-easy-steps"><h2>Three easy steps…</h2>
<ul>
  <li>
<strong>Collect some data:</strong> IMPS logs interactions automatically to build up a dataset</li>
  <li>
<strong>Train an MDRNN:</strong> IMPS includes good presets, no need to train for days/weeks</li>
  <li>
<strong>Perform!</strong> IMPS includes three interaction modes, scope to extend in future!</li>
</ul></section><section id="impslightpad" data-background-video="/assets/predictive-models-in-interactive-music/imps-lightpad-loop.mp4" data-background-size="cover" data-background-video-loop="true" data-background="#000000" data-background-opacity="1.0">
  
</section><section id="impsxtouch" data-background-video="/assets/predictive-models-in-interactive-music/imps-xtouch-loop.mp4" data-background-size="cover" data-background-video-loop="true" data-background="#000000" data-background-opacity="1.0">
  
</section><section id="impsglissrnn" data-background-video="/assets/predictive-models-in-interactive-music/glissrnn.mp4" data-background-size="cover" data-background-video-loop="true" data-background="#000000" data-background-opacity="1.0">
  
</section><section id="imps-practical"><h2>Experiment: Is this <em>practical</em>?</h2>
<ul>
  <li>Is it practical for real-time use?</li>
  <li>How do the MDRNN parameters affect time per prediction?</li>
  <li>What are “good defaults” for training parameters?</li>
  <li>Do you need a powerful/expensive computer?</li>
</ul></section><section id="imps-testsystems"><h2>Test Systems</h2>
<p><img src="/assets/imps/imps-test-computers.jpg" alt="Test computers"></p></section><section id="imps-timeperpredunits"><h2>Results: Time per prediction</h2>
<p><img src="/assets/imps/prediction_time_against_units.png" alt="Time per prediction vs LSTM units"></p>
<p>Time per prediction (ms) with different sizes of LSTM layers.</p></section><section id="imps-timeperpreddim"><h2>Results: Time per prediction</h2>
<p><img src="/assets/imps/prediction_time_against_dimension_64units.png" alt="Time per prediction vs MDN dimension"></p>
<p>Time per prediction (ms) with different MDN output dimensions. (64
LSTM units)</p></section><section id="imps-training-small"><h2>Results: Training Error vs Validation Set Error</h2>
<p><img src="/assets/imps/training_loss_12K_lightpad_model.png" alt=""></p>
<p>12K sample dataset (15 minutes of performance)</p>
<p>Takeaway: <strong>Smallest model best for small datasets.</strong> Don’t bother training for
too long.</p></section><section id="imps-training-medium"><h2>Results: Training Error vs Validation Set Error</h2>
<p><img src="/assets/imps/training_loss_100K_lightpad_model.png" alt=""></p>
<p>100K sample dataset (120 minutes of performance)</p>
<p>Takeaway: <strong>64- and 128-unit model still best!</strong></p></section><section id="imps-generation"><h2>Results: Exploring Generation</h2>
<p><img src="/assets/imps/robojam_temperature_sampling_0.png" alt="" style="width:45%"></p>
<p>Takeaway: Make Gaussians <strong>less diverse</strong>, make categorical <strong>more diverse</strong>.</p></section><section id="empititle" data-background-video="/assets/empi/empi-micro-demo.mp4" data-background-size="cover" data-background-video-muted="true" data-background-video-loop="true" data-background="#000000" data-background-opacity="0.7">
  
  <h1>Embodied Predictive Musical Instrument (EMPI)</h1>
  
</section><section id="empidetails"><h2>Embodied Predictive Musical Instrument (EMPI)</h2>
<p><img src="/assets/empi/EMPI-system-diagram.jpg" alt="" style="width:50%;"></p>
<ul>
  <li>Predicts next movement and time, represents physically.</li>
  <li>Experiments with interaction mappings; mainly focussed on call-response</li>
  <li>Weird and confusing/fun?</li>
</ul></section><section id="training-data"><h2>Training Data</h2>
<p><img src="/assets/empi/training_human.png" alt="Human Data">
<img src="/assets/empi/training_sine.png" alt="Sine Data" style="width:24%">
<img src="/assets/empi/training_square.png" alt="Square Data" style="width:24%">
<img src="/assets/empi/training_saw.png" alt="Saw Data" style="width:24%">
<img src="/assets/empi/training_noise.png" alt="Noise Data" style="width:24%"></p></section><section id="generated-data"><h2>Generated Data</h2>
<p><img src="/assets/empi/generation_human_4500points.png" alt="Human Generation" style="width:50%">
<img src="/assets/empi/generation_synth.png" alt="Synth Generation" style="width:50%">
<img src="/assets/empi/generation_noise.png" alt="Noise Generation" style="width:50%"></p></section><section id="empihuman" data-background-video="/assets/empi/empi-short-demo.mp4" data-background-size="cover" data-background-video-loop="true" data-background="#000000" data-background-opacity="1.0">
  
</section><section id="empinoise" data-background-video="/assets/empi/empi_synth.mp4" data-background-size="cover" data-background-video-loop="true" data-background="#000000" data-background-opacity="1.0">
  
</section><section id="empinoise" data-background-video="/assets/empi/empi_noise.mp4" data-background-size="cover" data-background-video-loop="true" data-background="#000000" data-background-opacity="1.0">
  
</section><section data-background-color="#000000" id="empi-experiment" data-background-image="/assets/empi/study-image.jpg" data-background-size="cover" data-background-opacity="0.4">
  
  <h2>Improvisations with EMPI</h2>
  
<ul>
  <li>
    <p>12 participants</p>
  </li>
  <li>
    <p>two independent factors: <em>model</em> and <em>feedback</em></p>
  </li>
  <li>
    <p>model: human, synthetic, noise</p>
  </li>
  <li>
    <p>feedback: motor on, motor off</p>
  </li>
</ul></section><section id="empi-survey"><h2>Results: Survey</h2>
<p><img src="/assets/empi/survey_boxplot_results.jpg" alt="" style="width:80%;"></p>
<p>Change of ML model had significant effect: Q2, Q4, Q5, Q6, Q7</p></section><section id="empi-survey-takeaway"><h2>Results: Survey</h2>
<ul>
  <li>
    <p>human model most “related”, noise was least</p>
  </li>
  <li>
    <p>human model most “musically creative”</p>
  </li>
  <li>
    <p>human model easiest to “influence”</p>
  </li>
  <li>
    <p>noise model not rated badly!</p>
  </li>
</ul>
<p>Participants generally preferred human or synth, but not always!</p></section><section id="empi-perf-length"><h2>Results: Performance Length</h2>
<p><img src="/assets/empi/performance_length.jpg" alt="" style="width:80%;"></p>
<p>Human and synth: <strong>more range</strong> of performance lengths with motor on.</p>
<p>Noise: <strong>more range</strong> with motor off.</p></section><section id="takeaways"><h2>Takeaways</h2>
<p>Studied self-contained intelligent instrument in <strong>genuine performance</strong>.</p>
<p>Physical representation could be <strong>polarising</strong>.</p>
<p>Performers work hard to <strong>understand</strong> and <strong>influence</strong> ML model.</p>
<p>Constrained, intelligent instrument can produce a <strong>compelling experience</strong>.</p></section><section data-background-color="#000000" id="whattodo" data-background-image="/assets/predictive-models-in-interactive-music/ipad-ensemble-2015.jpg" data-background-size="cover" data-background-opacity="0.5">
  
  <h2>How can intelligent instruments help us make music?</h2>
  
<p>Emulate or enhance ensemble experience</p>
<p>Engage in call-and-response improvisation</p>
<p>Model a performer’s personal style</p>
<p>Modify/improve performance actions in place</p></section><section data-background-color="#000000" id="evaluation" data-background-image="/assets/predictive-models-in-interactive-music/metatone-20170529-ifi.jpg" data-background-size="cover" data-background-opacity="0.4">
  
  <h2>Research questions...</h2>
  
<p>Are ML models practical for musical prediction?</p>
<p>Are intelligent instruments useful to musicians?</p>
<p>What happens when musicians and instrument <em>co-adapt</em>?</p>
<p>Can a musical practice be represented as a dataset?</p>
<p>What does a intelligent instrument <strong>album / concert</strong> sound like?</p></section><section class="center" data-background-color="#000">

<iframe class="stretch" src="https://www.youtube.com/embed/WumtMGHAuV8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe>

</section><section id="thanks"><h2>Thanks!</h2>
<p><img src="/assets/charlesgif.gif" alt="" style="width:25%;"></p>
<ul>
  <li>IMPS on <a href="https://github.com/cpmpercussion/imps">GitHub</a>
</li>
  <li>creative ML: <a href="https://creativeprediction.xyz/">creativeprediction.xyz</a>
</li>
  <li>Twitter/Github: <a href="https://www.twitter.com/cpmpercussion">@cpmpercussion</a>
</li>
  <li>Homepage: <a href="https://charlesmartin.com.au">charlesmartin.com.au</a>
</li>
</ul></section>
</div></div>

    
    <script src="/reveal.js/plugin/math/math.js"></script>
    
    <script src="/chalkboard-redux/plugin.js"></script>
    

    <script src="/reveal.js/reveal.js"></script>
    <script>
// this is the reveal config
Reveal.initialize(
  {
  width: 1920,
  height: 1080,
  margin: 0.1,
  center: true,
  controls: true,
  transition: 'fade',
  history: false,
  hash: true,
  slideNumber: true,
  plugins: [
    ChalkboardRedux,
    RevealMath
  ],
  math: {
    mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js',
    config: 'TeX-AMS_HTML-full',
  }
}

);
    </script>

    <!-- we don't use reveal.js's highlight.js plugin because it doesn't play -->
    <!-- nice with the "revealified" markdown-fenced-code-blocks -->

    <!-- basically, this is a consequence of this whole thing being hacks-upon-hacks -->

    <script src="/reveal.js/hljs/highlight.pack.js"></script>
    <script src="/reveal.js/hljs/extempore.min.js"></script>
    <link rel="stylesheet" href="/reveal.js/hljs/atom-one-dark.css">

    <script>
     hljs.highlightAll();
    </script>

  </body>
</html>
